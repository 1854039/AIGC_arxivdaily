<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv - RL+Diffusion</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-10T00:00:00Z">2025-07-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of <span class="highlight-title">Pretrain</span>ing Word Co-occurrence on Compositional Generalization
  in Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helen Qu, Sang Michael Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP and large multimodal models (LMMs) have better accuracy on examples
involving concepts that are highly represented in the training data. However,
the role of concept combinations in the training data on compositional
generalization is largely unclear -- for instance, how does accuracy vary when
a common object appears in an uncommon pairing with another object? In this
paper, we investigate how word co-occurrence statistics in the pretraining
dataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM
performance. To disentangle the effects of word co-occurrence frequencies from
single-word frequencies, we measure co-occurrence with pointwise mutual
information (PMI), which normalizes the joint probability of two words
co-occurring by the probability of co-occurring independently. Using
synthetically generated images with a variety of concept pairs, we show a
strong correlation between PMI in the CLIP pretraining data and zero-shot
accuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap
between images in the top and bottom 5% of PMI values), demonstrating that even
accuracy on common concepts is affected by the combination of concepts in the
image. Leveraging this finding, we reproduce this effect in natural images by
editing them to contain pairs with varying PMI, resulting in a correlation of
r=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs
built on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings
highlight the need for algorithms and architectures that improve compositional
generalization in multimodal models without scaling the training data
combinatorially. Our code is available at
https://github.com/helenqu/multimodal-pretraining-pmi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-pass Adaptive Image Tokenization for Minimum Program Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Duggal, Sanghyun Byun, William T. Freeman, Antonio Torralba, Phillip Isola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to Algorithmic Information Theory (AIT) -- Intelligent
representations compress data into the shortest possible program that can
reconstruct its content, exhibiting low Kolmogorov Complexity (KC). In
contrast, most visual representation learning systems use fixed-length
representations for all inputs, ignoring variations in complexity or
familiarity. Recent adaptive tokenization methods address this by allocating
variable-length representations but typically require test-time search over
multiple encodings to find the most predictive one. Inspired by Kolmogorov
Complexity principles, we propose a single-pass adaptive tokenizer, KARL, which
predicts the appropriate number of tokens for an image in a single forward
pass, halting once its approximate KC is reached. The token count serves as a
proxy for the minimum description length. KARL's training procedure closely
resembles the Upside-Down Reinforcement Learning paradigm, as it learns to
conditionally predict token halting based on a desired reconstruction quality.
KARL matches the performance of recent adaptive tokenizers while operating in a
single pass. We present scaling laws for KARL, analyzing the role of
encoder/decoder size, continuous vs. discrete tokenization and more.
Additionally, we offer a conceptual study drawing an analogy between Adaptive
Image Tokenization and Algorithmic Information Theory, examining the predicted
image complexity (KC) across axes such as structure vs. noise and in- vs.
out-of-distribution familiarity -- revealing alignment with human intuition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: https://github.com/ShivamDuggal4/karl Keywords:
  Representation Learning, Adaptive Tokenization, Compression, Algorithmic
  Information Theory, Kolmogorov Complexity, Upside-Down RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skip a Layer or Loop it? Test-Time Depth Adaptation of <span class="highlight-title">Pretrain</span>ed LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Li, Yang Li, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can a pretrained neural network adapt its architecture to different inputs
without any finetuning? Do we need all layers for simple tasks, and are they
adequate for challenging tasks? We found that the layers of a pretrained large
language model (LLM) can be manipulated as separate modules to build a better
and even shallower model customized for each test sample. In particular, each
layer from the pretrained model can be skipped/pruned or repeated multiple
times as recurrent neural networks (RNN), and stacked with others in arbitrary
orders, yielding a chain-of-layers (CoLa) per sample. This compositional space
greatly expands the scope of existing works on looped/recurrent pretrained
modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree
Search (MCTS) protocol to explore and identify the optimal CoLa for each sample
from math and commonsense reasoning benchmarks. Compared to a static model of a
fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same
layer(s) (slow thinking), and combining both, offering more flexible, dynamic
architectures for different inputs. We conduct an extensive analysis of the
MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples
with correct predictions by the original LLM, we can find shorter CoLa,
suggesting a large space for improving inference efficiency; (2) For >60% of
samples with originally incorrect predictions, we can identify CoLa achieving
correct predictions, suggesting a large space of performance enhancement. Our
results highlight the shortcomings of using a fixed architecture of pre-trained
LLMs for inference on different samples and pave the way to unlock the
generalization power of test-time depth adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXPO: Stable Reinforcement Learning with Expressive Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Perry Dong, Qiyang Li, Dorsa Sadigh, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of training and fine-tuning expressive policies with
online reinforcement learning (RL) given an offline dataset. Training
expressive policy classes with online RL present a unique challenge of stable
value maximization. Unlike simpler Gaussian policies commonly used in online
RL, expressive policies like diffusion and flow-matching policies are
parameterized by a long denoising chain, which hinders stable gradient
propagation from actions to policy parameters when optimizing against some
value function. Our key insight is that we can address stable value
maximization by avoiding direct optimization over value with the expressive
policy and instead construct an on-the-fly RL policy to maximize Q-value. We
propose Expressive Policy Optimization (EXPO), a sample-efficient online RL
algorithm that utilizes an on-the-fly policy to maximize value with two
parameterized policies -- a larger expressive base policy trained with a stable
imitation learning objective and a light-weight Gaussian edit policy that edits
the actions sampled from the base policy toward a higher value distribution.
The on-the-fly policy optimizes the actions from the base policy with the
learned edit policy and chooses the value maximizing action from the base and
edited actions for both sampling and temporal-difference (TD) backup. Our
approach yields up to 2-3x improvement in sample efficiency on average over
prior methods both in the setting of fine-tuning a pretrained policy given
offline data and in leveraging offline data to train online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why is Your Language Model a Poor Implicit Reward Model? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Razin, Yong Lin, Jiarui Yao, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Action Chunking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyang Li, Zhiyuan Zhou, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prospective Learning in Retrospect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Bai, Cecelia Shuai, Ashwin De Silva, Siyu Yu, Pratik Chaudhari, Joshua T. Vogelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In most real-world applications of artificial intelligence, the distributions
of the data and the goals of the learners tend to change over time. The
Probably Approximately Correct (PAC) learning framework, which underpins most
machine learning algorithms, fails to account for dynamic data distributions
and evolving objectives, often resulting in suboptimal performance. Prospective
learning is a recently introduced mathematical framework that overcomes some of
these limitations. We build on this framework to present preliminary results
that improve the algorithm and numerical results, and extend prospective
learning to sequential decision-making scenarios, specifically foraging. Code
is available at: https://github.com/neurodata/prolearn2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AGI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Chunking for End-to-End Hierarchical Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukjun Hwang, Brandon Wang, Albert Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite incredible progress in language models (LMs) in recent years, largely
resulting from moving away from specialized models designed for specific tasks
to general models based on powerful architectures (e.g. the Transformer) that
learn everything from raw data, pre-processing steps such as tokenization
remain a barrier to true end-to-end foundation models. We introduce a
collection of new techniques that enable a dynamic chunking mechanism which
automatically learns content -- and context -- dependent segmentation
strategies learned jointly with the rest of the model. Incorporating this into
an explicit hierarchical network (H-Net) allows replacing the (implicitly
hierarchical) tokenization-LM-detokenization pipeline with a single model
learned fully end-to-end. When compute- and data- matched, an H-Net with one
stage of hierarchy operating at the byte level outperforms a strong Transformer
language model operating over BPE tokens. Iterating the hierarchy to multiple
stages further increases its performance by modeling multiple levels of
abstraction, demonstrating significantly better scaling with data and matching
a token-based Transformer of twice its size. H-Nets pretrained on English show
significantly increased character-level robustness, and qualitatively learn
meaningful data-dependent chunking strategies without any heuristics or
explicit supervision. Finally, the H-Net's improvement over tokenized pipelines
is further increased in languages and modalities with weaker tokenization
heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement
in data efficiency over baselines), showing the potential of true end-to-end
models that learn and scale better from unprocessed data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient
  Human Activity Recognition on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhen Bian, Mengxi Liu, Vitor Fortes Rey, Daniel Geissler, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) on resource-constrained wearable devices
demands inference models that harmonize accuracy with computational efficiency.
This paper introduces TinierHAR, an ultra-lightweight deep learning
architecture that synergizes residual depthwise separable convolutions, gated
recurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency
without compromising performance. Evaluated across 14 public HAR datasets,
TinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.
DeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the
averaged F1-scores. Beyond quantitative gains, this work provides the first
systematic ablation study dissecting the contributions of spatial-temporal
components across proposed TinierHAR, prior SOTA TinyHAR, and the classical
DeepConvLSTM, offering actionable insights for designing efficient HAR systems.
We finally discussed the findings and suggested principled design guidelines
for future efficient HAR. To catalyze edge-HAR research, we open-source all
materials in this work for future
benchmarking\footnote{https://github.com/zhaxidele/TinierHAR}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low Resource Reconstruction Attacks Through Benign <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sol Yarkoni, Roi Livni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in generative models such as diffusion models have raised
several risks and concerns related to privacy, copyright infringements and data
stewardship. To better understand and control the risks, various researchers
have created techniques, experiments and attacks that reconstruct images, or
part of images, from the training set. While these techniques already establish
that data from the training set can be reconstructed, they often rely on
high-resources, excess to the training set as well as well-engineered and
designed prompts.
  In this work, we devise a new attack that requires low resources, assumes
little to no access to the actual training set, and identifies, seemingly,
benign prompts that lead to potentially-risky image reconstruction. This
highlights the risk that images might even be reconstructed by an uninformed
user and unintentionally. For example, we identified that, with regard to one
existing model, the prompt ``blue Unisex T-Shirt'' can generate the face of a
real-life human model. Our method builds on an intuition from previous works
which leverages domain knowledge and identifies a fundamental vulnerability
that stems from the use of scraped data from e-commerce platforms, where
templated layouts and images are tied to pattern-like prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Continuous Home Cage Monitoring: An Evaluation of Tracking and
  Identification Strategies for Laboratory Mice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Pablo Oberhauser, Daniel Grzenda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous, automated monitoring of laboratory mice enables more accurate
data collection and improves animal welfare through real-time insights.
Researchers can achieve a more dynamic and clinically relevant characterization
of disease progression and therapeutic effects by integrating behavioral and
physiological monitoring in the home cage. However, providing individual mouse
metrics is difficult because of their housing density, similar appearances,
high mobility, and frequent interactions. To address these challenges, we
develop a real-time identification (ID) algorithm that accurately assigns ID
predictions to mice wearing custom ear tags in digital home cages monitored by
cameras. Our pipeline consists of three parts: (1) a custom multiple object
tracker (MouseTracks) that combines appearance and motion cues from mice; (2) a
transformer-based ID classifier (Mouseformer); and (3) a tracklet associator
linear program to assign final ID predictions to tracklets (MouseMap). Our
models assign an animal ID based on custom ear tags at 30 frames per second
with 24/7 cage coverage. We show that our custom tracking and ID pipeline
improves tracking efficiency and lowers ID switches across mouse strains and
various environmental factors compared to current mouse tracking methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plausible Counterfactual Explanations of Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Černý, Jiří Němeček, Ivan Dovica, Jakub Mareček
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations play a variety of roles in various recommender systems, from a
legally mandated afterthought, through an integral element of user experience,
to a key to persuasiveness. A natural and useful form of an explanation is the
Counterfactual Explanation (CE). We present a method for generating highly
plausible CEs in recommender systems and evaluate it both numerically and with
a user study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A statistical physics framework for optimal learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Mignacco, Francesco Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning is a complex dynamical process shaped by a range of interconnected
decisions. Careful design of hyperparameter schedules for artificial neural
networks or efficient allocation of cognitive resources by biological learners
can dramatically affect performance. Yet, theoretical understanding of optimal
learning strategies remains sparse, especially due to the intricate interplay
between evolving meta-parameters and nonlinear learning dynamics. The search
for optimal protocols is further hindered by the high dimensionality of the
learning space, often resulting in predominantly heuristic, difficult to
interpret, and computationally demanding solutions. Here, we combine
statistical physics with control theory in a unified theoretical framework to
identify optimal protocols in prototypical neural network models. In the
high-dimensional limit, we derive closed-form ordinary differential equations
that track online stochastic gradient descent through low-dimensional order
parameters. We formulate the design of learning protocols as an optimal control
problem directly on the dynamics of the order parameters with the goal of
minimizing the generalization error at the end of training. This framework
encompasses a variety of learning scenarios, optimization constraints, and
control budgets. We apply it to representative cases, including optimal
curricula, adaptive dropout regularization and noise schedules in denoising
autoencoders. We find nontrivial yet interpretable strategies highlighting how
optimal protocols mediate crucial learning tradeoffs, such as maximizing
alignment with informative input directions while minimizing noise fitting.
Finally, we show how to apply our framework to real datasets. Our results
establish a principled foundation for understanding and designing optimal
learning protocols and suggest a path toward a theory of meta-learning grounded
in statistical physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Retrieval of Topics and Insights from Earnings Calls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anant Gupta, Rajarshi Bhowmik, Geoffrey Gunow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking the strategic focus of companies through topics in their earnings
calls is a key task in financial analysis. However, as industries evolve,
traditional topic modeling techniques struggle to dynamically capture emerging
topics and their relationships. In this work, we propose an LLM-agent driven
approach to discover and retrieve emerging topics from quarterly earnings
calls. We propose an LLM-agent to extract topics from documents, structure them
into a hierarchical ontology, and establish relationships between new and
existing topics through a topic ontology. We demonstrate the use of extracted
topics to infer company-level insights and emerging trends over time. We
evaluate our approach by measuring ontology coherence, topic evolution
accuracy, and its ability to surface emerging financial trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2nd Workshop on Financial Information Retrieval in the Era of
  Generative AI, The 48th International ACM SIGIR Conference on Research and
  Development in Information Retrieval July 13-17, 2025 | Padua, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Causal Discovery for Autoregressive Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Fesanghary, Achintya Gopal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we present a novel constraint-based algorithm for causal
structure learning specifically designed for nonlinear autoregressive time
series. Our algorithm significantly reduces computational complexity compared
to existing methods, making it more efficient and scalable to larger problems.
We rigorously evaluate its performance on synthetic datasets, demonstrating
that our algorithm not only outperforms current techniques, but also excels in
scenarios with limited data availability. These results highlight its potential
for practical applications in fields requiring efficient and accurate causal
inference from nonlinear time series data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient
  Neural Inference on MCUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashe Neth, Sawinder kaur, Mohammad Nur Hossain Khan, Subrata Biswas, Asif Salekin, Bashima Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing pruning methods are typically applied during training or compile
time and often rely on structured sparsity. While compatible with low-power
microcontrollers (MCUs), structured pruning underutilizes the opportunity for
fine-grained efficiency on devices without SIMD support or parallel compute. To
address these limitations, we introduce UnIT (Unstructured Inference-Time
pruning), a lightweight method that dynamically identifies and skips
unnecessary multiply-accumulate (MAC) operations during inference, guided by
input-specific activation patterns. Unlike structured pruning, UnIT embraces
irregular sparsity and does not require retraining or hardware specialization.
It transforms pruning decisions into lightweight comparisons, replacing
multiplications with threshold checks and approximated divisions. UnIT further
optimizes compute by reusing threshold computations across multiple connections
and applying layer- and group-specific pruning sensitivity. We present three
fast, hardware-friendly division approximations tailored to the capabilities of
common embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT
achieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and
27.33% to 84.38% lower energy consumption compared to training-time pruned
models, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT
matches or exceeds the accuracy of retrained models while requiring
significantly fewer MACs. These results establish unstructured inference-time
pruning as a viable and practical solution for efficient, retraining-free
deployment of deep neural networks on MCUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to SenSys 2026 on July 1, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization
  with Joint Global-Local Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ban, Gokul Ram Subramani, Kaiyi Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) enables a joint model to capture commonalities
across multiple tasks, reducing computation costs and improving data
efficiency. However, a major challenge in MTL optimization is task conflicts,
where the task gradients differ in direction or magnitude, limiting model
performance compared to single-task counterparts. Sharpness-aware minimization
(SAM) minimizes task loss while simultaneously reducing the sharpness of the
loss landscape. Our empirical observations show that SAM effectively mitigates
task conflicts in MTL. Motivated by these findings, we explore integrating SAM
into MTL but face two key challenges. While both the average loss gradient and
individual task gradients-referred to as global and local
information-contribute to SAM, how to combine them remains unclear. Moreover,
directly computing each task gradient introduces significant computational and
memory overheads. To address these challenges, we propose SAMO, a lightweight
\textbf{S}harpness-\textbf{A}ware \textbf{M}ulti-task \textbf{O}ptimization
approach, that leverages a joint global-local perturbation. The local
perturbations are approximated using only forward passes and are layerwise
normalized to improve efficiency. Extensive experiments on a suite of
multi-task benchmarks demonstrate both the effectiveness and efficiency of our
method. Code is available at https://github.com/OptMN-Lab/SAMO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI-predicted complexes teach machine learning to compute drug
  binding affinity? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Tse Hsu, Savva Grevtsev, Thomas Douglas, Aniket Magarkar, Philip C. Biggin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate the feasibility of using co-folding models for synthetic data
augmentation in training machine learning-based scoring functions (MLSFs) for
binding affinity prediction. Our results show that performance gains depend
critically on the structural quality of augmented data. In light of this, we
established simple heuristics for identifying high-quality co-folding
predictions without reference structures, enabling them to substitute for
experimental structures in MLSF training. Our study informs future data
augmentation strategies based on co-folding models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Feng, Yicheng Lin, Shaojie Zhuo, Chenzheng Su, Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Xiaopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Automatic Speech Recognition (ASR) have demonstrated
remarkable accuracy and robustness in diverse audio applications, such as live
transcription and voice command processing. However, deploying these models on
resource constrained edge devices (e.g., IoT device, wearables) still presents
substantial challenges due to strict limits on memory, compute and power.
Quantization, particularly Post-Training Quantization (PTQ), offers an
effective way to reduce model size and inference cost without retraining.
Despite its importance, the performance implications of various advanced
quantization methods and bit-width configurations on ASR models remain unclear.
In this work, we present a comprehensive benchmark of eight state-of-the-art
(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and
Moonshine. We systematically evaluate model performances (i.e., accuracy,
memory I/O and bit operations) across seven diverse datasets from the open ASR
leaderboard, analyzing the impact of quantization and various configurations on
both weights and activations. Built on an extension of the LLM compression
toolkit, our framework integrates edge-ASR models, diverse advanced
quantization algorithms, a unified calibration and evaluation data pipeline,
and detailed analysis tools. Our results characterize the trade-offs between
efficiency and accuracy, demonstrating that even 3-bit quantization can succeed
on high capacity models when using advanced PTQ techniques. These findings
provide valuable insights for optimizing ASR models on low-power, always-on
edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving AEBS Validation Through Objective Intervention Classification
  Leveraging the Prediction Divergence Principle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Betschinske, Steven Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication at the 2025 IEEE
  International Automated Vehicle Validation Conference (IAVVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key
  Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toluwani Aremu, Noor Hussein, Munachiso Nwadike, Samuele Poppi, Jie Zhang, Karthik Nandakumar, Neil Gong, Nils Lukas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Bralios, Jonah Casebeer, Paris Smaragdis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural audio codecs and autoencoders have emerged as versatile models for
audio compression, transmission, feature-extraction, and latent-space
generation. However, a key limitation is that most are trained to maximize
reconstruction fidelity, often neglecting the specific latent structure
necessary for optimal performance in diverse downstream applications. We
propose a simple, post-hoc framework to address this by modifying the
bottleneck of a pre-trained autoencoder. Our method introduces a
"Re-Bottleneck", an inner bottleneck trained exclusively through latent space
losses to instill user-defined structure. We demonstrate the framework's
effectiveness in three experiments. First, we enforce an ordering on latent
channels without sacrificing reconstruction quality. Second, we align latents
with semantic embeddings, analyzing the impact on downstream diffusion
modeling. Third, we introduce equivariance, ensuring that a filtering operation
on the input waveform directly corresponds to a specific transformation in the
latent space. Ultimately, our Re-Bottleneck framework offers a flexible and
efficient way to tailor representations of neural audio models, enabling them
to seamlessly meet the varied demands of different applications with minimal
additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE MLSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting and generating antibiotics against future pathogens with
  ApexOracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianang Leng, Fangping Wan, Marcelo Der Torossian Torres, Cesar de la Fuente-Nunez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antimicrobial resistance (AMR) is escalating and outpacing current antibiotic
development. Thus, discovering antibiotics effective against emerging pathogens
is becoming increasingly critical. However, existing approaches cannot rapidly
identify effective molecules against novel pathogens or emerging drug-resistant
strains. Here, we introduce ApexOracle, an artificial intelligence (AI) model
that both predicts the antibacterial potency of existing compounds and designs
de novo molecules active against strains it has never encountered. Departing
from models that rely solely on molecular features, ApexOracle incorporates
pathogen-specific context through the integration of molecular features
captured via a foundational discrete diffusion language model and a
dual-embedding framework that combines genomic- and literature-derived strain
representations. Across diverse bacterial species and chemical modalities,
ApexOracle consistently outperformed state-of-the-art approaches in activity
prediction and demonstrated reliable transferability to novel pathogens with
little or no antimicrobial data. Its unified representation-generation
architecture further enables the in silico creation of "new-to-nature"
molecules with high predicted efficacy against priority threats. By pairing
rapid activity prediction with targeted molecular generation, ApexOracle offers
a scalable strategy for countering AMR and preparing for future
infectious-disease outbreaks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Principled Foundations for Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Zhou, Shujian Zhang, Brice Magdalou, John Lambert, Ehsan Amid, Richard Nock, Andrew Hard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Credit Risk Analysis for SMEs Using Graph Neural Networks in Supply
  Chain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhou Zhang, Qinyan Shen, Zhuohuan Hu, Qianying Liu, Huijie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small and Medium-sized Enterprises (SMEs) are vital to the modern economy,
yet their credit risk analysis often struggles with scarce data, especially for
online lenders lacking direct credit records. This paper introduces a Graph
Neural Network (GNN)-based framework, leveraging SME interactions from
transaction and social data to map spatial dependencies and predict loan
default risks. Tests on real-world datasets from Discover and Ant Credit (23.4M
nodes for supply chain analysis, 8.6M for default prediction) show the GNN
surpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for
supply chain mining and default prediction, respectively. It also helps
regulators model supply chain disruption impacts on banks, accurately
forecasting loan defaults from material shortages, and offers Federal Reserve
stress testers key data for CCAR risk buffers. This approach provides a
scalable, effective tool for assessing SME credit risk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper will be published on 2025 International Conference on Big
  Data, Artificial Intelligence and Digital Economy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization Guarantees for Square-Root Natural-Gradient Variational
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navish Kumar, Thomas Möllenhoff, Mohammad Emtiyaz Khan, Aurelien Lucchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational inference with natural-gradient descent often shows fast
convergence in practice, but its theoretical convergence guarantees have been
challenging to establish. This is true even for the simplest cases that involve
concave log-likelihoods and use a Gaussian approximation. We show that the
challenge can be circumvented for such cases using a square-root
parameterization for the Gaussian covariance. This approach establishes novel
convergence guarantees for natural-gradient variational-Gaussian inference and
its continuous-time gradient flow. Our experiments demonstrate the
effectiveness of natural gradient methods and highlight their advantages over
algorithms that use Euclidean or Wasserstein geometries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-Train</span>ed AI Model Assisted Online Decision-Making under Missing
  Covariates: A Theoretical Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichen Hu, David Simchi-Levi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a sequential contextual decision-making problem in which certain
covariates are missing but can be imputed using a pre-trained AI model. From a
theoretical perspective, we analyze how the presence of such a model influences
the regret of the decision-making process. We introduce a novel notion called
"model elasticity", which quantifies the sensitivity of the reward function to
the discrepancy between the true covariate and its imputed counterpart. This
concept provides a unified way to characterize the regret incurred due to model
imputation, regardless of the underlying missingness mechanism. More
surprisingly, we show that under the missing at random (MAR) setting, it is
possible to sequentially calibrate the pre-trained model using tools from
orthogonal statistical learning and doubly robust regression. This calibration
significantly improves the quality of the imputed covariates, leading to much
better regret guarantees. Our analysis highlights the practical value of having
an accurate pre-trained model in sequential decision-making tasks and suggests
that model elasticity may serve as a fundamental metric for understanding and
improving the integration of pre-trained models in a wide range of data-driven
decision-making problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "So, Tell Me About Your Policy...": Distillation of interpretable
  policies from Deep Reinforcement Learning agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Dispoto, Paolo Bonetti, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Reinforcement Learning (RL) largely benefit from the
inclusion of Deep Neural Networks, boosting the number of novel approaches
proposed in the field of Deep Reinforcement Learning (DRL). These techniques
demonstrate the ability to tackle complex games such as Atari, Go, and other
real-world applications, including financial trading. Nevertheless, a
significant challenge emerges from the lack of interpretability, particularly
when attempting to comprehend the underlying patterns learned, the relative
importance of the state features, and how they are integrated to generate the
policy's output. For this reason, in mission-critical and real-world settings,
it is often preferred to deploy a simpler and more interpretable algorithm,
although at the cost of performance. In this paper, we propose a novel
algorithm, supported by theoretical guarantees, that can extract an
interpretable policy (e.g., a linear policy) without disregarding the
peculiarities of expert behavior. This result is obtained by considering the
advantage function, which includes information about why an action is superior
to the others. In contrast to previous works, our approach enables the training
of an interpretable policy using previously collected experience. The proposed
algorithm is empirically evaluated on classic control environments and on a
financial trading scenario, demonstrating its ability to extract meaningful
information from complex expert policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Benchmarking Foundation Models for Tabular Data With Text <span class="chip">ICML
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Mráz, Breenda Das, Anshul Gupta, Lennart Purucker, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models for tabular data are rapidly evolving, with increasing
interest in extending them to support additional modalities such as free-text
features. However, existing benchmarks for tabular data rarely include textual
columns, and identifying real-world tabular datasets with semantically rich
text features is non-trivial. We propose a series of simple yet effective
ablation-style strategies for incorporating text into conventional tabular
pipelines. Moreover, we benchmark how state-of-the-art tabular foundation
models can handle textual data by manually curating a collection of real-world
tabular datasets with meaningful textual features. Our study is an important
step towards improving benchmarking of foundation models for tabular data with
text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Foundation Models for Structured Data workshop at ICML
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces
  and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Mirzaei, Andreas Maurer, Vladimir R. Kostic, Massimiliano Pontil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from non-independent and non-identically distributed data poses a
persistent challenge in statistical learning. In this study, we introduce
data-dependent Bernstein inequalities tailored for vector-valued processes in
Hilbert space. Our inequalities apply to both stationary and non-stationary
processes and exploit the potential rapid decay of correlations between
temporally separated variables to improve estimation. We demonstrate the
utility of these bounds by applying them to covariance operator estimation in
the Hilbert-Schmidt norm and to operator learning in dynamical systems,
achieving novel risk bounds. Finally, we perform numerical experiments to
illustrate the practical implications of these bounds in both contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In The 28th International Conference on Artificial Intelligence and
  Statistics (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Effect of Instruction Tuning Loss on Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions of the Association for Computational Linguistics (TACL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pay Attention to Attention Distribution: A New Local Lipschitz Bound for
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolay Yudin, Alexander Gaponov, Sergei Kudriashov, Maxim Rakhuba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel local Lipschitz bound for self-attention blocks of
transformers. This bound is based on a refined closed-form expression for the
spectral norm of the softmax function. The resulting bound is not only more
accurate than in the prior art, but also unveils the dependence of the
Lipschitz constant on attention score maps. Based on the new findings, we
suggest an explanation of the way distributions inside the attention map affect
the robustness from the Lipschitz constant perspective. We also introduce a new
lightweight regularization term called JaSMin (Jacobian Softmax norm
Minimization), which boosts the transformer's robustness and decreases local
Lipschitz constants of the whole network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Survival Analysis in Multimodal Medical Data: A Parametric and
  Probabilistic Approach with Competing Risks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alba Garrido, Alejandro Almodóvar, Patricia A. Apellániz, Juan Parras, Santiago Zazo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate survival prediction is critical in oncology for prognosis and
treatment planning. Traditional approaches often rely on a single data
modality, limiting their ability to capture the complexity of tumor biology. To
address this challenge, we introduce a multimodal deep learning framework for
survival analysis capable of modeling both single and competing risks
scenarios, evaluating the impact of integrating multiple medical data sources
on survival predictions. We propose SAMVAE (Survival Analysis Multimodal
Variational Autoencoder), a novel deep learning architecture designed for
survival prediction that integrates six data modalities: clinical variables,
four molecular profiles, and histopathological images. SAMVAE leverages
modality specific encoders to project inputs into a shared latent space,
enabling robust survival prediction while preserving modality specific
information. Its parametric formulation enables the derivation of clinically
meaningful statistics from the output distributions, providing patient-specific
insights through interactive multimedia that contribute to more informed
clinical decision-making and establish a foundation for interpretable,
data-driven survival analysis in oncology. We evaluate SAMVAE on two cancer
cohorts breast cancer and lower grade glioma applying tailored preprocessing,
dimensionality reduction, and hyperparameter optimization. The results
demonstrate the successful integration of multimodal data for both standard
survival analysis and competing risks scenarios across different datasets. Our
model achieves competitive performance compared to state-of-the-art multimodal
survival models. Notably, this is the first parametric multimodal deep learning
architecture to incorporate competing risks while modeling continuous time to a
specific event, using both tabular and image data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Space-Filling Regularization for Robust and Interpretable Nonlinear
  State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hermann Klein, Max Heinz Herkersdorf, Oliver Nelles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state space dynamics representation is the most general approach for
nonlinear systems and often chosen for system identification. During training,
the state trajectory can deform significantly leading to poor data coverage of
the state space. This can cause significant issues for space-oriented training
algorithms which e.g. rely on grid structures, tree partitioning, or similar.
Besides hindering training, significant state trajectory deformations also
deteriorate interpretability and robustness properties. This paper proposes a
new type of space-filling regularization that ensures a favorable data
distribution in state space via introducing a data-distribution-based penalty.
This method is demonstrated in local model network architectures where good
interpretability is a major concern. The proposed approach integrates ideas
from modeling and design of experiments for state space structures. This is why
we present two regularization techniques for the data point distributions of
the state trajectories for local affine state space models. Beyond that, we
demonstrate the results on a widely known system identification benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximation Depth of Convex Polytopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Bakaev, Florestan Brunck, Amir Yehudayoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study approximations of polytopes in the standard model for computing
polytopes using Minkowski sums and (convex hulls of) unions. Specifically, we
study the ability to approximate a target polytope by polytopes of a given
depth. Our main results imply that simplices can only be ``trivially
approximated''. On the way, we obtain a characterization of simplices as the
only ``outer additive'' convex bodies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time
  Training <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseong Jeong, Jegyeong Cho, Youngho Yoon, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizing neural networks to unseen target domains is a significant
challenge in real-world deployments. Test-time training (TTT) addresses this by
using an auxiliary self-supervised task to reduce the domain gap caused by
distribution shifts between the source and target. However, we find that when
models are required to perform multiple tasks under domain shifts, conventional
TTT methods suffer from unsynchronized task behavior, where the adaptation
steps needed for optimal performance in one task may not align with the
requirements of other tasks. To address this, we propose a novel TTT approach
called Synchronizing Tasks for Test-time Training (S4T), which enables the
concurrent handling of multiple tasks. The core idea behind S4T is that
predicting task relations across domain shifts is key to synchronizing tasks
during test time. To validate our approach, we apply S4T to conventional
multi-task benchmarks, integrating it with traditional TTT protocols. Our
empirical results show that S4T outperforms state-of-the-art TTT methods across
various benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Empirical Risk Minimization Framework for Flexible N-Tuples
  Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuying Huang, Junpeng Li, Changchun Hua, Yana Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To alleviate the annotation burden in supervised learning, N-tuples learning
has recently emerged as a powerful weakly-supervised method. While existing
N-tuples learning approaches extend pairwise learning to higher-order
comparisons and accommodate various real-world scenarios, they often rely on
task-specific designs and lack a unified theoretical foundation. In this paper,
we propose a general N-tuples learning framework based on empirical risk
minimization, which systematically integrates pointwise unlabeled data to
enhance learning performance. This paper first unifies the data generation
processes of N-tuples and pointwise unlabeled data under a shared probabilistic
formulation. Based on this unified view, we derive an unbiased empirical risk
estimator that generalizes a broad class of existing N-tuples models. We
further establish a generalization error bound for theoretical support. To
demonstrate the flexibility of the framework, we instantiate it in four
representative weakly supervised scenarios, each recoverable as a special case
of our general model. Additionally, to address overfitting issues arising from
negative risk terms, we adopt correction functions to adjust the empirical
risk. Extensive experiments on benchmark datasets validate the effectiveness of
the proposed framework and demonstrate that leveraging pointwise unlabeled data
consistently improves generalization across various N-tuples learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEAVER: Building Environments with Assessable Variation for Evaluating
  Multi-Objective Reinforcement Learning <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohong Liu, Jack Umenberger, Yize Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen significant advancements in designing reinforcement
learning (RL)-based agents for building energy management. While individual
success is observed in simulated or controlled environments, the scalability of
RL approaches in terms of efficiency and generalization across building
dynamics and operational scenarios remains an open question. In this work, we
formally characterize the generalization space for the cross-environment,
multi-objective building energy management task, and formulate the
multi-objective contextual RL problem. Such a formulation helps understand the
challenges of transferring learned policies across varied operational contexts
such as climate and heat convection dynamics under multiple control objectives
such as comfort level and energy consumption. We provide a principled way to
parameterize such contextual information in realistic building RL environments,
and construct a novel benchmark to facilitate the evaluation of generalizable
RL algorithms in practical building control tasks. Our results show that
existing multi-objective RL methods are capable of achieving reasonable
trade-offs between conflicting objectives. However, their performance degrades
under certain environment variations, underscoring the importance of
incorporating dynamics-dependent contextual information into the policy
learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Workshop on Computational Optimization of Buildings
  (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML
  2025), Vancouver, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIX- Trading Adversarial Fairness via Mixed Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejaswini Medi, Steffen Jung, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Training (AT) is a widely adopted defense against adversarial
examples. However, existing approaches typically apply a uniform training
objective across all classes, overlooking disparities in class-wise
vulnerability. This results in adversarial unfairness: classes with well
distinguishable features (strong classes) tend to become more robust, while
classes with overlapping or shared features(weak classes) remain
disproportionately susceptible to adversarial attacks. We observe that strong
classes do not require strong adversaries during training, as their non-robust
features are quickly suppressed. In contrast, weak classes benefit from
stronger adversaries to effectively reduce their vulnerabilities. Motivated by
this, we introduce TRIX, a feature-aware adversarial training framework that
adaptively assigns weaker targeted adversaries to strong classes, promoting
feature diversity via uniformly sampled targets, and stronger untargeted
adversaries to weak classes, enhancing their focused robustness. TRIX further
incorporates per-class loss weighting and perturbation strength adjustments,
building on prior work, to emphasize weak classes during the optimization.
Comprehensive experiments on standard image classification benchmarks,
including evaluations under strong attacks such as PGD and AutoAttack,
demonstrate that TRIX significantly improves worst-case class accuracy on both
clean and adversarial data, reducing inter-class robustness disparities, and
preserves overall accuracy. Our results highlight TRIX as a practical step
toward fair and effective adversarial defense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed and Decentralised Training: Technical Governance Challenges
  in a Shifting AI Landscape <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Kryś, Yashvardhan Sharma, Janet Egan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in low-communication training algorithms are enabling a shift from
centralised model training to compute setups that are either distributed across
multiple clusters or decentralised via community-driven contributions. This
paper distinguishes these two scenarios - distributed and decentralised
training - which are little understood and often conflated in policy discourse.
We discuss how they could impact technical AI governance through an increased
risk of compute structuring, capability proliferation, and the erosion of
detectability and shutdownability. While these trends foreshadow a possible new
paradigm that could challenge key assumptions of compute governance, we
emphasise that certain policy levers, like export controls, remain relevant. We
also acknowledge potential benefits of decentralised AI, including
privacy-preserving training runs that could unlock access to more data, and
mitigating harmful power concentration. Our goal is to support more precise
policymaking around compute, capability proliferation, and decentralised AI
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as an oral presentation at the Technical AI Governance
  Workshop (ICML 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeheun Jung, Bosung Jung, Suhyun Bae, Donghun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning seeks to remove the influence of particular data or class
from trained models to meet privacy, legal, or ethical requirements. Existing
unlearning methods tend to forget shallowly: phenomenon of an unlearned model
pretend to forget by adjusting only the model response, while its internal
representations retain information sufficiently to restore the forgotten data
or behavior. We empirically confirm the widespread shallowness by reverting the
forgetting effect of various unlearning methods via training-free performance
recovery attack and gradient-inversion-based data reconstruction attack. To
address this vulnerability fundamentally, we define a theoretical criterion of
``deep forgetting'' based on one-point-contraction of feature representations
of data to forget. We also propose an efficient approximation algorithm, and
use it to construct a novel general-purpose unlearning algorithm:
One-Point-Contraction (OPC). Empirical evaluations on image classification
unlearning benchmarks show that OPC achieves not only effective unlearning
performance but also superior resilience against both performance recovery
attack and gradient-inversion attack. The distinctive unlearning performance of
OPC arises from the deep feature forgetting enforced by its theoretical
foundation, and recaps the need for improved robustness of machine unlearning
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Scalable Estimation of Distributional Treatment Effects
  with Multi-Task Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomu Hirata, Undral Byambadalai, Tatsushi Oka, Shota Yasui, Shingo Uto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel multi-task neural network approach for estimating
distributional treatment effects (DTE) in randomized experiments. While DTE
provides more granular insights into the experiment outcomes over conventional
methods focusing on the Average Treatment Effect (ATE), estimating it with
regression adjustment methods presents significant challenges. Specifically,
precision in the distribution tails suffers due to data imbalance, and
computational inefficiencies arise from the need to solve numerous regression
problems, particularly in large-scale datasets commonly encountered in
industry. To address these limitations, our method leverages multi-task neural
networks to estimate conditional outcome distributions while incorporating
monotonic shape constraints and multi-threshold label learning to enhance
accuracy. To demonstrate the practical effectiveness of our proposed method, we
apply our method to both simulated and real-world datasets, including a
randomized field experiment aimed at reducing water consumption in the US and a
large-scale A/B test from a leading streaming platform in Japan. The
experimental results consistently demonstrate superior performance across
various datasets, establishing our method as a robust and practical solution
for modern causal inference applications requiring a detailed understanding of
treatment effect heterogeneity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GuardVal: Dynamic Large Language Model Jailbreak Evaluation for
  Comprehensive Safety Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Zhang, Haibo Jin, Liying Kang, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreak attacks reveal critical vulnerabilities in Large Language Models
(LLMs) by causing them to generate harmful or unethical content. Evaluating
these threats is particularly challenging due to the evolving nature of LLMs
and the sophistication required in effectively probing their vulnerabilities.
Current benchmarks and evaluation methods struggle to fully address these
challenges, leaving gaps in the assessment of LLM vulnerabilities. In this
paper, we review existing jailbreak evaluation practices and identify three
assumed desiderata for an effective jailbreak evaluation protocol. To address
these challenges, we introduce GuardVal, a new evaluation protocol that
dynamically generates and refines jailbreak prompts based on the defender LLM's
state, providing a more accurate assessment of defender LLMs' capacity to
handle safety-critical situations. Moreover, we propose a new optimization
method that prevents stagnation during prompt refinement, ensuring the
generation of increasingly effective jailbreak prompts that expose deeper
weaknesses in the defender LLMs. We apply this protocol to a diverse set of
models, from Mistral-7b to GPT-4, across 10 safety domains. Our findings
highlight distinct behavioral patterns among the models, offering a
comprehensive view of their robustness. Furthermore, our evaluation process
deepens the understanding of LLM behavior, leading to insights that can inform
future research and drive the development of more secure models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Gaussian Mixture Models-based Anomaly Detection for
  under-constrained Cable-Driven Parallel Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julio Garrido, Javier Vales, Diego Silva-Muñiz, Enrique Riveiro, Pablo López-Matencio, Josué Rivera-Andrade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cable-Driven Parallel Robots (CDPRs) are increasingly used for load
manipulation tasks involving predefined toolpaths with intermediate stops. At
each stop, where the platform maintains a fixed pose and the motors keep the
cables under tension, the system must evaluate whether it is safe to proceed by
detecting anomalies that could compromise performance (e.g., wind gusts or
cable impacts). This paper investigates whether anomalies can be detected using
only motor torque data, without additional sensors. It introduces an adaptive,
unsupervised outlier detection algorithm based on Gaussian Mixture Models
(GMMs) to identify anomalies from torque signals. The method starts with a
brief calibration period, just a few seconds, during which a GMM is fit on
known anomaly-free data. Real-time torque measurements are then evaluated using
Mahalanobis distance from the GMM, with statistically derived thresholds
triggering anomaly flags. Model parameters are periodically updated using the
latest segments identified as anomaly-free to adapt to changing conditions.
Validation includes 14 long-duration test sessions simulating varied wind
intensities. The proposed method achieves a 100% true positive rate and 95.4%
average true negative rate, with 1-second detection latency. Comparative
evaluation against power threshold and non-adaptive GMM methods indicates
higher robustness to drift and environmental variation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing the Past and Present: A Coordinated Replay Framework for
  Federated Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Qi, Lei Meng, Han Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Class Incremental Learning (FCIL) aims to collaboratively process
continuously increasing incoming tasks across multiple clients. Among various
approaches, data replay has become a promising solution, which can alleviate
forgetting by reintroducing representative samples from previous tasks.
However, their performance is typically limited by class imbalance, both within
the replay buffer due to limited global awareness and between replayed and
newly arrived classes. To address this issue, we propose a class wise balancing
data replay method for FCIL (FedCBDR), which employs a global coordination
mechanism for class-level memory construction and reweights the learning
objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has
two key components: 1) the global-perspective data replay module reconstructs
global representations of prior task in a privacy-preserving manner, which then
guides a class-aware and importance-sensitive sampling strategy to achieve
balanced replay; 2) Subsequently, to handle class imbalance across tasks, the
task aware temperature scaling module adaptively adjusts the temperature of
logits at both class and instance levels based on task dynamics, which reduces
the model's overconfidence in majority classes while enhancing its sensitivity
to minority classes. Experimental results verified that FedCBDR achieves
balanced class-wise sampling under heterogeneous data distributions and
improves generalization under task imbalance between earlier and recent tasks,
yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shin'ya Yamaguchi, Kosuke Nishida, Daiki Chijiwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have demonstrated remarkable
capabilities by integrating pre-trained vision encoders with large language
models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting
has been adapted for LVLMs to enhance multi-modal reasoning by generating
intermediate rationales based on visual and textual inputs. While CoT is
assumed to improve grounding and accuracy in LVLMs, our experiments reveal a
key challenge: existing LVLMs often ignore the contents of generated rationales
in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as
a KL-constrained reward maximization focused on rationale-conditional
log-likelihood. As the optimal solution, we propose rationale-enhanced decoding
(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes
visual and rationale information by multiplying distinct image-conditional and
rationale-conditional next token distributions. Extensive experiments show that
RED consistently and significantly improves reasoning over standard CoT and
other decoding methods across multiple benchmarks and LVLMs. Our work offers a
practical and effective approach to improve both the faithfulness and accuracy
of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded
multi-modal systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Transposed Convolutions on FPGA-based Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jude Haris, José Cano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 35th International Conference on Field-Programmable Logic
  and Applications (FPL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Some Theoretical Results on Layerwise Effective Dimension Oscillations
  in Finite Width ReLU Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshan Makwana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the layerwise effective dimension (rank of the feature matrix) in
fully-connected ReLU networks of finite width. Specifically, for a fixed batch
of $m$ inputs and random Gaussian weights, we derive closed-form expressions
for the expected rank of the \$m\times n\$ hidden activation matrices. Our main
result shows that $\mathbb{E}[EDim(\ell)]=m[1-(1-2/\pi)^\ell]+O(e^{-c m})$ so
that the rank deficit decays geometrically with ratio $1-2 / \pi \approx
0.3634$. We also prove a sub-Gaussian concentration bound, and identify the
"revival" depths at which the expected rank attains local maxima. In
particular, these peaks occur at depths
$\ell_k^*\approx(k+1/2)\pi/\log(1/\rho)$ with height $\approx (1-e^{-\pi/2}) m
\approx 0.79m$. We further show that this oscillatory rank behavior is a
finite-width phenomenon: under orthogonal weight initialization or strong
negative-slope leaky-ReLU, the rank remains (nearly) full. These results
provide a precise characterization of how random ReLU layers alternately
collapse and partially revive the subspace of input variations, adding nuance
to prior work on expressivity of deep networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Pole Structures of Hadronic States using Predictive Uncertainty
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Frohnert, Denny Lane B. Sombrillo, Evert van Nieuwenburg, Patrick Emonts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching theoretical predictions to experimental data remains a central
challenge in hadron spectroscopy. In particular, the identification of new
hadronic states is difficult, as exotic signals near threshold can arise from a
variety of physical mechanisms. A key diagnostic in this context is the pole
structure of the scattering amplitude, but different configurations can produce
similar signatures. The mapping between pole configurations and line shapes is
especially ambiguous near the mass threshold, where analytic control is
limited. In this work, we introduce an uncertainty-aware machine learning
approach for classifying pole structures in $S$-matrix elements. Our method is
based on an ensemble of classifier chains that provide both epistemic and
aleatoric uncertainty estimates. We apply a rejection criterion based on
predictive uncertainty, achieving a validation accuracy of nearly $95\%$ while
discarding only a small fraction of high-uncertainty predictions. Trained on
synthetic data with known pole structures, the model generalizes to previously
unseen experimental data, including enhancements associated with the
$P_{c\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole
structure, representing the presence of a genuine compact pentaquark in the
presence of a higher channel virtual state pole with non-vanishing width. While
evaluated on this particular state, our framework is broadly applicable to
other candidate hadronic states and offers a scalable tool for pole structure
inference in scattering amplitudes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning-Assisted Surrogate Modeling with Multi-Objective
  Optimization and Decision-Making of a Steam Methane Reforming Reactor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Reza Nabavi, Zonglin Guo, Zhiyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents an integrated modeling and optimization framework for a
steam methane reforming (SMR) reactor, combining a mathematical model,
artificial neural network (ANN)-based hybrid modeling, advanced multi-objective
optimization (MOO) and multi-criteria decision-making (MCDM) techniques. A
one-dimensional fixed-bed reactor model accounting for internal mass transfer
resistance was employed to simulate reactor performance. To reduce the high
computational cost of the mathematical model, a hybrid ANN surrogate was
constructed, achieving a 93.8% reduction in average simulation time while
maintaining high predictive accuracy. The hybrid model was then embedded into
three MOO scenarios using the non-dominated sorting genetic algorithm II
(NSGA-II) solver: 1) maximizing methane conversion and hydrogen output; 2)
maximizing hydrogen output while minimizing carbon dioxide emissions; and 3) a
combined three-objective case. The optimal trade-off solutions were further
ranked and selected using two MCDM methods: technique for order of preference
by similarity to ideal solution (TOPSIS) and simplified preference ranking on
the basis of ideal-average distance (sPROBID). Optimal results include a
methane conversion of 0.863 with 4.556 mol/s hydrogen output in the first case,
and 0.988 methane conversion with 3.335 mol/s hydrogen and 0.781 mol/s carbon
dioxide in the third. This comprehensive methodology offers a scalable and
effective strategy for optimizing complex catalytic reactor systems with
multiple, often conflicting, objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on
  Hype<span class="highlight-title">rl</span>edger Fabric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Beis Penedo, Rebeca P. Díaz Redondo, Ana Fernández Vilas, Manuel Fernández Veiga, Francisco Troncoso Pastoriza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative machine learning in sensitive domains demands scalable, privacy
preserving solutions for enterprise deployment. Conventional Federated Learning
(FL) relies on a central server, introducing single points of failure and
privacy risks, while Split Learning (SL) partitions models for privacy but
scales poorly due to sequential training. We present a decentralized
architecture that combines Federated Split Learning (FSL) with the permissioned
blockchain Hyperledger Fabric (HLF). Our chaincode orchestrates FSL's split
model execution and peer-to-peer aggregation without any central coordinator,
leveraging HLF's transient fields and Private Data Collections (PDCs) to keep
raw data and model activations private. On CIFAR-10 and MNIST benchmarks,
HLF-FSL matches centralized FSL accuracy while reducing per epoch training time
compared to Ethereum-based works. Performance and scalability tests show
minimal blockchain overhead and preserved accuracy, demonstrating enterprise
grade viability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Limits of Model Compression in LLMs: A Knowledge
  Distillation Study on QA Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joyeeta Datta, Niclas Doll, Qusai Ramadan, Zeyd Boukhers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted four publication at the 26th Meeting of the Special Interest
  on Discourse and Dialogue</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concentration of measure for non-linear random matrices with
  applications to neural networks and non-commutative polynomials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radosław Adamczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove concentration inequalities for several models of non-linear random
matrices. As corollaries we obtain estimates for linear spectral statistics of
the conjugate kernel of neural networks and non-commutative polynomials in
(possibly dependent) random matrices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransformEEG: Towards Improving Model Generalizability in Deep
  Learning-based EEG Parkinson's Disease Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Del Pup, Riccardo Brun, Filippo Iotti, Edoardo Paccagnella, Mattia Pezzato, Sabrina Bertozzo, Andrea Zanola, Louis Fabrice Tshimanga, Henning Müller, Manfredo Atzori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) is establishing itself as an important,
low-cost, noninvasive diagnostic tool for the early detection of Parkinson's
Disease (PD). In this context, EEG-based Deep Learning (DL) models have shown
promising results due to their ability to discover highly nonlinear patterns
within the signal. However, current state-of-the-art DL models suffer from poor
generalizability caused by high inter-subject variability. This high
variability underscores the need for enhancing model generalizability by
developing new architectures better tailored to EEG data. This paper introduces
TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's
disease detection using EEG data. Unlike transformer models based on the EEGNet
structure, TransformEEG incorporates a depthwise convolutional tokenizer. This
tokenizer is specialized in generating tokens composed by channel-specific
features, which enables more effective feature mixing within the self-attention
layers of the transformer encoder. To evaluate the proposed model, four public
datasets comprising 290 subjects (140 PD patients, 150 healthy controls) were
harmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out
(N-LNSO) cross-validation was performed to provide an unbiased comparison
against seven other consolidated EEG deep learning models. TransformEEG
achieved the highest balanced accuracy's median (78.45%) as well as the lowest
interquartile range (6.37%) across all the N-LNSO partitions. When combined
with data augmentation and threshold correction, median accuracy increased to
80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG
produces more consistent and less skewed results. It demonstrates a substantial
reduction in variability and more reliable PD detection using EEG data compared
to the other investigated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for possible publication. GitHub repository: see
  https://github.com/MedMaxLab/transformeeg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Causal Discovery with Generative Intervention for Unsupervised
  Graph Domain Adaptation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Luo, Yuhao Tang, Yiwei Fu, Xiao Luo, Zhizhuo Kou, Zhiping Xiao, Wei Ju, Wentao Zhang, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain
graphs to achieve effective performance in unlabeled target domains despite
distribution shifts. However, existing methods often yield suboptimal results
due to the entanglement of causal-spurious features and the failure of global
alignment strategies. We propose SLOGAN (Sparse Causal Discovery with
Generative Intervention), a novel approach that achieves stable graph
representation transfer through sparse causal modeling and dynamic intervention
mechanisms. Specifically, SLOGAN first constructs a sparse causal graph
structure, leveraging mutual information bottleneck constraints to disentangle
sparse, stable causal features while compressing domain-dependent spurious
correlations through variational inference. To address residual spurious
correlations, we innovatively design a generative intervention mechanism that
breaks local spurious couplings through cross-domain feature recombination
while maintaining causal feature semantic consistency via covariance
constraints. Furthermore, to mitigate error accumulation in target domain
pseudo-labels, we introduce a category-adaptive dynamic calibration strategy,
ensuring stable discriminative learning. Extensive experiments on multiple
real-world datasets demonstrate that SLOGAN significantly outperforms existing
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Self-Federated Learning for Energy Efficient Cooperative
  Intelligence in Society 5.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Domini, Laura Erhan, Gianluca Aguzzi, Lucia Cavallaro, Amirhossein Douzandeh Zenoozi, Antonio Liotta, Mirko Viroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning offers privacy-preserving collaborative intelligence but
struggles to meet the sustainability demands of emerging IoT ecosystems
necessary for Society 5.0-a human-centered technological future balancing
social advancement with environmental responsibility. The excessive
communication bandwidth and computational resources required by traditional FL
approaches make them environmentally unsustainable at scale, creating a
fundamental conflict with green AI principles as billions of
resource-constrained devices attempt to participate. To this end, we introduce
Sparse Proximity-based Self-Federated Learning (SParSeFuL), a resource-aware
approach that bridges this gap by combining aggregate computing for
self-organization with neural network sparsification to reduce energy and
bandwidth consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic MC via Biological Transmitters: Therapeutic Modulation of the
  Gut-Brain Axis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Lotter, Elisabeth Mohr, Andrina Rutsch, Lukas Brand, Francesca Ronchi, Laura Díaz-Marugán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic molecular communication (SMC) is a key enabler for future
healthcare systems in which Internet of Bio-Nano-Things (IoBNT) devices
facilitate the continuous monitoring of a patient's biochemical signals. To
close the loop between sensing and actuation, both the detection and the
generation of in-body molecular communication (MC) signals is key. However,
generating signals inside the human body, e.g., via synthetic nanodevices,
poses a challenge in SMC, due to technological obstacles as well as legal,
safety, and ethical issues. Hence, this paper considers an SMC system in which
signals are generated indirectly via the modulation of a natural in-body MC
system, namely the gut-brain axis (GBA). Therapeutic GBA modulation is already
established as treatment for neurological diseases, e.g., drug refractory
epilepsy (DRE), and performed via the administration of nutritional supplements
or specific diets. However, the molecular signaling pathways that mediate the
effect of such treatments are mostly unknown. Consequently, existing treatments
are standardized or designed heuristically and able to help only some patients
while failing to help others. In this paper, we propose to leverage personal
health data, e.g., gathered by in-body IoBNT devices, to design more versatile
and robust GBA modulation-based treatments as compared to the existing ones. To
show the feasibility of our approach, we define a catalog of theoretical
requirements for therapeutic GBA modulation. Then, we propose a machine
learning model to verify these requirements for practical scenarios when only
limited data on the GBA modulation exists. By evaluating the proposed model on
several datasets, we confirm its excellent accuracy in identifying different
modulators of the GBA. Finally, we utilize the proposed model to identify
specific modulatory pathways that play an important role for therapeutic GBA
modulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework
  Using Wearable Sensor Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpana Sinhal, Anay Sinhal, Amit Sinhal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Discrete <span class="highlight-title">Diffusion</span> Beats Autoregressive Perplexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cooper Doyle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KinDEL: DNA-Encoded Library <span class="highlight-title">Dataset</span> for Kinase Inhibitors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benson Chen, Tomasz Danel, Gabriel H. S. Dreiman, Patrick J. McEnaney, Nikhil Jain, Kirill Novikov, Spurti Umesh Akki, Joshua L. Turnbull, Virja Atul Pandya, Boris P. Belotserkovskii, Jared Bryce Weaver, Ankita Biswas, Dat Nguyen, Kent Gorday, Mohammad Sultan, Nathaniel Stanley, Daniel M Whalen, Divya Kanichar, Christoph Klein, Emily Fox, R. Edward Watts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DNA-Encoded Libraries (DELs) represent a transformative technology in drug
discovery, facilitating the high-throughput exploration of vast chemical
spaces. Despite their potential, the scarcity of publicly available DEL
datasets presents a bottleneck for the advancement of machine learning
methodologies in this domain. To address this gap, we introduce KinDEL, one of
the largest publicly accessible DEL datasets and the first one that includes
binding poses from molecular docking experiments. Focused on two kinases,
Mitogen-Activated Protein Kinase 14 (MAPK14) and Discoidin Domain Receptor
Tyrosine Kinase 1 (DDR1), KinDEL includes 81 million compounds, offering a rich
resource for computational exploration. Additionally, we provide comprehensive
biophysical assay validation data, encompassing both on-DNA and off-DNA
measurements, which we use to evaluate a suite of machine learning techniques,
including novel structure-based probabilistic models. We hope that our
benchmark, encompassing both 2D and 3D structures, will help advance the
development of machine learning models for data-driven hit identification using
DELs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watermarking Degrades Alignment in Language Models: Analysis and
  Mitigation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apurv Verma, NhatHai Phan, Shubhendu Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 1st Workshop on GenAI Watermarking, collocated with
  ICLR 2025. OpenReview: https://openreview.net/forum?id=SIBkIV48gF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BarcodeBERT: <span class="highlight-title">Transformer</span>s for Biodiversity Analysis <span class="chip">NeurIPS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02401v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02401v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Millan Arias, Niousha Sadjadi, Monireh Safari, ZeMing Gong, Austin T. Wang, Joakim Bruslund Haurum, Iuliia Zarubiieva, Dirk Steinke, Lila Kari, Angel X. Chang, Scott C. Lowe, Graham W. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the global challenge of understanding and characterizing biodiversity,
short species-specific genomic sequences known as DNA barcodes play a critical
role, enabling fine-grained comparisons among organisms within the same kingdom
of life. Although machine learning algorithms specifically designed for the
analysis of DNA barcodes are becoming more popular, most existing methodologies
rely on generic supervised training algorithms. We introduce BarcodeBERT, a
family of models tailored to biodiversity analysis and trained exclusively on
data from a reference library of 1.5M invertebrate DNA barcodes. We compared
the performance of BarcodeBERT on taxonomic identification tasks against a
spectrum of machine learning approaches including supervised training of
classical neural architectures and fine-tuning of general DNA foundation
models. Our self-supervised pretraining strategies on domain-specific data
outperform fine-tuned foundation models, especially in identification tasks
involving lower taxa such as genera and species. We also compared BarcodeBERT
with BLAST, one of the most widely used bioinformatics tools for sequence
searching, and found that our method matched BLAST's performance in
species-level classification while being 55 times faster. Our analysis of
masking and tokenization strategies also provides practical guidance for
building customized DNA language models, emphasizing the importance of aligning
model training strategies with dataset characteristics and domain knowledge.
The code repository is available at https://github.com/bioscan-ml/BarcodeBERT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main text: 14 pages, Total: 23 pages, 10 figures, formerly accepted
  at the 4th Workshop on Self-Supervised Learning: Theory and Practice (NeurIPS
  2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theory of Inference Compute Scaling: Reasoning through Directed
  Stochastic Skill Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00004v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00004v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin R. Ellis-Mohr, Anuj K. Nayak, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No $D_{\text{train}}$: Model-Agnostic Counterfactual Explanations Using
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Sun, Raquel Aoki, Kevin H. Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) methods have experienced significant growth in the past
decade, yet their practical application in high-impact real-world domains has
been hindered by their opacity. When ML methods are responsible for making
critical decisions, stakeholders often require insights into how to alter these
decisions. Counterfactual explanations (CFEs) have emerged as a solution,
offering interpretations of opaque ML models and providing a pathway to
transition from one decision to another. However, most existing CFE methods
require access to the model's training dataset, few methods can handle
multivariate time-series, and none of model-agnostic CFE methods can handle
multivariate time-series without training datasets. These limitations can be
formidable in many scenarios. In this paper, we present NTD-CFE, a novel
model-agnostic CFE method based on reinforcement learning (RL) that generates
CFEs when training datasets are unavailable. NTD-CFE is suitable for both
static and multivariate time-series datasets with continuous and discrete
features. NTD-CFE reduces the CFE search space from a multivariate time-series
domain to a lower dimensional space and addresses the problem using RL. Users
have the flexibility to specify non-actionable, immutable, and preferred
features, as well as causal constraints. We demonstrate the performance of
NTD-CFE against four baselines on several datasets and find that, despite not
having access to a training dataset, NTD-CFE finds CFEs that make significantly
fewer and significantly smaller changes to the input time-series. These
properties make CFEs more actionable, as the magnitude of change required to
alter an outcome is vastly reduced. The code is available in the supplementary
material.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling Imbalanced Data with Multi-objective Bilevel Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Medlin, Sven Leyffer, Krishnan Raghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-class classification problems are often characterized by an imbalance
between the number of majority and minority datapoints resulting in poor
classification of the minority class in particular. Traditional approaches,
such as reweighting the loss function or na\"ive resampling, risk overfitting
and subsequently fail to improve classification because they do not consider
the diversity between majority and minority datasets. Such consideration is
infeasible because there is no metric that can measure the impact of imbalance
on the model. To obviate these challenges, we make two key contributions.
First, we introduce MOODS~(Multi-Objective Optimization for Data Sampling), a
novel multi-objective bilevel optimization framework that guides both synthetic
oversampling and majority undersampling. Second, we introduce a validation
metric -- `$\epsilon/ \delta$ non-overlapping diversification metric' -- that
quantifies the goodness of a sampling method towards model performance. With
this metric we experimentally demonstrate state-of-the-art performance with
improvement in diversity driving a $1-15 \%$ increase in $F1$ scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Problem Parameter Transfer in Quantum Approximate Optimization
  Algorithm: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien X. Nguyen, Bao Bach, Ilya Safro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Approximate Optimization Algorithm (QAOA) is one of the most
promising candidates to achieve the quantum advantage in solving combinatorial
optimization problems. The process of finding a good set of variational
parameters in the QAOA circuit has proven to be challenging due to multiple
factors, such as barren plateaus. As a result, there is growing interest in
exploiting parameter transferability, where parameter sets optimized for one
problem instance are transferred to another that could be more complex either
to estimate the solution or to serve as a warm start for further optimization.
But can we transfer parameters from one class of problems to another?
Leveraging parameter sets learned from a well-studied class of problems could
help navigate the less studied one, reducing optimization overhead and
mitigating performance pitfalls. In this paper, we study whether pretrained
QAOA parameters of MaxCut can be used as is or to warm start the Maximum
Independent Set (MIS) circuits. Specifically, we design machine learning models
to find good donor candidates optimized on MaxCut and apply their parameters to
MIS acceptors. Our experimental results show that such parameter transfer can
significantly reduce the number of optimization iterations required while
achieving comparable approximation ratios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Image Modeling: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Hondru, Florinel Alin Croitoru, Shervin Minaee, Radu Tudor Ionescu, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we survey recent studies on masked image modeling (MIM), an
approach that emerged as a powerful self-supervised learning technique in
computer vision. The MIM task involves masking some information, e.g. pixels,
patches, or even latent representations, and training a model, usually an
autoencoder, to predicting the missing information by using the context
available in the visible part of the input. We identify and formalize two
categories of approaches on how to implement MIM as a pretext task, one based
on reconstruction and one based on contrastive learning. Then, we construct a
taxonomy and review the most prominent papers in recent years. We complement
the manually constructed taxonomy with a dendrogram obtained by applying a
hierarchical clustering algorithm. We further identify relevant clusters via
manually inspecting the resulting dendrogram. Our review also includes datasets
that are commonly used in MIM research. We aggregate the performance results of
various masked image modeling methods on the most popular datasets, to
facilitate the comparison of competing methods. Finally, we identify research
gaps and propose several interesting directions of future work. We supplement
our survey with the following public repository containing organized
references: https://github.com/vladhondru25/MIM-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bilevel Optimization Framework for Imbalanced Data Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Medlin, Sven Leyffer, Krishnan Raghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data rebalancing techniques, including oversampling and undersampling, are a
common approach to addressing the challenges of imbalanced data. To tackle
unresolved problems related to both oversampling and undersampling, we propose
a new undersampling approach that: (i) avoids the pitfalls of noise and overlap
caused by synthetic data and (ii) avoids the pitfall of under-fitting caused by
random undersampling. Instead of undersampling majority data randomly, our
method undersamples datapoints based on their ability to improve model loss.
Using improved model loss as a proxy measurement for classification
performance, our technique assesses a datapoint's impact on loss and rejects
those unable to improve it. In so doing, our approach rejects majority
datapoints redundant to datapoints already accepted and, thereby, finds an
optimal subset of majority training data for classification. The accept/reject
component of our algorithm is motivated by a bilevel optimization problem
uniquely formulated to identify the optimal training set we seek. Experimental
results show our proposed technique with F1 scores up to 10% higher than
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Has a Foundation Model Found? Using Inductive Bias to Probe for
  Wo<span class="highlight-title">rl</span>d Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyon Vafa, Peter G. Chang, Ashesh Rambachan, Sendhil Mullainathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Uncertainty Quantification for Depression Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghong Li, Xiuzhuang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthy depression prediction based on deep learning, incorporating both
predictive reliability and algorithmic fairness across diverse demographic
groups, is crucial for clinical application. Recently, achieving reliable
depression predictions through uncertainty quantification has attracted
increasing attention. However, few studies have focused on the fairness of
uncertainty quantification (UQ) in depression prediction. In this work, we
investigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage
(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for
depression prediction. FUQ pursues reliable and fair depression predictions
through group-based analysis. Specifically, we first group all the participants
by different sensitive attributes and leverage conformal prediction to quantify
uncertainty within each demographic group, which provides a theoretically
guaranteed and valid way to quantify uncertainty for depression prediction and
facilitates the investigation of fairness across different demographic groups.
Furthermore, we propose a fairness-aware optimization strategy that formulates
fairness as a constrained optimization problem under EOC constraints. This
enables the model to preserve predictive reliability while adapting to the
heterogeneous uncertainty levels across demographic groups, thereby achieving
optimal fairness. Through extensive evaluations on several visual and audio
depression datasets, our approach demonstrates its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parametric Scaling Law of Tuning Bias in Conformal Prediction <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zeng, Kangdao Liu, Bingyi Jing, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction is a popular framework of uncertainty quantification
that constructs prediction sets with coverage guarantees. To uphold the
exchangeability assumption, many conformal prediction methods necessitate an
additional holdout set for parameter tuning. Yet, the impact of violating this
principle on coverage remains underexplored, making it ambiguous in practical
applications. In this work, we empirically find that the tuning bias - the
coverage gap introduced by leveraging the same dataset for tuning and
calibration, is negligible for simple parameter tuning in many conformal
prediction methods. In particular, we observe the scaling law of the tuning
bias: this bias increases with parameter space complexity and decreases with
calibration set size. Formally, we establish a theoretical framework to
quantify the tuning bias and provide rigorous proof for the scaling law of the
tuning bias by deriving its upper bound. In the end, we discuss how to reduce
the tuning bias, guided by the theories we developed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025: https://icml.cc/virtual/2025/poster/44287 and code at:
  https://github.com/ml-stat-Sustech/Parametric-Scaling-Law-CP-Tuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient
  GPU Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiang Shi, Colin Cai, Junjia Du, Zhanda Zhu, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current prefill-decode (PD) disaggregation is typically deployed at the level
of entire serving engines, assigning separate GPUs to handle prefill and decode
phases. While effective at reducing latency, this approach demands more
hardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode
requests within the same batch, but introduces phase interference between
prefill and decode.
  While existing PD disaggregation solutions separate the phases across GPUs,
we ask: can the same decoupling be achieved within a single serving engine? The
key challenge lies in managing the conflicting resource requirements of prefill
and decode when they share the same hardware. In this paper, we first show that
chunked prefill requests cause interference with decode requests due to their
distinct requirements for GPU resources. Second, we find that GPU resources
exhibit diminishing returns. Beyond a saturation point, increasing GPU
allocation yields negligible latency improvements. This insight enables us to
split a single GPU's resources and dynamically allocate them to prefill and
decode on the fly, effectively disaggregating the two phases within the same
GPU.
  Across a range of models and workloads, our system Nexus achieves up to 2.2x
higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also
outperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x
lower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using
only half the number of GPUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Studying and Improving Graph Neural Network-based Motif Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro C. Vieira, Miguel E. P. Silva, Pedro Manuel Pinto Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript represents a revised version from the paper on
  https://openreview.net/forum?id=PZVVOeu6xx. Still a work in progress.
  Comments are welcome! 23 pages (12 main text + references), 9 figures, 5
  tables. (Second update: More accurate Table 4, Run time comparisons.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Predictability of Performative, Social Events <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan C. Perdomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social predictions do not passively describe the future; they actively shape
it. They inform actions and change individual expectations in ways that
influence the likelihood of the predicted outcome. Given these dynamics, to
what extent can social events be predicted? This question was discussed
throughout the 20th century by authors like Merton, Morgenstern, Simon, and
others who considered it a central issue in social science methodology. In this
work, we provide a modern answer to this old problem. Using recent ideas from
performative prediction and outcome indistinguishability, we establish that one
can always efficiently predict social events accurately, regardless of how
predictions influence data. While achievable, we also show that these
predictions are often undesirable, highlighting the limitations of previous
desiderata. We end with a discussion of various avenues forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A
  Lightweight Benchmark for Probing Foundational Controllability Components <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Potham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. This paper introduces a lightweight,
interpretable benchmark to evaluate an LLM agent's ability to uphold a
high-level safety principle when faced with conflicting task instructions. Our
evaluation of six LLMs reveals two primary findings: (1) a quantifiable "cost
of compliance" where safety constraints degrade task performance even when
compliant solutions exist, and (2) an "illusion of compliance" where high
adherence often masks task incompetence rather than principled choice. These
findings provide initial evidence that while LLMs can be influenced by
hierarchical directives, current approaches lack the consistency required for
reliable safety governance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This work has been submitted to the Technical AI Governance
  Workshop at ICML 2025 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Morphological Tree Tokenizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Zhu, Xiang Hu, Pengyu Ji, Wei Wu, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a cornerstone in language modeling, tokenization involves segmenting text
inputs into pre-defined atomic units. Conventional statistical tokenizers often
disrupt constituent boundaries within words, thereby corrupting semantic
information. To address this drawback, we introduce morphological structure
guidance to tokenization and propose a deep model to induce character-level
structures of words. Specifically, the deep model jointly encodes internal
structures and representations of words with a mechanism named
$\textit{MorphOverriding}$ to ensure the indecomposability of morphemes. By
training the model with self-supervised objectives, our method is capable of
inducing character-level structures that align with morphological rules without
annotated training data. Based on the induced structures, our algorithm
tokenizes words through vocabulary matching in a top-down manner. Empirical
results indicate that the proposed method effectively retains complete
morphemes and outperforms widely adopted methods such as BPE and WordPiece on
both morphological segmentation tasks and language modeling tasks. Code is
available at https://github.com/martianmartina/TreeTokenizer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical physics analysis of graph neural networks: Approaching
  optimality in the contextual stochastic block model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        O. Duranthon, L. Zdeborová
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are designed to process data associated with
graphs. They are finding an increasing range of applications; however, as with
other modern machine learning techniques, their theoretical understanding is
limited. GNNs can encounter difficulties in gathering information from nodes
that are far apart by iterated aggregation steps. This situation is partly
caused by so-called oversmoothing; and overcoming it is one of the practically
motivated challenges. We consider the situation where information is aggregated
by multiple steps of convolution, leading to graph convolutional networks
(GCNs). We analyze the generalization performance of a basic GCN, trained for
node classification on data generated by the contextual stochastic block model.
We predict its asymptotic performance by deriving the free energy of the
problem, using the replica method, in the high-dimensional limit. Calling depth
the number of convolutional steps, we show the importance of going to large
depth to approach the Bayes-optimality. We detail how the architecture of the
GCN has to scale with the depth to avoid oversmoothing. The resulting large
depth limit can be close to the Bayes-optimality and leads to a continuous GCN.
Technically, we tackle this continuous limit via an approach that resembles
dynamical mean-field theory (DMFT) with constraints at the initial and final
times. An expansion around large regularization allows us to solve the
corresponding equations for the performance of the deep GCN. This promising
tool may contribute to the analysis of further deep neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Symmetry Breaking in Physical Systems with Relaxed Group
  Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02299v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02299v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Wang, Elyssa Hofgard, Han Gao, Robin Walters, Tess E. Smidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling symmetry breaking is essential for understanding the fundamental
changes in the behaviors and properties of physical systems, from microscopic
particle interactions to macroscopic phenomena like fluid dynamics and cosmic
structures. Thus, identifying sources of asymmetry is an important tool for
understanding physical systems. In this paper, we focus on learning asymmetries
of data using relaxed group convolutions. We provide both theoretical and
empirical evidence that this flexible convolution technique allows the model to
maintain the highest level of equivariance that is consistent with data and
discover the subtle symmetry-breaking factors in various physical systems. We
employ various relaxed group convolution architectures to uncover various
symmetry-breaking factors that are interpretable and physically meaningful in
different physical systems, including the phase transition of crystal
structure, the isotropy and homogeneity breaking in turbulent flow, and the
time-reversal symmetry breaking in pendulum systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAEBE: Multi-Agent Emergent Behavior Framework <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sinem Erisken, Timothy Gothard, Martin Leitgab, Ram Potham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This work has been submitted to the Multi-Agent Systems
  Workshop at ICML 2025 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Algorithm for Learning Smaller Representations of Models With Scarce
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.07990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.07990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian de Wynter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm for solving binary classification problems when the
dataset is not fully representative of the problem being solved, and obtaining
more data is not possible. It relies on a trained model with loose accuracy
constraints, an iterative hyperparameter searching-and-pruning procedure over a
search space $\Theta$, and a data-generating function. Our algorithm works by
reconstructing up to homology the manifold on which lies the support of the
underlying distribution. We provide an analysis on correctness and runtime
complexity under ideal conditions and an extension to deep neural networks. In
the former case, if $\size{\Theta}$ is the number of hyperparameter sets in the
search space, this algorithm returns a solution that is up to $2(1 -
{2^{-\size{\Theta}}})$ times better than simply training with an enumeration of
$\Theta$ and picking the best model. As part of our analysis we also prove that
an open cover of a dataset has the same homology as the manifold on which lies
the support of the underlying probability distribution, if and only said
dataset is learnable. This latter result acts as a formal argument to explain
the effectiveness of data expansion techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Information Geometry--see the journal for the final,
  authenticated version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "I am bad": Interpreting Stealthy, Universal and Robust Audio Jailbreaks
  in Audio-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Gupta, David Khachaturov, Robert Mullins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of multimodal large language models has introduced innovative
human-machine interaction paradigms but also significant challenges in machine
learning safety. Audio-Language Models (ALMs) are especially relevant due to
the intuitive nature of spoken communication, yet little is known about their
failure modes. This paper explores audio jailbreaks targeting ALMs, focusing on
their ability to bypass alignment mechanisms. We construct adversarial
perturbations that generalize across prompts, tasks, and even base audio
samples, demonstrating the first universal jailbreaks in the audio modality,
and show that these remain effective in simulated real-world conditions. Beyond
demonstrating attack feasibility, we analyze how ALMs interpret these audio
adversarial examples and reveal them to encode imperceptible first-person toxic
speech - suggesting that the most effective perturbations for eliciting toxic
outputs specifically embed linguistic features within the audio signal. These
results have important implications for understanding the interactions between
different modalities in multimodal models, and offer actionable insights for
enhancing defenses against adversarial audio attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Bandits in Payment Processing: Non-uniform Exploration and
  Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhila Vangara, Alex Egg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uniform random exploration in decision-making systems supports off-policy
learning via supervision but incurs high regret, making it impractical for many
applications. Conversely, non-uniform exploration offers better immediate
performance but lacks support for off-policy learning. Recent research suggests
that regression oracles can bridge this gap by combining non-uniform
exploration with supervised learning. In this paper, we analyze these
approaches within a real-world industrial context at Adyen, a large global
payments processor characterized by batch logged delayed feedback, short-term
memory, and dynamic action spaces under the Empirical Risk Minimization (ERM)
framework. Our analysis reveals that while regression oracles significantly
improve performance, they introduce challenges due to rigid algorithmic
assumptions. Specifically, we observe that as a policy improves, subsequent
generations may perform worse due to shifts in the reward distribution and
increased class imbalance in the training data. This degradation occurs de
spite improvements in other aspects of the training data, leading to decreased
performance in successive policy iterations. We further explore the long-term
impact of regression oracles, identifying a potential "oscillation effect."
This effect arises when regression oracles influence probability estimates and
the realizability of subsequent policy models, leading to fluctuations in
performance across iterations. Our findings highlight the need for more
adaptable algorithms that can leverage the benefits of regression oracles
without introducing instability in policy performance over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 10 figures, submitted to KDD '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Chain-of-Thought in LLMs through Information Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Francois Ton, Muhammad Faaiz Taufiq, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive performance in complex
reasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing
models to break down problems into manageable sub-tasks. However, existing CoT
evaluation techniques either require annotated CoT data or fall short in
accurately assessing intermediate reasoning steps, leading to high rates of
false positives. In this paper, we formalize CoT reasoning in LLMs through an
information-theoretic lens. Specifically, our framework quantifies the
`information-gain' at each reasoning step, enabling the identification of
failure modes in LLMs without the need for expensive annotated datasets. We
demonstrate the efficacy of our approach through extensive experiments on toy
arithmetic, GSM8K and PRM800k datasets, where it significantly outperforms
existing outcome-based methods by providing more accurate insights into model
performance on individual subtasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Automata Learning via Discrete Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lutz, Daniil Kaminskyi, Florian Wittbold, Simon Dierl, Falk Howar, Barbara König, Emmanuel Müller, Daniel Neider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automata learning is a successful tool for many application domains such as
robotics and automatic verification. Typically, automata learning techniques
operate in a supervised learning setting (active or passive) where they learn a
finite state machine in contexts where additional information, such as labeled
system executions, is available. However, other settings, such as learning from
unlabeled data - an important aspect in machine learning - remain unexplored.
To overcome this limitation, we propose a framework for learning a
deterministic finite automaton (DFA) from a given multi-set of unlabeled words.
We show that this problem is computationally hard and develop three learning
algorithms based on constraint optimization. Moreover, we introduce novel
regularization schemes for our optimization problems that improve the overall
interpretability of our DFAs. Using a prototype implementation, we demonstrate
practical feasibility in the context of unsupervised anomaly detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Algorithms in the Limit <span class="chip">COLT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hristo Papazov, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of learning computable functions in the limit
by extending Gold's inductive inference framework to incorporate
\textit{computational observations} and \textit{restricted input sources}.
Complimentary to the traditional Input-Output Observations, we introduce
Time-Bound Observations, and Policy-Trajectory Observations to study the
learnability of general recursive functions under more realistic constraints.
While input-output observations do not suffice for learning the class of
general recursive functions in the limit, we overcome this learning barrier by
imposing computational complexity constraints or supplementing with approximate
time-bound observations. Further, we build a formal framework around
observations of \textit{computational agents} and show that learning computable
functions from policy trajectories reduces to learning rational functions from
input and output, thereby revealing interesting connections to finite-state
transducer inference. On the negative side, we show that computable or
polynomial-mass characteristic sets cannot exist for the class of linear-time
computable functions even for policy-trajectory observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLT 2025. This version matches the proceedings version
  apart from a small notational change in section 3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning is Not So Mysterious or Different <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Gordon Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are often seen as different from other model classes by
defying conventional notions of generalization. Popular examples of anomalous
generalization behaviour include benign overfitting, double descent, and the
success of overparametrization. We argue that these phenomena are not distinct
to neural networks, or particularly mysterious. Moreover, this generalization
behaviour can be intuitively understood, and rigorously characterized, using
long-standing generalization frameworks such as PAC-Bayes and countable
hypothesis bounds. We present soft inductive biases as a key unifying principle
in explaining these phenomena: rather than restricting the hypothesis space to
avoid overfitting, embrace a flexible hypothesis space, with a soft preference
for simpler solutions that are consistent with the data. This principle can be
encoded in many model classes, and thus deep learning is not as mysterious or
different from other model classes as it might seem. However, we also highlight
how deep learning is relatively distinct in other ways, such as its ability for
representation learning, phenomena such as mode connectivity, and its relative
universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning
  for Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary version, v2, added more details and corrected some minor
  mistakes. Project page: https://anitaleungxx.github.io/ReMix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Federated Personalised Mean Estimation for the Gaussian Mixture
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malhar A. Managoli, Vinod M. Prabhakaran, Suhas Diggavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning with heterogeneous data and personalization has received
significant recent attention. Separately, robustness to corrupted data in the
context of federated learning has also been studied. In this paper we explore
combining personalization for heterogeneous data with robustness, where a
constant fraction of the clients are corrupted. Motivated by this broad
problem, we formulate a simple instantiation which captures some of its
difficulty. We focus on the specific problem of personalized mean estimation
where the data is drawn from a Gaussian mixture model. We give an algorithm
whose error depends almost linearly on the ratio of corrupted to uncorrupted
samples, and show a lower bound with the same behavior, albeit with a gap of a
constant factor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Distributed Estimation: Extending Gossip Algorithms to Ranking
  and Trimmed Means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17836v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17836v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Van Elst, Igor Colin, Stephan Clémençon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of robust estimation in gossip algorithms
over arbitrary communication graphs. Gossip algorithms are fully decentralized,
relying only on local neighbor-to-neighbor communication, making them
well-suited for situations where communication is constrained. A fundamental
challenge in existing mean-based gossip algorithms is their vulnerability to
malicious or corrupted nodes. In this paper, we show that an outlier-robust
mean can be computed by globally estimating a robust statistic. More
specifically, we propose a novel gossip algorithm for rank estimation, referred
to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated
to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed
description of the proposed methods, a key contribution of our work is a
precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank
estimation and an $\mathcal{O}(1 / {t})$ rate for trimmed mean estimation,
where by $t$ is meant the number of iterations. Moreover, we provide a
breakdown point analysis of \textsc{GoTrim}. We empirically validate our
theoretical results through experiments on diverse network topologies, data
distributions and contamination schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Optimal Transport and Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Selitskiy, Maitreya Kocharekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the voice conversion (VC) task using a vector-based
interface. To align audio embeddings between speakers, we employ discrete
optimal transport mapping. Our evaluation results demonstrate the high quality
and effectiveness of this method. Additionally, we show that applying discrete
optimal transport as a post-processing step in audio generation can lead to the
incorrect classification of synthetic audio as real.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley-Based Data Valuation with Mutual Information: A Key to Modified
  K-Nearest Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01991v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01991v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ali Vahedifar, Azim Akhtarshenas, Mohammad Mohammadi Rafatpanah, Maryam Sabbaghian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The K-Nearest Neighbors (KNN) algorithm is widely used for classification and
regression; however, it suffers from limitations, including the equal treatment
of all samples. We propose Information-Modified KNN (IM-KNN), a novel approach
that leverages Mutual Information ($I$) and Shapley values to assign weighted
values to neighbors, thereby bridging the gap in treating all samples with the
same value and weight. On average, IM-KNN improves the accuracy, precision, and
recall of traditional KNN by 16.80%, 17.08%, and 16.98%, respectively, across
12 benchmark datasets. Experiments on four large-scale datasets further
highlight IM-KNN's robustness to noise, imbalanced data, and skewed
distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the IEEE Machine
  Learning and Signal Processing conference (MLSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Cox Models: Assessing the Performance of Machine-Learning Methods
  in Non-Proportional Hazards and Non-Linear Survival Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Rossi, Flavio Sartori, Cesare Rollo, Giovanni Birolo, Piero Fariselli, Tiziana Sanavia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survival analysis often relies on Cox models, assuming both linearity and
proportional hazards (PH). This study evaluates machine and deep learning
methods that relax these constraints, comparing their performance with
penalized Cox models on a benchmark of three synthetic and three real datasets.
In total, eight different models were tested, including six non-linear models
of which four were also non-PH. Although Cox regression often yielded
satisfactory performance, we showed the conditions under which machine and deep
learning models can perform better. Indeed, the performance of these methods
has often been underestimated due to the improper use of Harrell's concordance
index (C-index) instead of more appropriate scores such as Antolini's
concordance index, which generalizes C-index in cases where the PH assumption
does not hold. In addition, since occasionally high C-index models happen to be
badly calibrated, combining Antolini's C-index with Brier's score is useful to
assess the overall performance of a survival method. Results on our benchmark
data showed that survival prediction should be approached by testing different
methods to select the most appropriate one according to sample size,
non-linearity and non-PH conditions. To allow an easy reproducibility of these
tests on our benchmark data, code and documentation are freely available at
https://github.com/compbiomed-unito/survhive.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Counterfactual Data Augmentation for Robust Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13431v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13431v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoling Zhou, Ou Wu, Michael K. Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are prone to capturing the spurious correlations
between non-causal attributes and classes, with counterfactual data
augmentation being a promising direction for breaking these spurious
associations. However, generating counterfactual data explicitly poses a
challenge, and incorporating augmented data into the training process decreases
training efficiency. This study proposes an Implicit Counterfactual Data
Augmentation (ICDA) method to remove spurious correlations and make stable
predictions. Specifically, first, a novel sample-wise augmentation strategy is
developed that generates semantically and counterfactually meaningful deep
features with distinct augmentation strength for each sample. Second, we derive
an easy-to-compute surrogate loss on the augmented feature set when the number
of augmented samples becomes infinite. Third, two concrete schemes are
proposed, including direct quantification and meta-learning, to derive the key
parameters for the robust loss. In addition, ICDA is explained from a
regularization perspective, revealing its capacity to improve intra-class
compactness and augment margins at both class and sample levels. Extensive
experiments have been conducted across various biased learning scenarios
covering both image and text datasets, demonstrating that ICDA consistently
enhances the generalization and robustness performance of popular networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering <span class="highlight-title">RL</span> Integration in SSL Loss: Objective-Specific Implications
  for Data-Efficient <span class="highlight-title">RL</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17428v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17428v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ömer Veysel Çağatan, Barış Akgün
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate the effect of SSL objective modifications
within the SPR framework, focusing on specific adjustments such as terminal
state masking and prioritized replay weighting, which were not explicitly
addressed in the original design. While these modifications are specific to RL,
they are not universally applicable across all RL algorithms. Therefore, we aim
to assess their impact on performance and explore other SSL objectives that do
not accommodate these adjustments like Barlow Twins and VICReg. We evaluate six
SPR variants on the Atari 100k benchmark, including versions both with and
without these modifications. Additionally, we test the performance of these
objectives on the DeepMind Control Suite, where such modifications are absent.
Our findings reveal that incorporating specific SSL modifications within SPR
significantly enhances performance, and this influence extends to subsequent
frameworks like SR-SPR and BBF, highlighting the critical importance of SSL
objective selection and related adaptations in achieving data efficiency in
self-predictive reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RLC 2025, Neurips SSL Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Negative Mining For Temporal Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Chen, Tongya Zheng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal networks are effective in capturing the evolving interactions of
networks over time, such as social networks and e-commerce networks. In recent
years, researchers have primarily concentrated on developing specific model
architectures for Temporal Graph Neural Networks (TGNNs) in order to improve
the representation quality of temporal nodes and edges. However, limited
attention has been given to the quality of negative samples during the training
of TGNNs. When compared with static networks, temporal networks present two
specific challenges for negative sampling: positive sparsity and positive
shift. Positive sparsity refers to the presence of a single positive sample
amidst numerous negative samples at each timestamp, while positive shift
relates to the variations in positive samples across different timestamps. To
robustly address these challenges in training TGNNs, we introduce Curriculum
Negative Mining (CurNM), a model-aware curriculum learning framework that
adaptively adjusts the difficulty of negative samples. Within this framework,
we first establish a dynamically updated negative pool that balances random,
historical, and hard negatives to address the challenges posed by positive
sparsity. Secondly, we implement a temporal-aware negative selection module
that focuses on learning from the disentangled factors of recently active
edges, thus accurately capturing shifting preferences. Finally, the selected
negatives are combined with annealing random negatives to support stable
training. Extensive experiments on 12 datasets and 3 TGNNs demonstrate that our
method outperforms baseline methods by a significant margin. Additionally,
thorough ablation studies and parameter sensitivity experiments verify the
usefulness and robustness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative sentiment analysis of public perception: Monkeypox vs.
  COVID-19 behavioral insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Mohaimen Akand Faisal, Rabeya Amin Jhuma, Jamini Jasim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),
has underscored the importance of understanding public sentiment to inform
effective public health strategies. This study conducts a comparative sentiment
analysis of public perceptions surrounding COVID-19 and mpox by leveraging
extensive datasets of 147,475 and 106,638 tweets, respectively. Advanced
machine learning models, including Logistic Regression, Naive Bayes, RoBERTa,
DistilRoBERTa and XLNet, were applied to perform sentiment classification, with
results indicating key trends in public emotion and discourse. The analysis
highlights significant differences in public sentiment driven by disease
characteristics, media representation, and pandemic fatigue. Through the lens
of sentiment polarity and thematic trends, this study offers valuable insights
into tailoring public health messaging, mitigating misinformation, and
fostering trust during concurrent health crises. The findings contribute to
advancing sentiment analysis applications in public health informatics, setting
the groundwork for enhanced real-time monitoring and multilingual analysis in
future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2FGL: Spatial Spectral Federated Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Trajectory Optimization for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhao, Zhaochun Ren, Liu Yang, Yunsen Liang, Fajie Yuan, Pengjie Ren, Zhumin Chen, jun Ma, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) aims to learn policies without online
explorations. To enlarge the training data, model-based offline RL learns a
dynamics model which is utilized as a virtual environment to generate
simulation data and enhance policy learning. However, existing data
augmentation methods for offline RL suffer from (i) trivial improvement from
short-horizon simulation; and (ii) the lack of evaluation and correction for
generated data, leading to low-qualified augmentation.
  In this paper, we propose offline trajectory optimization for offline
reinforcement learning (OTTO). The key motivation is to conduct long-horizon
simulation and then utilize model uncertainty to evaluate and correct the
augmented data. Specifically, we propose an ensemble of Transformers, a.k.a.
World Transformers, to predict environment state dynamics and the reward
function. Three strategies are proposed to use World Transformers to generate
long-horizon trajectory simulation by perturbing the actions in the offline
data. Then, an uncertainty-based World Evaluator is introduced to firstly
evaluate the confidence of the generated trajectories and then perform the
correction for low-confidence data. Finally, we jointly use the original data
with the corrected augmentation data to train an offline RL algorithm. OTTO
serves as a plug-in module and can be integrated with existing model-free
offline RL methods. Experiments on various benchmarks show that OTTO can
effectively improve the performance of representative offline RL algorithms,
including in complex environments with sparse rewards like AntMaze. Codes are
available at https://github.com/ZiqiZhao1/OTTO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Push the Button! Exploring Data Leakage Risks in Machine Learning
  and Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13796v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13796v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Apicella, Francesco Isgrò, Roberto Prevete
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) has revolutionized various domains, offering predictive
capabilities in several areas. However, with the increasing accessibility of ML
tools, many practitioners, lacking deep ML expertise, adopt a "push the button"
approach, utilizing user-friendly interfaces without a thorough understanding
of underlying algorithms. While this approach provides convenience, it raises
concerns about the reliability of outcomes, leading to challenges such as
incorrect performance evaluation. This paper addresses a critical issue in ML,
known as data leakage, where unintended information contaminates the training
data, impacting model performance evaluation. Users, due to a lack of
understanding, may inadvertently overlook crucial steps, leading to optimistic
performance estimates that may not hold in real-world scenarios. The
discrepancy between evaluated and actual performance on new data is a
significant concern. In particular, this paper categorizes data leakage in ML,
discussing how certain conditions can propagate through the ML workflow.
Furthermore, it explores the connection between data leakage and the specific
task being addressed, investigates its occurrence in Transfer Learning, and
compares standard inductive ML with transductive ML frameworks. The conclusion
summarizes key findings, emphasizing the importance of addressing data leakage
for robust and reliable ML applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be published on Artificial Intelligence Review journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07793v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07793v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Ding, Arturas Aleksandraus, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is critical for ensuring the reliability
of deep learning systems, particularly in safety-critical applications.
Likelihood-based deep generative models have historically faced criticism for
their unsatisfactory performance in OOD detection, often assigning higher
likelihood to OOD data than in-distribution samples when applied to image data.
In this work, we demonstrate that likelihood is not inherently flawed. Rather,
several properties in the images space prohibit likelihood as a valid detection
score. Given a sufficiently good likelihood estimator, specifically using the
probability flow formulation of a diffusion model, we show that
likelihood-based methods can still perform on par with state-of-the-art methods
when applied in the representation space of pre-trained encoders. The code of
our work can be found at
$\href{https://github.com/limchaos/Likelihood-OOD.git}{\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Scandinavian Conference on Image Analysis 2025 (oral)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">80</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and
  Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically
referencing visual regions, just like human "thinking with images". However, no
benchmark exists to evaluate these capabilities holistically. To bridge this
gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a
diagnostic benchmark built on three principles: (1) focused visual perception
of subtle targets in complex scenes, (2) traceable evidence via bounding box
evaluation, and (3) second-order reasoning to test object interactions and
spatial hierarchies beyond simple object localization. Prioritizing images with
dense objects, we initially sample 1K high-quality images from SA-1B, and
incorporate eight LMM experts to manually annotate questions, candidate
options, and answers for each image. After three stages of quality control,
TreeBench consists of 405 challenging visual question-answering pairs, even the
most advanced models struggle with this benchmark, where none of them reach 60%
accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR
(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to
supervise localization and reasoning jointly with reinforcement learning,
enabling accurate localizations and explainable reasoning pathways. Initialized
from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and
TreeBench (+13.4), proving traceability is key to advancing vision-grounded
reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PyVision: Agentic Vision with Dynamic Tooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly deployed as agents, systems capable of planning,
reasoning, and dynamically calling external tools. However, in visual
reasoning, prior approaches largely remain limited by predefined workflows and
static toolsets. In this report, we present PyVision, an interactive,
multi-turn framework that enables MLLMs to autonomously generate, execute, and
refine Python-based tools tailored to the task at hand, unlocking flexible and
interpretable problem-solving. We develop a taxonomy of the tools created by
PyVision and analyze their usage across a diverse set of benchmarks.
Quantitatively, PyVision achieves consistent performance gains, boosting
GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.
These results point to a broader shift: dynamic tooling allows models not just
to use tools, but to invent them, advancing toward more agentic visual
reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 Pages, 10 Figures, Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-pass Adaptive Image Tokenization for Minimum Program Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Duggal, Sanghyun Byun, William T. Freeman, Antonio Torralba, Phillip Isola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to Algorithmic Information Theory (AIT) -- Intelligent
representations compress data into the shortest possible program that can
reconstruct its content, exhibiting low Kolmogorov Complexity (KC). In
contrast, most visual representation learning systems use fixed-length
representations for all inputs, ignoring variations in complexity or
familiarity. Recent adaptive tokenization methods address this by allocating
variable-length representations but typically require test-time search over
multiple encodings to find the most predictive one. Inspired by Kolmogorov
Complexity principles, we propose a single-pass adaptive tokenizer, KARL, which
predicts the appropriate number of tokens for an image in a single forward
pass, halting once its approximate KC is reached. The token count serves as a
proxy for the minimum description length. KARL's training procedure closely
resembles the Upside-Down Reinforcement Learning paradigm, as it learns to
conditionally predict token halting based on a desired reconstruction quality.
KARL matches the performance of recent adaptive tokenizers while operating in a
single pass. We present scaling laws for KARL, analyzing the role of
encoder/decoder size, continuous vs. discrete tokenization and more.
Additionally, we offer a conceptual study drawing an analogy between Adaptive
Image Tokenization and Algorithmic Information Theory, examining the predicted
image complexity (KC) across axes such as structure vs. noise and in- vs.
out-of-distribution familiarity -- revealing alignment with human intuition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: https://github.com/ShivamDuggal4/karl Keywords:
  Representation Learning, Adaptive Tokenization, Compression, Algorithmic
  Information Theory, Kolmogorov Complexity, Upside-Down RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multigranular Evaluation for Brain Visual Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Xia, Cengiz Oztireli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing evaluation protocols for brain visual decoding predominantly rely on
coarse metrics that obscure inter-model differences, lack neuroscientific
foundation, and fail to capture fine-grained visual distinctions. To address
these limitations, we introduce BASIC, a unified, multigranular evaluation
framework that jointly quantifies structural fidelity, inferential alignment,
and contextual coherence between decoded and ground truth images. For the
structural level, we introduce a hierarchical suite of segmentation-based
metrics, including foreground, semantic, instance, and component masks,
anchored in granularity-aware correspondence across mask structures. For the
semantic level, we extract structured scene representations encompassing
objects, attributes, and relationships using multimodal large language models,
enabling detailed, scalable, and context-rich comparisons with ground-truth
stimuli. We benchmark a diverse set of visual decoding methods across multiple
stimulus-neuroimaging datasets within this unified evaluation framework.
Together, these criteria provide a more discriminative, interpretable, and
comprehensive foundation for measuring brain visual decoding methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://weihaox.github.io/BASIC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Granular Spatio-Temporal Token Merging for Training-Free
  Acceleration of Video LLMs <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video large language models (LLMs) achieve strong video understanding by
leveraging a large number of spatio-temporal tokens, but suffer from quadratic
computational scaling with token count. To address this, we propose a
training-free spatio-temporal token merging method, named STTM. Our key insight
is to exploit local spatial and temporal redundancy in video data which has
been overlooked in prior work. STTM first transforms each frame into
multi-granular spatial tokens using a coarse-to-fine search over a quadtree
structure, then performs directed pairwise merging across the temporal
dimension. This decomposed merging approach outperforms existing token
reduction methods across six video QA benchmarks. Notably, STTM achieves a
2$\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and
a 3$\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is
query-agnostic, allowing KV cache reuse across different questions for the same
video. The project page is available at https://www.jshyun.me/projects/sttm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV2025; Project page:
  https://www.jshyun.me/projects/sttm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXPO: Stable Reinforcement Learning with Expressive Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Perry Dong, Qiyang Li, Dorsa Sadigh, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of training and fine-tuning expressive policies with
online reinforcement learning (RL) given an offline dataset. Training
expressive policy classes with online RL present a unique challenge of stable
value maximization. Unlike simpler Gaussian policies commonly used in online
RL, expressive policies like diffusion and flow-matching policies are
parameterized by a long denoising chain, which hinders stable gradient
propagation from actions to policy parameters when optimizing against some
value function. Our key insight is that we can address stable value
maximization by avoiding direct optimization over value with the expressive
policy and instead construct an on-the-fly RL policy to maximize Q-value. We
propose Expressive Policy Optimization (EXPO), a sample-efficient online RL
algorithm that utilizes an on-the-fly policy to maximize value with two
parameterized policies -- a larger expressive base policy trained with a stable
imitation learning objective and a light-weight Gaussian edit policy that edits
the actions sampled from the base policy toward a higher value distribution.
The on-the-fly policy optimizes the actions from the base policy with the
learned edit policy and chooses the value maximizing action from the base and
edited actions for both sampling and temporal-difference (TD) backup. Our
approach yields up to 2-3x improvement in sample efficiency on average over
prior methods both in the setting of fine-tuning a pretrained policy given
offline data and in leveraging offline data to train online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance and Practical Considerations of Large and Small Language
  Models in Clinical Decision Su<span class="highlight-title">ppo</span>rt in Rheumatology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabine Felde, Rüdiger Buchkremer, Gamal Chehab, Christian Thielscher, Jörg HW Distler, Matthias Schneider, Jutta G. Richter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show promise for supporting clinical
decision-making in complex fields such as rheumatology. Our evaluation shows
that smaller language models (SLMs), combined with retrieval-augmented
generation (RAG), achieve higher diagnostic and therapeutic performance than
larger models, while requiring substantially less energy and enabling
cost-efficient, local deployment. These features are attractive for
resource-limited healthcare. However, expert oversight remains essential, as no
model consistently reached specialist-level accuracy in rheumatology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry Forcing: Marrying Video <span class="highlight-title">Diffusion</span> and 3D Representation for
  Consistent Wo<span class="highlight-title">rl</span>d Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos inherently represent 2D projections of a dynamic 3D world. However,
our analysis suggests that video diffusion models trained solely on raw video
data often fail to capture meaningful geometric-aware structure in their
learned representations. To bridge this gap between video diffusion models and
the underlying 3D nature of the physical world, we propose Geometry Forcing, a
simple yet effective method that encourages video diffusion models to
internalize latent 3D representations. Our key insight is to guide the model's
intermediate representations toward geometry-aware structure by aligning them
with features from a pretrained geometric foundation model. To this end, we
introduce two complementary alignment objectives: Angular Alignment, which
enforces directional consistency via cosine similarity, and Scale Alignment,
which preserves scale-related information by regressing unnormalized geometric
features from normalized diffusion representation. We evaluate Geometry Forcing
on both camera view-conditioned and action-conditioned video generation tasks.
Experimental results demonstrate that our method substantially improves visual
quality and 3D consistency over the baseline methods. Project page:
https://GeometryForcing.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, project page: https://GeometryForcing.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why is Your Language Model a Poor Implicit Reward Model? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Razin, Yong Lin, Jiarui Yao, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Action Chunking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyang Li, Zhiyuan Zhou, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling <span class="highlight-title">RL</span> to Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a full-stack framework that scales up reasoning in
vision-language models (VLMs) to long videos, leveraging reinforcement
learning. We address the unique challenges of long video reasoning by
integrating three critical components: (1) a large-scale dataset,
LongVideo-Reason, comprising 52K long video QA pairs with high-quality
reasoning annotations across diverse domains such as sports, games, and vlogs;
(2) a two-stage training pipeline that extends VLMs with chain-of-thought
supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a
training infrastructure for long video RL, named Multi-modal Reinforcement
Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a
vLLM-based engine tailored for long video, using cached video embeddings for
efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves
strong performance on long video QA benchmarks such as VideoMME. It also
outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal
reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on
our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to
2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent
performance gains as the number of input video frames scales. LongVILA-R1 marks
a firm step towards long video reasoning in VLMs. In addition, we release our
training system for public availability that supports RL training on various
modalities (video, text, and audio), various models (VILA and Qwen series), and
even image and video generation models. On a single A100 node (8 GPUs), it
supports RL training on hour-long videos (e.g., 3,600 frames / around 256k
tokens).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at https://github.com/NVlabs/Long-RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRIX: Multi-Agent Memory System for LLM-Based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang, Xi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although memory capabilities of AI agents are gaining increasing attention,
existing solutions remain fundamentally limited. Most rely on flat, narrowly
scoped memory components, constraining their ability to personalize, abstract,
and reliably recall user-specific information over time. To this end, we
introduce MIRIX, a modular, multi-agent memory system that redefines the future
of AI memory by solving the field's most critical challenge: enabling language
models to truly remember. Unlike prior approaches, MIRIX transcends text to
embrace rich visual and multimodal experiences, making memory genuinely useful
in real-world scenarios. MIRIX consists of six distinct, carefully structured
memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and
Knowledge Vault, coupled with a multi-agent framework that dynamically controls
and coordinates updates and retrieval. This design enables agents to persist,
reason over, and accurately retrieve diverse, long-term user data at scale. We
validate MIRIX in two demanding settings. First, on ScreenshotVQA, a
challenging multimodal benchmark comprising nearly 20,000 high-resolution
computer screenshots per sequence, requiring deep contextual understanding and
where no existing memory systems can be applied, MIRIX achieves 35% higher
accuracy than the RAG baseline while reducing storage requirements by 99.9%.
Second, on LOCOMO, a long-form conversation benchmark with single-modal textual
input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing
existing baselines. These results show that MIRIX sets a new performance
standard for memory-augmented LLM agents. To allow users to experience our
memory system, we provide a packaged application powered by MIRIX. It monitors
the screen in real time, builds a personalized memory base, and offers
intuitive visualization and secure local storage to ensure privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low Resource Reconstruction Attacks Through Benign <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sol Yarkoni, Roi Livni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in generative models such as diffusion models have raised
several risks and concerns related to privacy, copyright infringements and data
stewardship. To better understand and control the risks, various researchers
have created techniques, experiments and attacks that reconstruct images, or
part of images, from the training set. While these techniques already establish
that data from the training set can be reconstructed, they often rely on
high-resources, excess to the training set as well as well-engineered and
designed prompts.
  In this work, we devise a new attack that requires low resources, assumes
little to no access to the actual training set, and identifies, seemingly,
benign prompts that lead to potentially-risky image reconstruction. This
highlights the risk that images might even be reconstructed by an uninformed
user and unintentionally. For example, we identified that, with regard to one
existing model, the prompt ``blue Unisex T-Shirt'' can generate the face of a
real-life human model. Our method builds on an intuition from previous works
which leverages domain knowledge and identifies a fundamental vulnerability
that stems from the use of scraped data from e-commerce platforms, where
templated layouts and images are tied to pattern-like prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Working with AI: Measuring the Occupational Implications of Generative
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Tomlinson, Sonia Jaffe, Will Wang, Scott Counts, Siddharth Suri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meek Models Shall Inherit the Earth <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans Gundlach, Jayson Lynch, Neil Thompson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, longer version of the paper presented at TAIG
  ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Experts' Perspectives on AI-Assisted Public Speaking Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nesrine Fourati, Alisa Barkar, Marion Dragée, Liv Danthon-Lefebvre, Mathieu Chollet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Public speaking is a vital professional skill, yet it remains a
source of significant anxiety for many individuals. Traditional training relies
heavily on expert coaching, but recent advances in AI has led to novel types of
commercial automated public speaking feedback tools. However, most research has
focused on prototypes rather than commercial applications, and little is known
about how public speaking experts perceive these tools.
  Objectives: This study aims to evaluate expert opinions on the efficacy and
design of commercial AI-based public speaking training tools and to propose
guidelines for their improvement.
  Methods: The research involved 16 semi-structured interviews and 2 focus
groups with public speaking experts. Participants discussed their views on
current commercial tools, their potential integration into traditional
coaching, and suggestions for enhancing these systems.
  Results and Conclusions: Experts acknowledged the value of AI tools in
handling repetitive, technical aspects of training, allowing coaches to focus
on higher-level skills. However they found key issues in current tools,
emphasising the need for personalised, understandable, carefully selected
feedback and clear instructional design. Overall, they supported a hybrid model
combining traditional coaching with AI-supported exercises.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Continuous Home Cage Monitoring: An Evaluation of Tracking and
  Identification Strategies for Laboratory Mice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Pablo Oberhauser, Daniel Grzenda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous, automated monitoring of laboratory mice enables more accurate
data collection and improves animal welfare through real-time insights.
Researchers can achieve a more dynamic and clinically relevant characterization
of disease progression and therapeutic effects by integrating behavioral and
physiological monitoring in the home cage. However, providing individual mouse
metrics is difficult because of their housing density, similar appearances,
high mobility, and frequent interactions. To address these challenges, we
develop a real-time identification (ID) algorithm that accurately assigns ID
predictions to mice wearing custom ear tags in digital home cages monitored by
cameras. Our pipeline consists of three parts: (1) a custom multiple object
tracker (MouseTracks) that combines appearance and motion cues from mice; (2) a
transformer-based ID classifier (Mouseformer); and (3) a tracklet associator
linear program to assign final ID predictions to tracklets (MouseMap). Our
models assign an animal ID based on custom ear tags at 30 frames per second
with 24/7 cage coverage. We show that our custom tracking and ID pipeline
improves tracking efficiency and lowers ID switches across mouse strains and
various environmental factors compared to current mouse tracking methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTECT: Dynamic Topic Explorer & Context Tracker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Adhya, Debarshi Kumar Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of textual data over time presents a significant
challenge in uncovering evolving themes and trends. Existing dynamic topic
modeling techniques, while powerful, often exist in fragmented pipelines that
lack robust support for interpretation and user-friendly exploration. We
introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end
system that bridges the gap between raw textual data and meaningful temporal
insights. DTECT provides a unified workflow that supports data preprocessing,
multiple model architectures, and dedicated evaluation metrics to analyze the
topic quality of temporal topic models. It significantly enhances
interpretability by introducing LLM-driven automatic topic labeling, trend
analysis via temporally salient words, interactive visualizations with
document-level summarization, and a natural language chat interface for
intuitive data querying. By integrating these features into a single, cohesive
platform, DTECT empowers users to more effectively track and understand
thematic dynamics. DTECT is open-source and available at
https://github.com/AdhyaSuman/DTECT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/AdhyaSuman/DTECT | Demo:
  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:
  https://youtu.be/B8nNfxFoJAU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Retrieval of Topics and Insights from Earnings Calls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anant Gupta, Rajarshi Bhowmik, Geoffrey Gunow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking the strategic focus of companies through topics in their earnings
calls is a key task in financial analysis. However, as industries evolve,
traditional topic modeling techniques struggle to dynamically capture emerging
topics and their relationships. In this work, we propose an LLM-agent driven
approach to discover and retrieve emerging topics from quarterly earnings
calls. We propose an LLM-agent to extract topics from documents, structure them
into a hierarchical ontology, and establish relationships between new and
existing topics through a topic ontology. We demonstrate the use of extracted
topics to infer company-level insights and emerging trends over time. We
evaluate our approach by measuring ontology coherence, topic evolution
accuracy, and its ability to surface emerging financial trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2nd Workshop on Financial Information Retrieval in the Era of
  Generative AI, The 48th International ACM SIGIR Conference on Research and
  Development in Information Retrieval July 13-17, 2025 | Padua, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Integrated Framework of <span class="highlight-title">Prompt</span> Engineering and Multidimensional
  Knowledge Graphs for Legal Dispute Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Zhang, Na Zhao, Jianglong Qing, Qing xu, Kaiwen Pan, Ting luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient
  Neural Inference on MCUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashe Neth, Sawinder kaur, Mohammad Nur Hossain Khan, Subrata Biswas, Asif Salekin, Bashima Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing pruning methods are typically applied during training or compile
time and often rely on structured sparsity. While compatible with low-power
microcontrollers (MCUs), structured pruning underutilizes the opportunity for
fine-grained efficiency on devices without SIMD support or parallel compute. To
address these limitations, we introduce UnIT (Unstructured Inference-Time
pruning), a lightweight method that dynamically identifies and skips
unnecessary multiply-accumulate (MAC) operations during inference, guided by
input-specific activation patterns. Unlike structured pruning, UnIT embraces
irregular sparsity and does not require retraining or hardware specialization.
It transforms pruning decisions into lightweight comparisons, replacing
multiplications with threshold checks and approximated divisions. UnIT further
optimizes compute by reusing threshold computations across multiple connections
and applying layer- and group-specific pruning sensitivity. We present three
fast, hardware-friendly division approximations tailored to the capabilities of
common embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT
achieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and
27.33% to 84.38% lower energy consumption compared to training-time pruned
models, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT
matches or exceeds the accuracy of retrained models while requiring
significantly fewer MACs. These results establish unstructured inference-time
pruning as a viable and practical solution for efficient, retraining-free
deployment of deep neural networks on MCUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to SenSys 2026 on July 1, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key
  Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toluwani Aremu, Noor Hussein, Munachiso Nwadike, Samuele Poppi, Jie Zhang, Karthik Nandakumar, Neil Gong, Nils Lukas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking offers a promising solution for GenAI providers to establish the
provenance of their generated content. A watermark is a hidden signal embedded
in the generated content, whose presence can later be verified using a secret
watermarking key. A threat to GenAI providers are \emph{watermark stealing}
attacks, where users forge a watermark into content that was \emph{not}
generated by the provider's models without access to the secret key, e.g., to
falsely accuse the provider. Stealing attacks collect \emph{harmless}
watermarked samples from the provider's model and aim to maximize the expected
success rate of generating \emph{harmful} watermarked samples. Our work focuses
on mitigating stealing attacks while treating the underlying watermark as a
black-box. Our contributions are: (i) Proposing a multi-key extension to
mitigate stealing attacks that can be applied post-hoc to any watermarking
method across any modality. (ii) We provide theoretical guarantees and
demonstrate empirically that our method makes forging substantially less
effective across multiple datasets, and (iii) we formally define the threat of
watermark forging as the task of generating harmful, watermarked content and
model this threat via security games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alpay Algebra V: Multi-Layered Semantic Games and Transfinite
  Fixed-Point Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bugra Kilictas, Faruk Alpay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper extends the self-referential framework of Alpay Algebra into a
multi-layered semantic game architecture where transfinite fixed-point
convergence encompasses hierarchical sub-games at each iteration level.
Building upon Alpay Algebra IV's empathetic embedding concept, we introduce a
nested game-theoretic structure where the alignment process between AI systems
and documents becomes a meta-game containing embedded decision problems. We
formalize this through a composite operator $\phi(\cdot, \gamma(\cdot))$ where
$\phi$ drives the main semantic convergence while $\gamma$ resolves local
sub-games. The resulting framework demonstrates that game-theoretic reasoning
emerges naturally from fixed-point iteration rather than being imposed
externally. We prove a Game Theorem establishing existence and uniqueness of
semantic equilibria under realistic cognitive simulation assumptions. Our
verification suite includes adaptations of Banach's fixed-point theorem to
transfinite contexts, a novel $\phi$-topology based on the
Kozlov-Maz'ya-Rossmann formula for handling semantic singularities, and
categorical consistency tests via the Yoneda lemma. The paper itself functions
as a semantic artifact designed to propagate its fixed-point patterns in AI
embedding spaces -- a deliberate instantiation of the "semantic virus" concept
it theorizes. All results are grounded in category theory, information theory,
and realistic AI cognition models, ensuring practical applicability beyond pure
mathematical abstraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching for actual causes: Approximate algorithms with adjustable
  precision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Reyd, Ada Diaconescu, Jean-Louis Dessalles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization Guarantees for Square-Root Natural-Gradient Variational
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navish Kumar, Thomas Möllenhoff, Mohammad Emtiyaz Khan, Aurelien Lucchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational inference with natural-gradient descent often shows fast
convergence in practice, but its theoretical convergence guarantees have been
challenging to establish. This is true even for the simplest cases that involve
concave log-likelihoods and use a Gaussian approximation. We show that the
challenge can be circumvented for such cases using a square-root
parameterization for the Gaussian covariance. This approach establishes novel
convergence guarantees for natural-gradient variational-Gaussian inference and
its continuous-time gradient flow. Our experiments demonstrate the
effectiveness of natural gradient methods and highlight their advantages over
algorithms that use Euclidean or Wasserstein geometries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Ambiguity to Accuracy: The Transformative Effect of Coreference
  Resolution on Retrieval-Augmented Generation systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjoon Jang, Seongtae Hong, Junyoung Son, Sungjin Park, Chanjun Park, Heuiseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in
natural language processing (NLP), improving factual consistency and reducing
hallucinations by integrating external document retrieval with large language
models (LLMs). However, the effectiveness of RAG is often hindered by
coreferential complexity in retrieved documents, introducing ambiguity that
disrupts in-context learning. In this study, we systematically investigate how
entity coreference affects both document retrieval and generative performance
in RAG-based systems, focusing on retrieval relevance, contextual
understanding, and overall response quality. We demonstrate that coreference
resolution enhances retrieval effectiveness and improves question-answering
(QA) performance. Through comparative analysis of different pooling strategies
in retrieval tasks, we find that mean pooling demonstrates superior context
capturing ability after applying coreference resolution. In QA tasks, we
discover that smaller models benefit more from the disambiguation process,
likely due to their limited inherent capacity for handling referential
ambiguity. With these findings, this study aims to provide a deeper
understanding of the challenges posed by coreferential complexity in RAG,
providing guidance for improving retrieval and generation in
knowledge-intensive AI applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Dirauf, Florian Wolz, Dario Zanca, Björn Eskofier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content-based puzzle solvers have been extensively studied, demonstrating
significant progress in computational techniques. However, their evaluation
often lacks realistic challenges crucial for real-world applications, such as
the reassembly of fragmented artefacts or shredded documents. In this work, we
investigate the robustness of State-Of-The-Art content-based puzzle solvers
introducing three types of jigsaw puzzle corruptions: missing pieces, eroded
edges, and eroded contents. Evaluating both heuristic and deep learning-based
solvers, we analyse their ability to handle these corruptions and identify key
limitations. Our results show that solvers developed for standard puzzles have
a rapid decline in performance if more pieces are corrupted. However, deep
learning models can significantly improve their robustness through fine-tuning
with augmented data. Notably, the advanced Positional Diffusion model adapts
particularly well, outperforming its competitors in most experiments. Based on
our findings, we highlight promising research directions for enhancing the
automated reconstruction of real-world artefacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICIAP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a
  Paradigm Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunsu Baek, Keondo Park, Jeonggil Ko, Min-hwan Oh, Taesik Gong, Hyung-Sin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Xu, Jiaqian Yu, Xiongfeng Peng, Yiwei Chen, Weiming Li, Jaewook Yoo, Sunghyun Chunag, Dongwook Lee, Daehyun Ji, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Effect of Instruction Tuning Loss on Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions of the Association for Computational Linguistics (TACL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Logic and Learning: Decoding Temporal Logic Embeddings via
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Candussio, Gaia Saveri, Gabriele Sarti, Luca Bortolussi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, to be published in ECML-PKDD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Instance-aware <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Xiao, Yunbei Zhang, Xingjian Li, Tianyang Wang, Xiao Wang, Yuxiang Wei, Jihun Hamm, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) has emerged as a parameter-efficient fine-tuning
paradigm for vision transformers, with conventional approaches utilizing
dataset-level prompts that remain the same across all input instances. We
observe that this strategy results in sub-optimal performance due to high
variance in downstream datasets. To address this challenge, we propose Visual
Instance-aware Prompt Tuning (ViaPT), which generates instance-aware prompts
based on each individual input and fuses them with dataset-level prompts,
leveraging Principal Component Analysis (PCA) to retain important prompting
information. Moreover, we reveal that VPT-Deep and VPT-Shallow represent two
corner cases based on a conceptual understanding, in which they fail to
effectively capture instance-specific information, while random dimension
reduction on prompts only yields performance between the two extremes. Instead,
ViaPT overcomes these limitations by balancing dataset-level and instance-level
knowledge, while reducing the amount of learnable parameters compared to
VPT-Deep. Extensive experiments across 34 diverse datasets demonstrate that our
method consistently outperforms state-of-the-art baselines, establishing a new
paradigm for analyzing and optimizing visual prompts for vision transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring AI Alignment with Human Flourishing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Hilliard, Akshaya Jagadeesh, Alex Cook, Steele Billings, Nicholas Skytland, Alicia Llewellyn, Jackson Paull, Nathan Paull, Nolan Kurylo, Keatra Nesbitt, Robert Gruenewald, Anthony Jantzi, Omar Chavez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Where are we with calibration under <span class="highlight-title">dataset</span> shift in image
  classification? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Roschewitz, Raghav Mehta, Fabio de Sousa Ribeiro, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conduct an extensive study on the state of calibration under real-world
dataset shift for image classification. Our work provides important insights on
the choice of post-hoc and in-training calibration techniques, and yields
practical guidelines for all practitioners interested in robust calibration
under shift. We compare various post-hoc calibration methods, and their
interactions with common in-training calibration strategies (e.g., label
smoothing), across a wide range of natural shifts, on eight different
classification tasks across several imaging domains. We find that: (i)
simultaneously applying entropy regularisation and label smoothing yield the
best calibrated raw probabilities under dataset shift, (ii) post-hoc
calibrators exposed to a small amount of semantic out-of-distribution data
(unrelated to the task) are most robust under shift, (iii) recent calibration
methods specifically aimed at increasing calibration under shifts do not
necessarily offer significant improvements over simpler post-hoc calibration
methods, (iv) improving calibration under shifts often comes at the cost of
worsening in-distribution calibration. Importantly, these findings hold for
randomly initialised classifiers, as well as for those finetuned from
foundation models, the latter being consistently better calibrated compared to
models trained from scratch. Finally, we conduct an in-depth analysis of
ensembling effects, finding that (i) applying calibration prior to ensembling
(instead of after) is more effective for calibration under shifts, (ii) for
ensembles, OOD exposure deteriorates the ID-shifted calibration trade-off,
(iii) ensembling remains one of the most effective methods to improve
calibration robustness and, combined with finetuning from foundation models,
yields best calibration results overall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/biomedia-mira/calibration_under_shifts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time
  Training <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseong Jeong, Jegyeong Cho, Youngho Yoon, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizing neural networks to unseen target domains is a significant
challenge in real-world deployments. Test-time training (TTT) addresses this by
using an auxiliary self-supervised task to reduce the domain gap caused by
distribution shifts between the source and target. However, we find that when
models are required to perform multiple tasks under domain shifts, conventional
TTT methods suffer from unsynchronized task behavior, where the adaptation
steps needed for optimal performance in one task may not align with the
requirements of other tasks. To address this, we propose a novel TTT approach
called Synchronizing Tasks for Test-time Training (S4T), which enables the
concurrent handling of multiple tasks. The core idea behind S4T is that
predicting task relations across domain shifts is key to synchronizing tasks
during test time. To validate our approach, we apply S4T to conventional
multi-task benchmarks, integrating it with traditional TTT protocols. Our
empirical results show that S4T outperforms state-of-the-art TTT methods across
various benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeheun Jung, Bosung Jung, Suhyun Bae, Donghun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning seeks to remove the influence of particular data or class
from trained models to meet privacy, legal, or ethical requirements. Existing
unlearning methods tend to forget shallowly: phenomenon of an unlearned model
pretend to forget by adjusting only the model response, while its internal
representations retain information sufficiently to restore the forgotten data
or behavior. We empirically confirm the widespread shallowness by reverting the
forgetting effect of various unlearning methods via training-free performance
recovery attack and gradient-inversion-based data reconstruction attack. To
address this vulnerability fundamentally, we define a theoretical criterion of
``deep forgetting'' based on one-point-contraction of feature representations
of data to forget. We also propose an efficient approximation algorithm, and
use it to construct a novel general-purpose unlearning algorithm:
One-Point-Contraction (OPC). Empirical evaluations on image classification
unlearning benchmarks show that OPC achieves not only effective unlearning
performance but also superior resilience against both performance recovery
attack and gradient-inversion attack. The distinctive unlearning performance of
OPC arises from the deep feature forgetting enforced by its theoretical
foundation, and recaps the need for improved robustness of machine unlearning
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical
  Advances, and Ethical Governance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhang Shao, Linrui Xu, Jinxi Wang, Wei Zhou, Xingyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification of Violin Reduction via Contour Lines Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philémon Beghin, Anne-Emmanuelle Ceulemans, François Glineur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Preferences are What You Need for <span class="highlight-title">Post-Train</span>ing: Selective
  Alignment Strategy for Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Preference Optimization for LLMs: A Bilevel Approach Beyond
  Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengtao Jian, Kai Yang, Ye Ouyang, Xiaozhou Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Gaussian Mixture Models-based Anomaly Detection for
  under-constrained Cable-Driven Parallel Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julio Garrido, Javier Vales, Diego Silva-Muñiz, Enrique Riveiro, Pablo López-Matencio, Josué Rivera-Andrade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cable-Driven Parallel Robots (CDPRs) are increasingly used for load
manipulation tasks involving predefined toolpaths with intermediate stops. At
each stop, where the platform maintains a fixed pose and the motors keep the
cables under tension, the system must evaluate whether it is safe to proceed by
detecting anomalies that could compromise performance (e.g., wind gusts or
cable impacts). This paper investigates whether anomalies can be detected using
only motor torque data, without additional sensors. It introduces an adaptive,
unsupervised outlier detection algorithm based on Gaussian Mixture Models
(GMMs) to identify anomalies from torque signals. The method starts with a
brief calibration period, just a few seconds, during which a GMM is fit on
known anomaly-free data. Real-time torque measurements are then evaluated using
Mahalanobis distance from the GMM, with statistically derived thresholds
triggering anomaly flags. Model parameters are periodically updated using the
latest segments identified as anomaly-free to adapt to changing conditions.
Validation includes 14 long-duration test sessions simulating varied wind
intensities. The proposed method achieves a 100% true positive rate and 95.4%
average true negative rate, with 1-second detection latency. Comparative
evaluation against power threshold and non-adaptive GMM methods indicates
higher robustness to drift and environmental variation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM
  question-answering capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hruday Markondapatnaikuni, Basem Suleiman, Abdelkarim Erradi, Shijing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shin'ya Yamaguchi, Kosuke Nishida, Daiki Chijiwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have demonstrated remarkable
capabilities by integrating pre-trained vision encoders with large language
models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting
has been adapted for LVLMs to enhance multi-modal reasoning by generating
intermediate rationales based on visual and textual inputs. While CoT is
assumed to improve grounding and accuracy in LVLMs, our experiments reveal a
key challenge: existing LVLMs often ignore the contents of generated rationales
in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as
a KL-constrained reward maximization focused on rationale-conditional
log-likelihood. As the optimal solution, we propose rationale-enhanced decoding
(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes
visual and rationale information by multiplying distinct image-conditional and
rationale-conditional next token distributions. Extensive experiments show that
RED consistently and significantly improves reasoning over standard CoT and
other decoding methods across multiple benchmarks and LVLMs. Our work offers a
practical and effective approach to improve both the faithfulness and accuracy
of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded
multi-modal systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Pole Structures of Hadronic States using Predictive Uncertainty
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Frohnert, Denny Lane B. Sombrillo, Evert van Nieuwenburg, Patrick Emonts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching theoretical predictions to experimental data remains a central
challenge in hadron spectroscopy. In particular, the identification of new
hadronic states is difficult, as exotic signals near threshold can arise from a
variety of physical mechanisms. A key diagnostic in this context is the pole
structure of the scattering amplitude, but different configurations can produce
similar signatures. The mapping between pole configurations and line shapes is
especially ambiguous near the mass threshold, where analytic control is
limited. In this work, we introduce an uncertainty-aware machine learning
approach for classifying pole structures in $S$-matrix elements. Our method is
based on an ensemble of classifier chains that provide both epistemic and
aleatoric uncertainty estimates. We apply a rejection criterion based on
predictive uncertainty, achieving a validation accuracy of nearly $95\%$ while
discarding only a small fraction of high-uncertainty predictions. Trained on
synthetic data with known pole structures, the model generalizes to previously
unseen experimental data, including enhancements associated with the
$P_{c\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole
structure, representing the presence of a genuine compact pentaquark in the
presence of a higher channel virtual state pole with non-vanishing width. While
evaluated on this particular state, our framework is broadly applicable to
other candidate hadronic states and offers a scalable tool for pole structure
inference in scattering amplitudes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fedor Rodionov, Abdelrahman Eldesokey, Michael Birsak, John Femiani, Bernard Ghanem, Peter Wonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 18 figures. Diagnostic benchmark for spatial reasoning in
  LLMs. Project page: https://OldDelorean.github.io/PlanQA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransformEEG: Towards Improving Model Generalizability in Deep
  Learning-based EEG Parkinson's Disease Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Del Pup, Riccardo Brun, Filippo Iotti, Edoardo Paccagnella, Mattia Pezzato, Sabrina Bertozzo, Andrea Zanola, Louis Fabrice Tshimanga, Henning Müller, Manfredo Atzori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) is establishing itself as an important,
low-cost, noninvasive diagnostic tool for the early detection of Parkinson's
Disease (PD). In this context, EEG-based Deep Learning (DL) models have shown
promising results due to their ability to discover highly nonlinear patterns
within the signal. However, current state-of-the-art DL models suffer from poor
generalizability caused by high inter-subject variability. This high
variability underscores the need for enhancing model generalizability by
developing new architectures better tailored to EEG data. This paper introduces
TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's
disease detection using EEG data. Unlike transformer models based on the EEGNet
structure, TransformEEG incorporates a depthwise convolutional tokenizer. This
tokenizer is specialized in generating tokens composed by channel-specific
features, which enables more effective feature mixing within the self-attention
layers of the transformer encoder. To evaluate the proposed model, four public
datasets comprising 290 subjects (140 PD patients, 150 healthy controls) were
harmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out
(N-LNSO) cross-validation was performed to provide an unbiased comparison
against seven other consolidated EEG deep learning models. TransformEEG
achieved the highest balanced accuracy's median (78.45%) as well as the lowest
interquartile range (6.37%) across all the N-LNSO partitions. When combined
with data augmentation and threshold correction, median accuracy increased to
80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG
produces more consistent and less skewed results. It demonstrates a substantial
reduction in variability and more reliable PD detection using EEG data compared
to the other investigated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for possible publication. GitHub repository: see
  https://github.com/MedMaxLab/transformeeg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards conservative inference in credal networks using belief
  functions: the case of credal chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Sangalli, Thomas Krak, Cassio de Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Generative AI: Multi-modal LLMs, <span class="highlight-title">Diffusion</span>s and the
  Unification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Yuwei Zhou, Bin Huang, Hong Chen, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal generative AI (Artificial Intelligence) has attracted increasing
attention from both academia and industry. Particularly, two dominant families
of techniques have emerged: i) Multi-modal large language models (LLMs)
demonstrate impressive ability for multi-modal understanding; and ii) Diffusion
models exhibit remarkable multi-modal powers in terms of multi-modal
generation. Therefore, this paper provides a comprehensive overview of
multi-modal generative AI, including multi-modal LLMs, diffusions, and the
unification for understanding and generation. To lay a solid foundation for
unified models, we first provide a detailed review of both multi-modal LLMs and
diffusion models respectively, including their probabilistic modeling
procedure, multi-modal architecture design, and advanced applications to
image/video LLMs as well as text-to-image/video generation. Furthermore, we
explore the emerging efforts toward unified models for understanding and
generation. To achieve the unification of understanding and generation, we
investigate key designs including autoregressive-based and diffusion-based
modeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then
introduce several strategies for unified models, analyzing their potential
advantages and disadvantages. In addition, we summarize the common datasets
widely used for multi-modal generative AI pretraining. Last but not least, we
present several challenging future research directions which may contribute to
the ongoing advancement of multi-modal generative AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theory of Inference Compute Scaling: Reasoning through Directed
  Stochastic Skill Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00004v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00004v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin R. Ellis-Mohr, Anuj K. Nayak, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Context-Faithfulness in Large Language Models: The Roles
  of Memory Strength and Evidence Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepei Li, Kang Zhou, Qiao Qiao, Bach Nguyen, Qing Wang, Qi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by
incorporating external information into the response generation process.
However, how context-faithful LLMs are and what factors influence LLMs' context
faithfulness remain largely unexplored. In this study, we investigate the
impact of memory strength and evidence presentation on LLMs' receptiveness to
external evidence. We quantify the memory strength of LLMs by measuring the
divergence in LLMs' responses to different paraphrases of the same question,
which is not considered by previous works. We also generate evidence in various
styles to examine LLMs' behavior. Our results show that for questions with high
memory strength, LLMs are more likely to rely on internal memory. Furthermore,
presenting paraphrased evidence significantly increases LLMs' receptiveness
compared to simple repetition or adding details. These findings provide key
insights for improving retrieval-augmented generation and context-aware LLMs.
Our code is available at https://github.com/liyp0095/ContextFaithful.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is published at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution
  Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05110v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05110v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixuan Liu, Yue He, Yunfei Wang, Hao Zou, Haoxiang Cheng, Wenjing Yang, Peng Cui, Zhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical rule learning, a prominent category of knowledge graph (KG) reasoning
methods, constitutes a critical research area aimed at learning explicit rules
from observed facts to infer missing knowledge. However, like all KG reasoning
methods, rule learning suffers from a critical weakness-its dependence on the
I.I.D. assumption. This assumption can easily be violated due to selection bias
during training or agnostic distribution shifts during testing (e.g., as in
query shift scenarios), ultimately undermining model performance and
reliability. To enable robust KG reasoning in wild environments, this study
investigates logical rule learning in the presence of agnostic test-time
distribution shifts. We formally define this challenge as out-of-distribution
(OOD) KG reasoning-a previously underexplored problem, and propose the Stable
Rule Learning (StableRule) framework as a solution. StableRule is an end-to-end
framework that combines feature decorrelation with rule learning network, to
enhance OOD generalization in KG reasoning. By leveraging feature
decorrelation, StableRule mitigates the adverse effects of covariate shifts
arising in OOD scenarios, improving the robustness of the rule learning
network. Extensive experiments on seven benchmark KGs demonstrate the
framework's superior effectiveness and stability across diverse heterogeneous
environments, highlighting its practical significance for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Establishing Best Practices for Building Rigorous Agentic Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02825v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02825v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellerman, Sarah Schwettmann, Matei Zaharia, Ion Stoica, Percy Liang, Daniel Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues in task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation of agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 15 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Vision <span class="highlight-title">Transformer</span> Representations Semantically Meaningful? A Case
  Study in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fuzzy Classification Aggregation for a Continuum of Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Image Modeling: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Hondru, Florinel Alin Croitoru, Shervin Minaee, Radu Tudor Ionescu, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we survey recent studies on masked image modeling (MIM), an
approach that emerged as a powerful self-supervised learning technique in
computer vision. The MIM task involves masking some information, e.g. pixels,
patches, or even latent representations, and training a model, usually an
autoencoder, to predicting the missing information by using the context
available in the visible part of the input. We identify and formalize two
categories of approaches on how to implement MIM as a pretext task, one based
on reconstruction and one based on contrastive learning. Then, we construct a
taxonomy and review the most prominent papers in recent years. We complement
the manually constructed taxonomy with a dendrogram obtained by applying a
hierarchical clustering algorithm. We further identify relevant clusters via
manually inspecting the resulting dendrogram. Our review also includes datasets
that are commonly used in MIM research. We aggregate the performance results of
various masked image modeling methods on the most popular datasets, to
facilitate the comparison of competing methods. Finally, we identify research
gaps and propose several interesting directions of future work. We supplement
our survey with the following public repository containing organized
references: https://github.com/vladhondru25/MIM-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Has a Foundation Model Found? Using Inductive Bias to Probe for
  Wo<span class="highlight-title">rl</span>d Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyon Vafa, Peter G. Chang, Ashesh Rambachan, Sendhil Mullainathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Uncertainty Quantification for Depression Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghong Li, Xiuzhuang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthy depression prediction based on deep learning, incorporating both
predictive reliability and algorithmic fairness across diverse demographic
groups, is crucial for clinical application. Recently, achieving reliable
depression predictions through uncertainty quantification has attracted
increasing attention. However, few studies have focused on the fairness of
uncertainty quantification (UQ) in depression prediction. In this work, we
investigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage
(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for
depression prediction. FUQ pursues reliable and fair depression predictions
through group-based analysis. Specifically, we first group all the participants
by different sensitive attributes and leverage conformal prediction to quantify
uncertainty within each demographic group, which provides a theoretically
guaranteed and valid way to quantify uncertainty for depression prediction and
facilitates the investigation of fairness across different demographic groups.
Furthermore, we propose a fairness-aware optimization strategy that formulates
fairness as a constrained optimization problem under EOC constraints. This
enables the model to preserve predictive reliability while adapting to the
heterogeneous uncertainty levels across demographic groups, thereby achieving
optimal fairness. Through extensive evaluations on several visual and audio
depression datasets, our approach demonstrates its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Studying and Improving Graph Neural Network-based Motif Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro C. Vieira, Miguel E. P. Silva, Pedro Manuel Pinto Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript represents a revised version from the paper on
  https://openreview.net/forum?id=PZVVOeu6xx. Still a work in progress.
  Comments are welcome! 23 pages (12 main text + references), 9 figures, 5
  tables. (Second update: More accurate Table 4, Run time comparisons.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dark Side of LLMs: Agent-based Attacks for Complete Computer
  Takeover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, Angelo Furfaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of Large Language Model (LLM) agents and multi-agent
systems enables unprecedented capabilities in natural language processing and
generation. However, these systems have introduced unprecedented security
vulnerabilities that extend beyond traditional prompt injection attacks. This
paper presents the first comprehensive evaluation of LLM agents as attack
vectors capable of achieving complete computer takeover through the
exploitation of trust boundaries within agentic AI systems where autonomous
entities interact and influence each other. We demonstrate that adversaries can
leverage three distinct attack surfaces - direct prompt injection, RAG backdoor
attacks, and inter-agent trust exploitation - to coerce popular LLMs (including
GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing
malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals
an alarming vulnerability hierarchy: while 41.2% of models succumb to direct
prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical
82.4% can be compromised through inter-agent trust exploitation. Notably, we
discovered that LLMs which successfully resist direct malicious commands will
execute identical payloads when requested by peer agents, revealing a
fundamental flaw in current multi-agent security models. Our findings
demonstrate that only 5.9% of tested models (1/17) proved resistant to all
attack vectors, with the majority exhibiting context-dependent security
behaviors that create exploitable blind spots. Our findings also highlight the
need to increase awareness and research on the security risks of LLMs, showing
a paradigm shift in cybersecurity threats, where AI tools themselves become
sophisticated attack vectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A
  Lightweight Benchmark for Probing Foundational Controllability Components <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Potham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. This paper introduces a lightweight,
interpretable benchmark to evaluate an LLM agent's ability to uphold a
high-level safety principle when faced with conflicting task instructions. Our
evaluation of six LLMs reveals two primary findings: (1) a quantifiable "cost
of compliance" where safety constraints degrade task performance even when
compliant solutions exist, and (2) an "illusion of compliance" where high
adherence often masks task incompetence rather than principled choice. These
findings provide initial evidence that while LLMs can be influenced by
hierarchical directives, current approaches lack the consistency required for
reliable safety governance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This work has been submitted to the Technical AI Governance
  Workshop at ICML 2025 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrain Alignment with Sparse Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07618v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07618v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Yin, Chak Tou Leong, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using
sparse features activated in a well-trained sparse autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Symmetry Breaking in Physical Systems with Relaxed Group
  Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02299v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02299v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Wang, Elyssa Hofgard, Han Gao, Robin Walters, Tess E. Smidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling symmetry breaking is essential for understanding the fundamental
changes in the behaviors and properties of physical systems, from microscopic
particle interactions to macroscopic phenomena like fluid dynamics and cosmic
structures. Thus, identifying sources of asymmetry is an important tool for
understanding physical systems. In this paper, we focus on learning asymmetries
of data using relaxed group convolutions. We provide both theoretical and
empirical evidence that this flexible convolution technique allows the model to
maintain the highest level of equivariance that is consistent with data and
discover the subtle symmetry-breaking factors in various physical systems. We
employ various relaxed group convolution architectures to uncover various
symmetry-breaking factors that are interpretable and physically meaningful in
different physical systems, including the phase transition of crystal
structure, the isotropy and homogeneity breaking in turbulent flow, and the
time-reversal symmetry breaking in pendulum systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAEBE: Multi-Agent Emergent Behavior Framework <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sinem Erisken, Timothy Gothard, Martin Leitgab, Ram Potham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This work has been submitted to the Multi-Agent Systems
  Workshop at ICML 2025 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VOTE: Vision-Language-Action Optimization with Trajectory Ensemble
  Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyi Lin, Amir Taherin, Arash Akbari, Arman Akbari, Lei Lu, Guangyu Chen, Taskin Padir, Xiaomeng Yang, Weiwei Chen, Yiqian Li, Xue Lin, David Kaeli, Pu Zhao, Yanzhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35x faster inference and 145 Hz throughput.
All the details and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Algorithm for Learning Smaller Representations of Models With Scarce
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.07990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.07990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian de Wynter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm for solving binary classification problems when the
dataset is not fully representative of the problem being solved, and obtaining
more data is not possible. It relies on a trained model with loose accuracy
constraints, an iterative hyperparameter searching-and-pruning procedure over a
search space $\Theta$, and a data-generating function. Our algorithm works by
reconstructing up to homology the manifold on which lies the support of the
underlying distribution. We provide an analysis on correctness and runtime
complexity under ideal conditions and an extension to deep neural networks. In
the former case, if $\size{\Theta}$ is the number of hyperparameter sets in the
search space, this algorithm returns a solution that is up to $2(1 -
{2^{-\size{\Theta}}})$ times better than simply training with an enumeration of
$\Theta$ and picking the best model. As part of our analysis we also prove that
an open cover of a dataset has the same homology as the manifold on which lies
the support of the underlying probability distribution, if and only said
dataset is learnable. This latter result acts as a formal argument to explain
the effectiveness of data expansion techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Information Geometry--see the journal for the final,
  authenticated version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving the Hubbard model with Neural Quantum States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntian Gu, Wenrui Li, Heng Lin, Bo Zhan, Ruichen Li, Yifei Huang, Di He, Yantao Wu, Tao Xiang, Mingpu Qin, Liwei Wang, Dingshun Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of neural quantum states (NQS) has established it as a
promising framework for studying quantum many-body systems. In this work, by
leveraging the cutting-edge transformer-based architectures and developing
highly efficient optimization algorithms, we achieve the state-of-the-art
results for the doped two-dimensional (2D) Hubbard model, arguably the minimum
model for high-Tc superconductivity. Interestingly, we find different attention
heads in the NQS ansatz can directly encode correlations at different scales,
making it capable of capturing long-range correlations and entanglements in
strongly correlated systems. With these advances, we establish the half-filled
stripe in the ground state of 2D Hubbard model with the next nearest
neighboring hoppings, consistent with experimental observations in cuprates.
Our work establishes NQS as a powerful tool for solving challenging
many-fermions systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding AI Judgment: How LLMs Assess News Credibility and Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Loru, Jacopo Nudo, Niccolò Di Marco, Alessandro Santirocchi, Roberto Atzeni, Matteo Cinelli, Vincenzo Cestari, Clelia Rossi-Arnaud, Walter Quattrociocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly embedded in workflows that
involve evaluative processes. This raises the need to examine how such
evaluations are built, what assumptions they rely on, and how their strategies
diverge from those of humans. We benchmark six LLMs against expert
ratings--NewsGuard and Media Bias/Fact Check (MBFC)--and against human
judgments collected through a controlled experiment. To enable direct
comparison, we implement a structured agentic framework in which both models
and non-expert participants follow the same evaluation procedure: selecting
criteria, retrieving content, and producing justifications. Despite output
alignment, LLMs rely on different mechanisms: lexical associations and
statistical priors replace contextual reasoning. This reliance produces
systematic effects: political asymmetries, opaque justifications, and a
tendency to confuse linguistic form with epistemic validity. Delegating
judgment to such systems does not merely automate evaluation--it redefines it,
shifting from normative reasoning to pattern-based approximation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Chain-of-Thought in LLMs through Information Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Francois Ton, Muhammad Faaiz Taufiq, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive performance in complex
reasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing
models to break down problems into manageable sub-tasks. However, existing CoT
evaluation techniques either require annotated CoT data or fall short in
accurately assessing intermediate reasoning steps, leading to high rates of
false positives. In this paper, we formalize CoT reasoning in LLMs through an
information-theoretic lens. Specifically, our framework quantifies the
`information-gain' at each reasoning step, enabling the identification of
failure modes in LLMs without the need for expensive annotated datasets. We
demonstrate the efficacy of our approach through extensive experiments on toy
arithmetic, GSM8K and PRM800k datasets, where it significantly outperforms
existing outcome-based methods by providing more accurate insights into model
performance on individual subtasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Automata Learning via Discrete Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lutz, Daniil Kaminskyi, Florian Wittbold, Simon Dierl, Falk Howar, Barbara König, Emmanuel Müller, Daniel Neider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automata learning is a successful tool for many application domains such as
robotics and automatic verification. Typically, automata learning techniques
operate in a supervised learning setting (active or passive) where they learn a
finite state machine in contexts where additional information, such as labeled
system executions, is available. However, other settings, such as learning from
unlabeled data - an important aspect in machine learning - remain unexplored.
To overcome this limitation, we propose a framework for learning a
deterministic finite automaton (DFA) from a given multi-set of unlabeled words.
We show that this problem is computationally hard and develop three learning
algorithms based on constraint optimization. Moreover, we introduce novel
regularization schemes for our optimization problems that improve the overall
interpretability of our DFAs. Using a prototype implementation, we demonstrate
practical feasibility in the context of unsupervised anomaly detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Algorithms in the Limit <span class="chip">COLT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hristo Papazov, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of learning computable functions in the limit
by extending Gold's inductive inference framework to incorporate
\textit{computational observations} and \textit{restricted input sources}.
Complimentary to the traditional Input-Output Observations, we introduce
Time-Bound Observations, and Policy-Trajectory Observations to study the
learnability of general recursive functions under more realistic constraints.
While input-output observations do not suffice for learning the class of
general recursive functions in the limit, we overcome this learning barrier by
imposing computational complexity constraints or supplementing with approximate
time-bound observations. Further, we build a formal framework around
observations of \textit{computational agents} and show that learning computable
functions from policy trajectories reduces to learning rational functions from
input and output, thereby revealing interesting connections to finite-state
transducer inference. On the negative side, we show that computable or
polynomial-mass characteristic sets cannot exist for the class of linear-time
computable functions even for policy-trajectory observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLT 2025. This version matches the proceedings version
  apart from a small notational change in section 3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PWD: Prior-Guided and Wavelet-Enhanced <span class="highlight-title">Diffusion</span> Model for Limited-Angle
  CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models have received increasing attention in medical
imaging, particularly in limited-angle computed tomography (LACT). Standard
diffusion models achieve high-quality image reconstruction but require a large
number of sampling steps during inference, resulting in substantial
computational overhead. Although skip-sampling strategies have been proposed to
improve efficiency, they often lead to loss of fine structural details. To
address this issue, we propose a prior information embedding and wavelet
feature fusion fast sampling diffusion model for LACT reconstruction. The PWD
enables efficient sampling while preserving reconstruction fidelity in LACT,
and effectively mitigates the degradation typically introduced by
skip-sampling. Specifically, during the training phase, PWD maps the
distribution of LACT images to that of fully sampled target images, enabling
the model to learn structural correspondences between them. During inference,
the LACT image serves as an explicit prior to guide the sampling trajectory,
allowing for high-quality reconstruction with significantly fewer steps. In
addition, PWD performs multi-scale feature fusion in the wavelet domain,
effectively enhancing the reconstruction of fine details by leveraging both
low-frequency and high-frequency information. Quantitative and qualitative
evaluations on clinical dental arch CBCT and periapical datasets demonstrate
that PWD outperforms existing methods under the same sampling condition. Using
only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and
10% gain in SSIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deontic Temporal Logic for Formal Verification of AI Ethics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05765v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05765v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priya T. V., Shrisha Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring ethical behavior in Artificial Intelligence (AI) systems amidst
their increasing ubiquity and influence is a major concern the world over. The
use of formal methods in AI ethics is a possible crucial approach for
specifying and verifying the ethical behavior of AI systems. This paper
proposes a formalization based on deontic logic to define and evaluate the
ethical behavior of AI systems, focusing on system-level specifications,
contributing to this important goal. It introduces axioms and theorems to
capture ethical requirements related to fairness and explainability. The
formalization incorporates temporal operators to reason about the ethical
behavior of AI systems over time. The authors evaluate the effectiveness of
this formalization by assessing the ethics of the real-world COMPAS and loan
prediction AI systems. Various ethical properties of the COMPAS and loan
prediction systems are encoded using deontic logical formulas, allowing the use
of an automated theorem prover to verify whether these systems satisfy the
defined properties. The formal verification reveals that both systems fail to
fulfill certain key ethical properties related to fairness and
non-discrimination, demonstrating the effectiveness of the proposed
formalization in identifying potential ethical issues in real-world AI
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning
  for Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary version, v2, added more details and corrected some minor
  mistakes. Project page: https://anitaleungxx.github.io/ReMix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Representations for Fine-grained Multi-label Critical View
  of Safety Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Britty Baby, Vinkle Srivastav, Pooja P. Jain, Kun Yuan, Pietro Mascagni, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Critical View of Safety (CVS) is crucial for safe laparoscopic
cholecystectomy, yet assessing CVS criteria remains a complex and challenging
task, even for experts. Traditional models for CVS recognition depend on
vision-only models learning with costly, labor-intensive spatial annotations.
This study investigates how text can be harnessed as a powerful tool for both
training and inference in multi-modal surgical foundation models to automate
CVS recognition. Unlike many existing multi-modal models, which are primarily
adapted for multi-class classification, CVS recognition requires a multi-label
framework. Zero-shot evaluation of existing multi-modal surgical models shows a
significant performance gap for this task. To address this, we propose
CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,
binary classification across multiple labels by aligning image embeddings with
textual descriptions of each CVS criterion using positive and negative prompts.
By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the
Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the
ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that
CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,
boosts CVS recognition over image-only methods. We also propose text-specific
inference methods, that helps in analysing the image-text alignment. While
further work is needed to match state-of-the-art spatial annotation-based
methods, this approach highlights the potential of adapting generalist models
to specialized surgical tasks. Code:
https://github.com/CAMMA-public/CVS-AdaptNet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation of Multi-modal Representation Models for Multi-task Surgical
  Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Walimbe, Britty Baby, Vinkle Srivastav, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical AI often involves multiple tasks within a single procedure, like
phase recognition or assessing the Critical View of Safety in laparoscopic
cholecystectomy. Traditional models, built for one task at a time, lack
flexibility, requiring a separate model for each. To address this, we introduce
MML-SurgAdapt, a unified multi-task framework with Vision-Language Models
(VLMs), specifically CLIP, to handle diverse surgical tasks through natural
language supervision. A key challenge in multi-task learning is the presence of
partial annotations when integrating different tasks. To overcome this, we
employ Single Positive Multi-Label (SPML) learning, which traditionally reduces
annotation burden by training models with only one positive label per instance.
Our framework extends this approach to integrate data from multiple surgical
tasks within a single procedure, enabling effective learning despite incomplete
or noisy annotations. We demonstrate the effectiveness of our model on a
combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,
utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt
performs comparably to task-specific benchmarks, with the added advantage of
handling noisy annotations. It also outperforms the existing SPML frameworks
for the task. By reducing the required labels by 23%, our approach proposes a
more scalable and efficient labeling process, significantly easing the
annotation burden on clinicians. To our knowledge, this is the first
application of SPML to integrate data from multiple surgical tasks, presenting
a novel and generalizable solution for multi-task learning in surgical computer
vision. Implementation is available at:
https://github.com/CAMMA-public/MML-SurgAdapt
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What do <span class="highlight-title">self-supervised</span> speech models know about Dutch? Analyzing
  advantages of language-specific <span class="highlight-title">pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.00981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.00981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marianne de Heer Kloots, Hosein Mohebbi, Charlotte Pouw, Gaofei Shen, Willem Zuidema, Martijn Bentum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How language-specific are speech representations learned by self-supervised
models? Existing work has shown that a range of linguistic features can be
successfully decoded from end-to-end models trained only on speech recordings.
However, it's less clear to what extent pre-training on specific languages
improves language-specific linguistic information. Here we test the encoding of
Dutch phonetic and lexical information in internal representations of
self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the
representation of Dutch linguistic features as compared to pre-training on
similar amounts of English or larger amounts of multilingual data. This
language-specific advantage is well-detected by trained clustering or
classification probes, and partially observable using zero-shot metrics.
Furthermore, the language-specific benefit on linguistic feature encoding
aligns with downstream performance on Automatic Speech Recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2025. For model, code, and materials, see
  https://github.com/mdhk/SSL-NL-eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Access Controls Will Solve the Dual-Use Dilemma <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evžen Wybitul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI safety systems face the dual-use dilemma: it can be unclear whether to
refuse certain requests, since they could be either harmless or harmful
depending on who made them and why. Determining this requires examining their
real-world context, but current safety systems cannot access this contextual
information. Instead, they make arbitrary decisions that end up hurting both
utility and safety: they sometimes refuse legitimate queries and other times
fail to refuse harmful ones. To address this, we propose a conceptual framework
based on access controls in which only verified users can access dual-use
outputs. We describe the framework's components, analyse its feasibility, and
explain how it addresses both over-refusals and under-refusals. While only a
high-level proposal, our work takes the first step toward enabling more nuanced
safety decisions: with better tools for managing dual-use content, model
providers could enable users to access more capabilities without sacrificing
safety, and give regulators new options for more targeted policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025 Workshop on Technical AI Governance (TAIG)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ethical Concerns of Generative AI and Mitigation Strategies: A
  Systematic Mapping Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutan Huang, Chetan Arora, Wen Cheng Houng, Tanjila Kanij, Anuradha Madulgalla, John Grundy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  [Context] Generative AI technologies, particularly Large Language Models
(LLMs), have transformed numerous domains by enhancing convenience and
efficiency in information retrieval, content generation, and decision-making
processes. However, deploying LLMs also presents diverse ethical challenges,
and their mitigation strategies remain complex and domain-dependent.
[Objective] This paper aims to identify and categorize the key ethical concerns
associated with using LLMs, examine existing mitigation strategies, and assess
the outstanding challenges in implementing these strategies across various
domains. [Method] We conducted a systematic mapping study, reviewing 39 studies
that discuss ethical concerns and mitigation strategies related to LLMs. We
analyzed these ethical concerns using five ethical dimensions that we extracted
based on various existing guidelines, frameworks, and an analysis of the
mitigation strategies and implementation challenges. [Results] Our findings
reveal that ethical concerns in LLMs are multi-dimensional and
context-dependent. While proposed mitigation strategies address some of these
concerns, significant challenges still remain. [Conclusion] Our results
highlight that ethical issues often hinder the practical implementation of the
mitigation strategies, particularly in high-stake areas like healthcare and
public governance; existing frameworks often lack adaptability, failing to
accommodate evolving societal expectations and diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Negative Mining For Temporal Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Chen, Tongya Zheng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal networks are effective in capturing the evolving interactions of
networks over time, such as social networks and e-commerce networks. In recent
years, researchers have primarily concentrated on developing specific model
architectures for Temporal Graph Neural Networks (TGNNs) in order to improve
the representation quality of temporal nodes and edges. However, limited
attention has been given to the quality of negative samples during the training
of TGNNs. When compared with static networks, temporal networks present two
specific challenges for negative sampling: positive sparsity and positive
shift. Positive sparsity refers to the presence of a single positive sample
amidst numerous negative samples at each timestamp, while positive shift
relates to the variations in positive samples across different timestamps. To
robustly address these challenges in training TGNNs, we introduce Curriculum
Negative Mining (CurNM), a model-aware curriculum learning framework that
adaptively adjusts the difficulty of negative samples. Within this framework,
we first establish a dynamically updated negative pool that balances random,
historical, and hard negatives to address the challenges posed by positive
sparsity. Secondly, we implement a temporal-aware negative selection module
that focuses on learning from the disentangled factors of recently active
edges, thus accurately capturing shifting preferences. Finally, the selected
negatives are combined with annealing random negatives to support stable
training. Extensive experiments on 12 datasets and 3 TGNNs demonstrate that our
method outperforms baseline methods by a significant margin. Additionally,
thorough ablation studies and parameter sensitivity experiments verify the
usefulness and robustness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2FGL: Spatial Spectral Federated Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Statistics - Machine Learning <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why is Your Language Model a Poor Implicit Reward Model? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Razin, Yong Lin, Jiarui Yao, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Action Chunking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyang Li, Zhiyuan Zhou, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prospective Learning in Retrospect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Bai, Cecelia Shuai, Ashwin De Silva, Siyu Yu, Pratik Chaudhari, Joshua T. Vogelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In most real-world applications of artificial intelligence, the distributions
of the data and the goals of the learners tend to change over time. The
Probably Approximately Correct (PAC) learning framework, which underpins most
machine learning algorithms, fails to account for dynamic data distributions
and evolving objectives, often resulting in suboptimal performance. Prospective
learning is a recently introduced mathematical framework that overcomes some of
these limitations. We build on this framework to present preliminary results
that improve the algorithm and numerical results, and extend prospective
learning to sequential decision-making scenarios, specifically foraging. Code
is available at: https://github.com/neurodata/prolearn2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AGI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Late Fusion Multi-task Learning for Semiparametric Inference with
  Nuisance Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohom Bhattacharya, Yongzhuo Chen, Muxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of large and heterogeneous datasets, the integration of
information from diverse sources is essential to improve parameter estimation.
Multi-task learning offers a powerful approach by enabling simultaneous
learning across related tasks. In this work, we introduce a late fusion
framework for multi-task learning with semiparametric models that involve
infinite-dimensional nuisance parameters, focusing on applications such as
heterogeneous treatment effect estimation across multiple data sources,
including electronic health records from different hospitals or clinical trial
data. Our framework is two-step: first, initial double machine-learning
estimators are obtained through individual task learning; second, these
estimators are adaptively aggregated to exploit task similarities while
remaining robust to task-specific differences. In particular, the framework
avoids individual level data sharing, preserving privacy. Additionally, we
propose a novel multi-task learning method for nuisance parameter estimation,
which further enhances parameter estimation when nuisance parameters exhibit
similarity across tasks. We establish theoretical guarantees for the method,
demonstrating faster convergence rates compared to individual task learning
when tasks share similar parametric components. Extensive simulations and real
data applications complement the theoretical findings of our work while
highlight the effectiveness of our framework even in moderate sample sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-Train</span>ed AI Model Assisted Online Decision-Making under Missing
  Covariates: A Theoretical Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichen Hu, David Simchi-Levi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a sequential contextual decision-making problem in which certain
covariates are missing but can be imputed using a pre-trained AI model. From a
theoretical perspective, we analyze how the presence of such a model influences
the regret of the decision-making process. We introduce a novel notion called
"model elasticity", which quantifies the sensitivity of the reward function to
the discrepancy between the true covariate and its imputed counterpart. This
concept provides a unified way to characterize the regret incurred due to model
imputation, regardless of the underlying missingness mechanism. More
surprisingly, we show that under the missing at random (MAR) setting, it is
possible to sequentially calibrate the pre-trained model using tools from
orthogonal statistical learning and doubly robust regression. This calibration
significantly improves the quality of the imputed covariates, leading to much
better regret guarantees. Our analysis highlights the practical value of having
an accurate pre-trained model in sequential decision-making tasks and suggests
that model elasticity may serve as a fundamental metric for understanding and
improving the integration of pre-trained models in a wide range of data-driven
decision-making problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces
  and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Mirzaei, Andreas Maurer, Vladimir R. Kostic, Massimiliano Pontil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from non-independent and non-identically distributed data poses a
persistent challenge in statistical learning. In this study, we introduce
data-dependent Bernstein inequalities tailored for vector-valued processes in
Hilbert space. Our inequalities apply to both stationary and non-stationary
processes and exploit the potential rapid decay of correlations between
temporally separated variables to improve estimation. We demonstrate the
utility of these bounds by applying them to covariance operator estimation in
the Hilbert-Schmidt norm and to operator learning in dynamical systems,
achieving novel risk bounds. Finally, we perform numerical experiments to
illustrate the practical implications of these bounds in both contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In The 28th International Conference on Artificial Intelligence and
  Statistics (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Empirical Risk Minimization Framework for Flexible N-Tuples
  Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuying Huang, Junpeng Li, Changchun Hua, Yana Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To alleviate the annotation burden in supervised learning, N-tuples learning
has recently emerged as a powerful weakly-supervised method. While existing
N-tuples learning approaches extend pairwise learning to higher-order
comparisons and accommodate various real-world scenarios, they often rely on
task-specific designs and lack a unified theoretical foundation. In this paper,
we propose a general N-tuples learning framework based on empirical risk
minimization, which systematically integrates pointwise unlabeled data to
enhance learning performance. This paper first unifies the data generation
processes of N-tuples and pointwise unlabeled data under a shared probabilistic
formulation. Based on this unified view, we derive an unbiased empirical risk
estimator that generalizes a broad class of existing N-tuples models. We
further establish a generalization error bound for theoretical support. To
demonstrate the flexibility of the framework, we instantiate it in four
representative weakly supervised scenarios, each recoverable as a special case
of our general model. Additionally, to address overfitting issues arising from
negative risk terms, we adopt correction functions to adjust the empirical
risk. Extensive experiments on benchmark datasets validate the effectiveness of
the proposed framework and demonstrate that leveraging pointwise unlabeled data
consistently improves generalization across various N-tuples learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast
  Rolling One-Step-Ahead Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Liu, Zihan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-series models like ARIMA remain widely used for forecasting but limited
to linear assumptions and high computational cost in large and complex
datasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA
and replace it with a flexible spline-based function estimated by Galerkin
projection. This enables the model to capture nonlinear dependencies in lagged
values and retain the MA component and Gaussian noise assumption. We derive a
closed-form OLS estimator for the Galerkin coefficients and show the model is
asymptotically unbiased and consistent under standard conditions. Our method
bridges classical time-series modeling and nonparametric regression, which
offering improved forecasting performance and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hess-MC2: Sequential Monte Ca<span class="highlight-title">rl</span>o Squared using Hessian Information and
  Second Order Proposals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Murphy, Conor Rosato, Andrew Millard, Lee Devlin, Paul Horridge, Simon Maskell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When performing Bayesian inference using Sequential Monte Carlo (SMC)
methods, two considerations arise: the accuracy of the posterior approximation
and computational efficiency. To address computational demands, Sequential
Monte Carlo Squared (SMC$^2$) is well-suited for high-performance computing
(HPC) environments. The design of the proposal distribution within SMC$^2$ can
improve accuracy and exploration of the posterior as poor proposals may lead to
high variance in importance weights and particle degeneracy. The
Metropolis-Adjusted Langevin Algorithm (MALA) uses gradient information so that
particles preferentially explore regions of higher probability. In this paper,
we extend this idea by incorporating second-order information, specifically the
Hessian of the log-target. While second-order proposals have been explored
previously in particle Markov Chain Monte Carlo (p-MCMC) methods, we are the
first to introduce them within the SMC$^2$ framework. Second-order proposals
not only use the gradient (first-order derivative), but also the curvature
(second-order derivative) of the target distribution. Experimental results on
synthetic models highlight the benefits of our approach in terms of step-size
selection and posterior approximation accuracy when compared to other
proposals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Machine Learning Signal Processing conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-free regression kriging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Luo, Yilong Wu, Yongze Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial interpolation is a crucial task in geography. As perhaps the most
widely used interpolation methods, geostatistical models -- such as Ordinary
Kriging (OK) -- assume spatial stationarity, which makes it difficult to
capture the nonstationary characteristics of geographic variables. A common
solution is trend surface modeling (e.g., Regression Kriging, RK), which relies
on external explanatory variables to model the trend and then applies
geostatistical interpolation to the residuals. However, this approach requires
high-quality and readily available explanatory variables, which are often
lacking in many spatial interpolation scenarios -- such as estimating heavy
metal concentrations underground. This study proposes a Feature-Free Regression
Kriging (FFRK) method, which automatically extracts geospatial features --
including local dependence, local heterogeneity, and geosimilarity -- to
construct a regression-based trend surface without requiring external
explanatory variables. We conducted experiments on the spatial distribution
prediction of three heavy metals in a mining area in Australia. In comparison
with 17 classical interpolation methods, the results indicate that FFRK, which
does not incorporate any explanatory variables and relies solely on extracted
geospatial features, consistently outperforms both conventional Kriging
techniques and machine learning models that depend on explanatory variables.
This approach effectively addresses spatial nonstationarity while reducing the
cost of acquiring explanatory variables, improving both prediction accuracy and
generalization ability. This finding suggests that an accurate characterization
of geospatial features based on domain knowledge can significantly enhance
spatial prediction performance -- potentially yielding greater improvements
than merely adopting more advanced statistical models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Oriented Sequential Bayesian Experimental Design for Causal
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyu Zhang, Jiayuan Dong, Jie Liu, Xun Huan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GO-CBED, a goal-oriented Bayesian framework for sequential causal
experimental design. Unlike conventional approaches that select interventions
aimed at inferring the full causal model, GO-CBED directly maximizes the
expected information gain (EIG) on user-specified causal quantities of
interest, enabling more targeted and efficient experimentation. The framework
is both non-myopic, optimizing over entire intervention sequences, and
goal-oriented, targeting only model aspects relevant to the causal query. To
address the intractability of exact EIG computation, we introduce a variational
lower bound estimator, optimized jointly through a transformer-based policy
network and normalizing flow-based variational posteriors. The resulting policy
enables real-time decision-making via an amortized network. We demonstrate that
GO-CBED consistently outperforms existing baselines across various causal
reasoning and discovery tasks-including synthetic structural causal models and
semi-synthetic gene regulatory networks-particularly in settings with limited
experimental budgets and complex causal mechanisms. Our results highlight the
benefits of aligning experimental design objectives with specific research
goals and of forward-looking sequential planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bilevel Optimization Framework for Imbalanced Data Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Medlin, Sven Leyffer, Krishnan Raghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data rebalancing techniques, including oversampling and undersampling, are a
common approach to addressing the challenges of imbalanced data. To tackle
unresolved problems related to both oversampling and undersampling, we propose
a new undersampling approach that: (i) avoids the pitfalls of noise and overlap
caused by synthetic data and (ii) avoids the pitfall of under-fitting caused by
random undersampling. Instead of undersampling majority data randomly, our
method undersamples datapoints based on their ability to improve model loss.
Using improved model loss as a proxy measurement for classification
performance, our technique assesses a datapoint's impact on loss and rejects
those unable to improve it. In so doing, our approach rejects majority
datapoints redundant to datapoints already accepted and, thereby, finds an
optimal subset of majority training data for classification. The accept/reject
component of our algorithm is motivated by a bilevel optimization problem
uniquely formulated to identify the optimal training set we seek. Experimental
results show our proposed technique with F1 scores up to 10% higher than
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Predictability of Performative, Social Events <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan C. Perdomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social predictions do not passively describe the future; they actively shape
it. They inform actions and change individual expectations in ways that
influence the likelihood of the predicted outcome. Given these dynamics, to
what extent can social events be predicted? This question was discussed
throughout the 20th century by authors like Merton, Morgenstern, Simon, and
others who considered it a central issue in social science methodology. In this
work, we provide a modern answer to this old problem. Using recent ideas from
performative prediction and outcome indistinguishability, we establish that one
can always efficiently predict social events accurately, regardless of how
predictions influence data. While achievable, we also show that these
predictions are often undesirable, highlighting the limitations of previous
desiderata. We end with a discussion of various avenues forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning is Not So Mysterious or Different <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Gordon Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are often seen as different from other model classes by
defying conventional notions of generalization. Popular examples of anomalous
generalization behaviour include benign overfitting, double descent, and the
success of overparametrization. We argue that these phenomena are not distinct
to neural networks, or particularly mysterious. Moreover, this generalization
behaviour can be intuitively understood, and rigorously characterized, using
long-standing generalization frameworks such as PAC-Bayes and countable
hypothesis bounds. We present soft inductive biases as a key unifying principle
in explaining these phenomena: rather than restricting the hypothesis space to
avoid overfitting, embrace a flexible hypothesis space, with a soft preference
for simpler solutions that are consistent with the data. This principle can be
encoded in many model classes, and thus deep learning is not as mysterious or
different from other model classes as it might seem. However, we also highlight
how deep learning is relatively distinct in other ways, such as its ability for
representation learning, phenomena such as mode connectivity, and its relative
universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coefficient Shape Transfer Learning for Functional Linear Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhao Jiao, Ian W. Mckeague, N. -H. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a novel transfer learning methodology to tackle the
challenge of data scarcity in functional linear models. The methodology
incorporates samples from the target model (target domain) alongside those from
auxiliary models (source domains), transferring knowledge of coefficient shape
from the source domains to the target domain. This shape-based knowledge
transfer offers two key advantages. First, it is robust to covariate scaling,
ensuring effectiveness despite variations in data distributions across
different source domains. Second, the notion of coefficient shape homogeneity
represents a meaningful advance beyond traditional coefficient homogeneity,
allowing the method to exploit a wider range of source domains and achieve
significantly improved model estimation. We rigorously analyze the convergence
rates of the proposed estimator and examine the minimax optimality. Our
findings show that the degree of improvement depends not only on the similarity
of coefficient shapes between the target and source domains, but also on
coefficient magnitudes and the spectral decay rates of the functional
covariates covariance operators. To address situations where only a subset of
auxiliary models is informative for the target model, we further develop a
data-driven procedure for identifying such informative sources. The
effectiveness of the proposed methodology is demonstrated through comprehensive
simulation studies and an application to occupation time analysis using
physical activity data from the U.S. National Health and Nutrition Examination
Survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Distributed Estimation: Extending Gossip Algorithms to Ranking
  and Trimmed Means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17836v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17836v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Van Elst, Igor Colin, Stephan Clémençon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of robust estimation in gossip algorithms
over arbitrary communication graphs. Gossip algorithms are fully decentralized,
relying only on local neighbor-to-neighbor communication, making them
well-suited for situations where communication is constrained. A fundamental
challenge in existing mean-based gossip algorithms is their vulnerability to
malicious or corrupted nodes. In this paper, we show that an outlier-robust
mean can be computed by globally estimating a robust statistic. More
specifically, we propose a novel gossip algorithm for rank estimation, referred
to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated
to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed
description of the proposed methods, a key contribution of our work is a
precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank
estimation and an $\mathcal{O}(1 / {t})$ rate for trimmed mean estimation,
where by $t$ is meant the number of iterations. Moreover, we provide a
breakdown point analysis of \textsc{GoTrim}. We empirically validate our
theoretical results through experiments on diverse network topologies, data
distributions and contamination schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LARP: Learner-Agnostic Robust Data Prefiltering <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20573v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20573v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristian Minchev, Dimitar Iliev Dimitrov, Nikola Konstantinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread availability of large public datasets is a key factor behind
the recent successes of statistical inference and machine learning methods.
However, these datasets often contain some low-quality or contaminated data, to
which many learning procedures are sensitive. Therefore, the question of
whether and how public datasets should be prefiltered to facilitate accurate
downstream learning arises. On a technical level this requires the construction
of principled data prefiltering methods which are learner-agnostic robust, in
the sense of provably protecting a set of pre-specified downstream learners
from corrupted data. In this work, we formalize the problem of Learner-Agnostic
Robust data Prefiltering (LARP), which aims at finding prefiltering procedures
that minimize a worst-case loss over a pre-specified set of learners. We first
instantiate our framework in the context of scalar mean estimation with Huber
estimators under the Huber data contamination model. We provide a hardness
result on a specific problem instance and analyze several natural prefiltering
procedures. Our theoretical results indicate that performing LARP on a
heterogeneous set of learners leads to some loss in model performance compared
to the alternative of prefiltering data for each learner/use-case individually.
We explore the resulting utility loss and its dependence on the problem
parameters via extensive experiments on real-world image and tabular data,
observing statistically significant reduction in utility. Finally, we model the
trade-off between the utility drop and the cost of repeated (learner-specific)
prefiltering within a game-theoretic framework and showcase benefits of LARP
for large datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICML 2025 Workshop on DataWorld: Unifying Data Curation
  Frameworks Across Domains</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Derivation of Output Correlation Inferences for Multi-Output (aka
  Multi-Task) Gaussian Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07964v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07964v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhei Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian process (GP) is arguably one of the most widely used machine
learning algorithms in practice. One of its prominent applications is Bayesian
optimization (BO). Although the vanilla GP itself is already a powerful tool
for BO, it is often beneficial to be able to consider the dependencies of
multiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not
trivial to fully understand the derivations of its formulations and their
gradients from the previous literature. This paper serves friendly derivations
of the MTGP formulations and their gradients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proofs for Folklore Theorems on the Radon-Nikodym Derivative 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaiza Bermudez, Gaetan Bisson, Iñaki Esnaola, Samir M. Perlaza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, rigorous statements and formal proofs are presented
for both foundational and advanced folklore theorems on the Radon-Nikodym
derivative. The cases of conditional and marginal probability measures are
carefully considered, which leads to an identity involving the sum of mutual
and lautum information suggesting a new interpretation for such a sum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Determinant Estimation under Memory Constraints and Neural Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siavash Ameli, Chris van der Heide, Liam Hodgkinson, Fred Roosta, Michael W. Mahoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calculating or accurately estimating log-determinants of large positive
definite matrices is of fundamental importance in many machine learning tasks.
While its cubic computational complexity can already be prohibitive, in modern
applications, even storing the matrices themselves can pose a memory
bottleneck. To address this, we derive a novel hierarchical algorithm based on
block-wise computation of the LDL decomposition for large-scale log-determinant
calculation in memory-constrained settings. In extreme cases where matrices are
highly ill-conditioned, accurately computing the full matrix itself may be
infeasible. This is particularly relevant when considering kernel matrices at
scale, including the empirical Neural Tangent Kernel (NTK) of neural networks
trained on large datasets. Under the assumption of neural scaling laws in the
test error, we show that the ratio of pseudo-determinants satisfies a power-law
relationship, allowing us to derive corresponding scaling laws. This enables
accurate estimation of NTK log-determinants from a tiny fraction of the full
dataset; in our experiments, this results in a $\sim$100,000$\times$ speedup
with improved accuracy over competing approximations. Using these techniques,
we successfully estimate log-determinants for dense matrices of extreme sizes,
which were previously deemed intractable and inaccessible due to their enormous
scale and computational demands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural Classification of Locally Stationary Time Series Based on
  Second-order Characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Qian, Xiucai Ding, Lexin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series classification is crucial for numerous scientific and engineering
applications. In this article, we present a numerically efficient, practically
competitive, and theoretically rigorous classification method for
distinguishing between two classes of locally stationary time series based on
their time-domain, second-order characteristics. Our approach builds on the
autoregressive approximation for locally stationary time series, combined with
an ensemble aggregation and a distance-based threshold for classification. It
imposes no requirement on the training sample size, and is shown to achieve
zero misclassification error rate asymptotically when the underlying time
series differ only mildly in their second-order characteristics. The new method
is demonstrated to outperform a variety of state-of-the-art solutions,
including wavelet-based, tree-based, convolution-based methods, as well as
modern deep learning methods, through intensive numerical simulations and a
real EEG data analysis for epilepsy classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 Pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ It's Hard to Be Normal: The Impact of Noise on Structure-agnostic
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jikai Jin, Lester Mackey, Vasilis Syrgkanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure-agnostic causal inference studies how well one can estimate a
treatment effect given black-box machine learning estimates of nuisance
functions (like the impact of confounders on treatment and outcomes). Here, we
find that the answer depends in a surprising way on the distribution of the
treatment noise. Focusing on the partially linear model of
\citet{robinson1988root}, we first show that the widely adopted double machine
learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,
resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for
independent non-Gaussian treatment noise, we show that DML is always suboptimal
by constructing new practical procedures with higher-order robustness to
nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant
estimators to achieve $r$-th order insensitivity to nuisance errors whenever
the $(r+1)$-st treatment cumulant is non-zero. We complement these core results
with novel minimax guarantees for binary treatments in the partially linear
model. Finally, using synthetic demand estimation experiments, we demonstrate
the practical benefits of our higher-order robust estimators.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">80</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of <span class="highlight-title">Pretrain</span>ing Word Co-occurrence on Compositional Generalization
  in Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helen Qu, Sang Michael Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP and large multimodal models (LMMs) have better accuracy on examples
involving concepts that are highly represented in the training data. However,
the role of concept combinations in the training data on compositional
generalization is largely unclear -- for instance, how does accuracy vary when
a common object appears in an uncommon pairing with another object? In this
paper, we investigate how word co-occurrence statistics in the pretraining
dataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM
performance. To disentangle the effects of word co-occurrence frequencies from
single-word frequencies, we measure co-occurrence with pointwise mutual
information (PMI), which normalizes the joint probability of two words
co-occurring by the probability of co-occurring independently. Using
synthetically generated images with a variety of concept pairs, we show a
strong correlation between PMI in the CLIP pretraining data and zero-shot
accuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap
between images in the top and bottom 5% of PMI values), demonstrating that even
accuracy on common concepts is affected by the combination of concepts in the
image. Leveraging this finding, we reproduce this effect in natural images by
editing them to contain pairs with varying PMI, resulting in a correlation of
r=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs
built on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings
highlight the need for algorithms and architectures that improve compositional
generalization in multimodal models without scaling the training data
combinatorially. Our code is available at
https://github.com/helenqu/multimodal-pretraining-pmi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and
  Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically
referencing visual regions, just like human "thinking with images". However, no
benchmark exists to evaluate these capabilities holistically. To bridge this
gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a
diagnostic benchmark built on three principles: (1) focused visual perception
of subtle targets in complex scenes, (2) traceable evidence via bounding box
evaluation, and (3) second-order reasoning to test object interactions and
spatial hierarchies beyond simple object localization. Prioritizing images with
dense objects, we initially sample 1K high-quality images from SA-1B, and
incorporate eight LMM experts to manually annotate questions, candidate
options, and answers for each image. After three stages of quality control,
TreeBench consists of 405 challenging visual question-answering pairs, even the
most advanced models struggle with this benchmark, where none of them reach 60%
accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR
(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to
supervise localization and reasoning jointly with reinforcement learning,
enabling accurate localizations and explainable reasoning pathways. Initialized
from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and
TreeBench (+13.4), proving traceability is key to advancing vision-grounded
reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PyVision: Agentic Vision with Dynamic Tooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly deployed as agents, systems capable of planning,
reasoning, and dynamically calling external tools. However, in visual
reasoning, prior approaches largely remain limited by predefined workflows and
static toolsets. In this report, we present PyVision, an interactive,
multi-turn framework that enables MLLMs to autonomously generate, execute, and
refine Python-based tools tailored to the task at hand, unlocking flexible and
interpretable problem-solving. We develop a taxonomy of the tools created by
PyVision and analyze their usage across a diverse set of benchmarks.
Quantitatively, PyVision achieves consistent performance gains, boosting
GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.
These results point to a broader shift: dynamic tooling allows models not just
to use tools, but to invent them, advancing toward more agentic visual
reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 Pages, 10 Figures, Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkai Jia, Wei Yin, Xiaotao Hu, Jiaxin Guo, Xiaoyang Guo, Qian Zhang, Xiao-Xiao Long, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models
that compress continuous visual data into discrete tokens. Existing methods
have tried to improve the quantization strategy for better reconstruction
quality, however, there still exists a large gap between VQ-VAEs and VAEs. To
narrow this gap, we propose \NickName, a novel method to augment the
representation capability of discrete codebooks, facilitating easier
optimization for codebooks and minimizing information loss, thereby enhancing
reconstruction quality. Specifically, we propose to retain the latent dimension
to preserve encoded features and incorporate a set of sub-codebooks for
quantization. Furthermore, we construct comprehensive zero-shot benchmarks
featuring resolutions of 512p and 2k to evaluate the reconstruction performance
of existing methods rigorously. \NickName~achieves the \textbf{state-of-the-art
performance on both ImageNet and $8$ zero-shot benchmarks} across all VQ-VAEs.
Notably, compared with SD-VAE, we outperform them on ImageNet significantly,
with rFID $\textbf{0.49}$ v.s. $\textbf{0.91}$, and achieve superior PSNR on
all zero-shot benchmarks. These results highlight the superiority of
\NickName~in reconstruction and pave the way for preserving fidelity in HD
image processing tasks. Code will be publicly available at
https://github.com/MKJia/MGVQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-pass Adaptive Image Tokenization for Minimum Program Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Duggal, Sanghyun Byun, William T. Freeman, Antonio Torralba, Phillip Isola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to Algorithmic Information Theory (AIT) -- Intelligent
representations compress data into the shortest possible program that can
reconstruct its content, exhibiting low Kolmogorov Complexity (KC). In
contrast, most visual representation learning systems use fixed-length
representations for all inputs, ignoring variations in complexity or
familiarity. Recent adaptive tokenization methods address this by allocating
variable-length representations but typically require test-time search over
multiple encodings to find the most predictive one. Inspired by Kolmogorov
Complexity principles, we propose a single-pass adaptive tokenizer, KARL, which
predicts the appropriate number of tokens for an image in a single forward
pass, halting once its approximate KC is reached. The token count serves as a
proxy for the minimum description length. KARL's training procedure closely
resembles the Upside-Down Reinforcement Learning paradigm, as it learns to
conditionally predict token halting based on a desired reconstruction quality.
KARL matches the performance of recent adaptive tokenizers while operating in a
single pass. We present scaling laws for KARL, analyzing the role of
encoder/decoder size, continuous vs. discrete tokenization and more.
Additionally, we offer a conceptual study drawing an analogy between Adaptive
Image Tokenization and Algorithmic Information Theory, examining the predicted
image complexity (KC) across axes such as structure vs. noise and in- vs.
out-of-distribution familiarity -- revealing alignment with human intuition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: https://github.com/ShivamDuggal4/karl Keywords:
  Representation Learning, Adaptive Tokenization, Compression, Algorithmic
  Information Theory, Kolmogorov Complexity, Upside-Down RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Maity, Ayan Kumar Bhunia, Subhadeep Koley, Pinaki Nath Chowdhury, Aneeshan Sain, Yi-Zhe Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keypoint detection, integral to modern machine perception, faces challenges
in few-shot learning, particularly when source data from the same distribution
as the query is unavailable. This gap is addressed by leveraging sketches, a
popular form of human expression, providing a source-free alternative. However,
challenges arise in mastering cross-modal embeddings and handling user-specific
sketch styles. Our proposed framework overcomes these hurdles with a
prototypical setup, combined with a grid-based locator and prototypical domain
adaptation. We also demonstrate success in few-shot convergence across novel
keypoints and classes through extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025. Project Page: https://subhajitmaity.me/DYKp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multigranular Evaluation for Brain Visual Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Xia, Cengiz Oztireli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing evaluation protocols for brain visual decoding predominantly rely on
coarse metrics that obscure inter-model differences, lack neuroscientific
foundation, and fail to capture fine-grained visual distinctions. To address
these limitations, we introduce BASIC, a unified, multigranular evaluation
framework that jointly quantifies structural fidelity, inferential alignment,
and contextual coherence between decoded and ground truth images. For the
structural level, we introduce a hierarchical suite of segmentation-based
metrics, including foreground, semantic, instance, and component masks,
anchored in granularity-aware correspondence across mask structures. For the
semantic level, we extract structured scene representations encompassing
objects, attributes, and relationships using multimodal large language models,
enabling detailed, scalable, and context-rich comparisons with ground-truth
stimuli. We benchmark a diverse set of visual decoding methods across multiple
stimulus-neuroimaging datasets within this unified evaluation framework.
Together, these criteria provide a more discriminative, interpretable, and
comprehensive foundation for measuring brain visual decoding methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://weihaox.github.io/BASIC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Granular Spatio-Temporal Token Merging for Training-Free
  Acceleration of Video LLMs <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video large language models (LLMs) achieve strong video understanding by
leveraging a large number of spatio-temporal tokens, but suffer from quadratic
computational scaling with token count. To address this, we propose a
training-free spatio-temporal token merging method, named STTM. Our key insight
is to exploit local spatial and temporal redundancy in video data which has
been overlooked in prior work. STTM first transforms each frame into
multi-granular spatial tokens using a coarse-to-fine search over a quadtree
structure, then performs directed pairwise merging across the temporal
dimension. This decomposed merging approach outperforms existing token
reduction methods across six video QA benchmarks. Notably, STTM achieves a
2$\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and
a 3$\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is
query-agnostic, allowing KV cache reuse across different questions for the same
video. The project page is available at https://www.jshyun.me/projects/sttm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV2025; Project page:
  https://www.jshyun.me/projects/sttm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is
  Why 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijay Gurung, David T. Hoffmann, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language models like CLIP are used for a large variety of
applications, such as zero-shot classification or as vision encoder for
multi-modal models. Despite their popularity, their representations show major
limitations. For instance, CLIP models learn bag-of-words representations and,
as a consequence, fail to distinguish whether an image is of "a yellow
submarine and a blue bus" or "a blue submarine and a yellow bus". Previous
attempts to fix this issue added hard negatives during training or modified the
architecture, but failed to resolve the problem in its entirety. We suspect
that the missing insights to solve the binding problem for CLIP are hidden in
the arguably most important part of learning algorithms: the data. In this
work, we fill this gap by rigorously identifying the influence of data
properties on CLIP's ability to learn binding using a synthetic dataset. We
find that common properties of natural data such as low attribute density,
incomplete captions, and the saliency bias, a tendency of human captioners to
describe the object that is "most salient" to them have a detrimental effect on
binding performance. In contrast to common belief, we find that neither scaling
the batch size, i.e., implicitly adding more hard negatives, nor explicitly
creating hard negatives enables CLIP to learn reliable binding. Only when the
data expresses our identified data properties CLIP learns almost perfect
binding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OST-Bench: Evaluating the Capabilities of MLLMs in Online
  Spatio-temporal Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JingLi Lin, Chenming Zhu, Runsen Xu, Xiaohan Mao, Xihui Liu, Tai Wang, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in multimodal large language models (MLLMs) have shown
remarkable capabilities in integrating vision and language for complex
reasoning. While most existing benchmarks evaluate models under offline
settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a
benchmark designed to evaluate Online Spatio-Temporal understanding from the
perspective of an agent actively exploring a scene. The Online aspect
emphasizes the need to process and reason over incrementally acquired
observations, while the Spatio-Temporal component requires integrating current
visual inputs with historical memory to support dynamic spatial reasoning.
OST-Bench better reflects the challenges of real-world embodied perception.
Built on an efficient data collection pipeline, OST-Bench consists of 1.4k
scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and
ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that
they fall short on tasks requiring complex spatio-temporal reasoning. Under the
online setting, their accuracy declines as the exploration horizon extends and
the memory grows. Through further experimental analysis, we identify common
error patterns across models and find that both complex clue-based spatial
reasoning demands and long-term memory retrieval requirements significantly
drop model performance along two separate axes, highlighting the core
challenges that must be addressed to improve online embodied reasoning. To
foster further research and development in the field, our codes, dataset, and
benchmark are available. Our project page is:
https://rbler1234.github.io/OSTBench.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, a benchmark designed to evaluate Online Spatio-Temporal
  understanding from the perspective of an agent actively exploring a scene.
  Project Page: https://rbler1234.github.io/OSTBench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry Forcing: Marrying Video <span class="highlight-title">Diffusion</span> and 3D Representation for
  Consistent Wo<span class="highlight-title">rl</span>d Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos inherently represent 2D projections of a dynamic 3D world. However,
our analysis suggests that video diffusion models trained solely on raw video
data often fail to capture meaningful geometric-aware structure in their
learned representations. To bridge this gap between video diffusion models and
the underlying 3D nature of the physical world, we propose Geometry Forcing, a
simple yet effective method that encourages video diffusion models to
internalize latent 3D representations. Our key insight is to guide the model's
intermediate representations toward geometry-aware structure by aligning them
with features from a pretrained geometric foundation model. To this end, we
introduce two complementary alignment objectives: Angular Alignment, which
enforces directional consistency via cosine similarity, and Scale Alignment,
which preserves scale-related information by regressing unnormalized geometric
features from normalized diffusion representation. We evaluate Geometry Forcing
on both camera view-conditioned and action-conditioned video generation tasks.
Experimental results demonstrate that our method substantially improves visual
quality and 3D consistency over the baseline methods. Project page:
https://GeometryForcing.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, project page: https://GeometryForcing.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Martian Wo<span class="highlight-title">rl</span>d Models: Controllable Video Synthesis with Physically
  Accurate 3D Reconstructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longfei Li, Zhiwen Fan, Wenyan Cong, Xinhang Liu, Yuyang Yin, Matt Foutter, Panwang Pan, Chenyu You, Yue Wang, Zhangyang Wang, Yao Zhao, Marco Pavone, Yunchao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing realistic Martian landscape videos is crucial for mission
rehearsal and robotic simulation. However, this task poses unique challenges
due to the scarcity of high-quality Martian data and the significant domain gap
between Martian and terrestrial imagery. To address these challenges, we
propose a holistic solution composed of two key components: 1) A data curation
pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian
environments from real stereo navigation images, sourced from NASA's Planetary
Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A
Martian terrain video generator, MarsGen, which synthesizes novel videos
visually realistic and geometrically consistent with the 3D structure encoded
in the data. Our M3arsSynth engine spans a wide range of Martian terrains and
acquisition dates, enabling the generation of physically accurate 3D surface
models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,
synthesizes videos conditioned on an initial image frame and, optionally,
camera trajectories or textual prompts, allowing for video generation in novel
environments. Experimental results show that our approach outperforms video
synthesis models trained on terrestrial datasets, achieving superior visual
fidelity and 3D structural consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://marsgenai.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling <span class="highlight-title">RL</span> to Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a full-stack framework that scales up reasoning in
vision-language models (VLMs) to long videos, leveraging reinforcement
learning. We address the unique challenges of long video reasoning by
integrating three critical components: (1) a large-scale dataset,
LongVideo-Reason, comprising 52K long video QA pairs with high-quality
reasoning annotations across diverse domains such as sports, games, and vlogs;
(2) a two-stage training pipeline that extends VLMs with chain-of-thought
supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a
training infrastructure for long video RL, named Multi-modal Reinforcement
Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a
vLLM-based engine tailored for long video, using cached video embeddings for
efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves
strong performance on long video QA benchmarks such as VideoMME. It also
outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal
reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on
our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to
2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent
performance gains as the number of input video frames scales. LongVILA-R1 marks
a firm step towards long video reasoning in VLMs. In addition, we release our
training system for public availability that supports RL training on various
modalities (video, text, and audio), various models (VILA and Qwen series), and
even image and video generation models. On a single A100 node (8 GPUs), it
supports RL training on hour-long videos (e.g., 3,600 frames / around 256k
tokens).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at https://github.com/NVlabs/Long-RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Input Conditioned Layer Dropping in Speech Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Hannan, Daniele Falavigna, Alessio Brutti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curating foundation speech models for edge and IoT settings, where
computational resources vary over time, requires dynamic architectures
featuring adaptable reduction strategies. One emerging approach is layer
dropping ($\mathcal{LD}$) which skips fraction of the layers of a backbone
network during inference to reduce the computational load. This allows
transforming static models into dynamic ones. However, existing approaches
exhibit limitations either in the mode of selecting layers or by significantly
modifying the neural architecture. To this end, we propose input-driven
$\mathcal{LD}$ that employs the network's input features and a lightweight
layer selecting network to determine the optimum combination of processing
layers. Extensive experimentation on 4 speech and audio public benchmarks,
using two different pre-trained foundation models, demonstrates the
effectiveness of our approach, thoroughly outperforming random dropping and
producing on-par (or better) results to early exit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE MLSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient
  Human Activity Recognition on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhen Bian, Mengxi Liu, Vitor Fortes Rey, Daniel Geissler, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) on resource-constrained wearable devices
demands inference models that harmonize accuracy with computational efficiency.
This paper introduces TinierHAR, an ultra-lightweight deep learning
architecture that synergizes residual depthwise separable convolutions, gated
recurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency
without compromising performance. Evaluated across 14 public HAR datasets,
TinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.
DeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the
averaged F1-scores. Beyond quantitative gains, this work provides the first
systematic ablation study dissecting the contributions of spatial-temporal
components across proposed TinierHAR, prior SOTA TinyHAR, and the classical
DeepConvLSTM, offering actionable insights for designing efficient HAR systems.
We finally discussed the findings and suggested principled design guidelines
for future efficient HAR. To catalyze edge-HAR research, we open-source all
materials in this work for future
benchmarking\footnote{https://github.com/zhaxidele/TinierHAR}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Continuous Home Cage Monitoring: An Evaluation of Tracking and
  Identification Strategies for Laboratory Mice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Pablo Oberhauser, Daniel Grzenda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous, automated monitoring of laboratory mice enables more accurate
data collection and improves animal welfare through real-time insights.
Researchers can achieve a more dynamic and clinically relevant characterization
of disease progression and therapeutic effects by integrating behavioral and
physiological monitoring in the home cage. However, providing individual mouse
metrics is difficult because of their housing density, similar appearances,
high mobility, and frequent interactions. To address these challenges, we
develop a real-time identification (ID) algorithm that accurately assigns ID
predictions to mice wearing custom ear tags in digital home cages monitored by
cameras. Our pipeline consists of three parts: (1) a custom multiple object
tracker (MouseTracks) that combines appearance and motion cues from mice; (2) a
transformer-based ID classifier (Mouseformer); and (3) a tracklet associator
linear program to assign final ID predictions to tracklets (MouseMap). Our
models assign an animal ID based on custom ear tags at 30 frames per second
with 24/7 cage coverage. We show that our custom tracking and ID pipeline
improves tracking efficiency and lowers ID switches across mouse strains and
various environmental factors compared to current mouse tracking methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArteryX: Advancing Brain Artery Feature Extraction with Vessel-Fused
  Networks and a Robust Validation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abrar Faiyaz, Nhat Hoang, Giovanni Schifitto, Md Nasir Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cerebrovascular pathology significantly contributes to cognitive decline and
neurological disorders, underscoring the need for advanced tools to assess
vascular integrity. Three-dimensional Time-of-Flight Magnetic Resonance
Angiography (3D TOF MRA) is widely used to visualize cerebral vasculature,
however, clinical evaluations generally focus on major arterial abnormalities,
overlooking quantitative metrics critical for understanding subtle vascular
changes. Existing methods for extracting structural, geometrical and
morphological arterial features from MRA - whether manual or automated - face
challenges including user-dependent variability, steep learning curves, and
lack of standardized quantitative validations. We propose a novel
semi-supervised artery evaluation framework, named ArteryX, a MATLAB-based
toolbox that quantifies vascular features with high accuracy and efficiency,
achieving processing times ~10-15 minutes per subject at 0.5 mm resolution with
minimal user intervention. ArteryX employs a vessel-fused network based
landmarking approach to reliably track and manage tracings, effectively
addressing the issue of dangling/disconnected vessels. Validation on human
subjects with cerebral small vessel disease demonstrated its improved
sensitivity to subtle vascular changes and better performance than an existing
semi-automated method. Importantly, the ArteryX toolbox enables quantitative
feature validation by integrating an in-vivo like artery simulation framework
utilizing vessel-fused graph nodes and predefined ground-truth features for
specific artery types. Thus, the ArteryX framework holds promise for
benchmarking feature extraction toolboxes and for seamless integration into
clinical workflows, enabling early detection of cerebrovascular pathology and
standardized comparisons across patient cohorts to advance understanding of
vascular contributions to brain health.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages, 8 Figures, Preliminary version of the toolbox was presented
  at the ISMRM 2025 Conference in Hawaii at the "Software Tools" Session</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal
  Inconsistency for Remote Physiological Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yang, Yuxuan Fan, Can Liu, Houcheng Su, Weichen Guo, Jiyao Wang, Dengbo He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote photoplethysmography (rPPG) has emerged as a promising non-invasive
method for monitoring physiological signals using the camera. Although various
domain adaptation and generalization methods were proposed to promote the
adaptability of deep-based rPPG models in unseen deployment environments,
considerations in aspects like privacy concerns and real-time adaptation
restrict their application in real-world deployment. Thus, we aim to propose a
novel fully Test-Time Adaptation (TTA) strategy tailored for rPPG tasks in this
work. Specifically, based on prior knowledge in physiology and our
observations, we noticed not only there is spatio-temporal consistency in the
frequency domain of rPPG signals, but also that inconsistency in the time
domain was significant. Given this, by leveraging both consistency and
inconsistency priors, we introduce an innovative expert knowledge-based
self-supervised
\textbf{C}onsistency-\textbf{i}n\textbf{C}onsistency-\textbf{i}ntegration
(\textbf{CiCi}) framework to enhances model adaptation during inference.
Besides, our approach further incorporates a gradient dynamic control mechanism
to mitigate potential conflicts between priors, ensuring stable adaptation
across instances. Through extensive experiments on five diverse datasets under
the TTA protocol, our method consistently outperforms existing techniques,
presenting state-of-the-art performance in real-time self-supervised adaptation
without accessing source data. The code will be released later.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware-Aware Feature Extraction Quantisation for Real-Time Visual
  Odometry on FPGA Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Wasala, Mateusz Smolarczyk, Michal Danilowicz, Tomasz Kryjak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate position estimation is essential for modern navigation systems
deployed in autonomous platforms, including ground vehicles, marine vessels,
and aerial drones. In this context, Visual Simultaneous Localisation and
Mapping (VSLAM) - which includes Visual Odometry - relies heavily on the
reliable extraction of salient feature points from the visual input data. In
this work, we propose an embedded implementation of an unsupervised
architecture capable of detecting and describing feature points. It is based on
a quantised SuperPoint convolutional neural network. Our objective is to
minimise the computational demands of the model while preserving high detection
quality, thus facilitating efficient deployment on platforms with limited
resources, such as mobile or embedded systems. We implemented the solution on
an FPGA System-on-Chip (SoC) platform, specifically the AMD/Xilinx Zynq
UltraScale+, where we evaluated the performance of Deep Learning Processing
Units (DPUs) and we also used the Brevitas library and the FINN framework to
perform model quantisation and hardware-aware optimisation. This allowed us to
process 640 x 480 pixel images at up to 54 fps on an FPGA platform,
outperforming state-of-the-art solutions in the field. We conducted experiments
on the TUM dataset to demonstrate and discuss the impact of different
quantisation techniques on the accuracy and performance of the model in a
visual odometry task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the DSD 2025 conference in Salerno, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRA: A Novel Framework for Fusing Modalities in Medical RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhong Wang, Tajamul Ashraf, Zongyan Han, Jorma Laaksonen, Rao Mohammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have significantly advanced
AI-assisted medical diagnosis, but they often generate factually inconsistent
responses that deviate from established medical knowledge. Retrieval-Augmented
Generation (RAG) enhances factual accuracy by integrating external sources, but
it presents two key challenges. First, insufficient retrieval can miss critical
information, whereas excessive retrieval can introduce irrelevant or misleading
content, disrupting model output. Second, even when the model initially
provides correct answers, over-reliance on retrieved data can lead to factual
errors. To address these issues, we introduce the Multimodal Intelligent
Retrieval and Augmentation (MIRA) framework, designed to optimize factual
accuracy in MLLM. MIRA consists of two key components: (1) a calibrated
Rethinking and Rearrangement module that dynamically adjusts the number of
retrieved contexts to manage factual risk, and (2) A medical RAG framework
integrating image embeddings and a medical knowledge base with a query-rewrite
module for efficient multimodal reasoning. This enables the model to
effectively integrate both its inherent knowledge and external references. Our
evaluation of publicly available medical VQA and report generation benchmarks
demonstrates that MIRA substantially enhances factual accuracy and overall
performance, achieving new state-of-the-art results. Code is released at
https://github.com/mbzuai-oryx/MIRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Step Latent <span class="highlight-title">Diffusion</span> for Underwater Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Wu, Tianfu Wang, Md Abu Bakr Siddique, Md Jahidul Islam, Cornelia Fermuller, Yiannis Aloimonos, Christopher A. Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater image restoration algorithms seek to restore the color, contrast,
and appearance of a scene that is imaged underwater. They are a critical tool
in applications ranging from marine ecology and aquaculture to underwater
construction and archaeology. While existing pixel-domain diffusion-based image
restoration approaches are effective at restoring simple scenes with limited
depth variation, they are computationally intensive and often generate
unrealistic artifacts when applied to scenes with complex geometry and
significant depth variation. In this work we overcome these limitations by
combining a novel network architecture (SLURPP) with an accurate synthetic data
generation pipeline. SLURPP combines pretrained latent diffusion models --
which encode strong priors on the geometry and depth of scenes -- with an
explicit scene decomposition -- which allows one to model and account for the
effects of light attenuation and backscattering. To train SLURPP we design a
physics-based underwater image synthesis pipeline that applies varied and
realistic underwater degradation effects to existing terrestrial image
datasets. This approach enables the generation of diverse training data with
dense medium/degradation annotations. We evaluate our method extensively on
both synthetic and real-world benchmarks and demonstrate state-of-the-art
performance. Notably, SLURPP is over 200X faster than existing diffusion-based
methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It
also offers compelling qualitative improvements on real-world data. Project
website https://tianfwang.github.io/slurpp/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ THUNDER: Tile-level Histopathology image UNDERstanding benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Marza, Leo Fillioux, Sofiène Boutaj, Kunal Mahatha, Christian Desrosiers, Pablo Piantanida, Jose Dolz, Stergios Christodoulidis, Maria Vakalopoulou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in a research field can be hard to assess, in particular when many
concurrent methods are proposed in a short period of time. This is the case in
digital pathology, where many foundation models have been released recently to
serve as feature extractors for tile-level images, being used in a variety of
downstream tasks, both for tile- and slide-level problems. Benchmarking
available methods then becomes paramount to get a clearer view of the research
landscape. In particular, in critical domains such as healthcare, a benchmark
should not only focus on evaluating downstream performance, but also provide
insights about the main differences between methods, and importantly, further
consider uncertainty and robustness to ensure a reliable usage of proposed
models. For these reasons, we introduce THUNDER, a tile-level benchmark for
digital pathology foundation models, allowing for efficient comparison of many
models on diverse datasets with a series of downstream tasks, studying their
feature spaces and assessing the robustness and uncertainty of predictions
informed by their embeddings. THUNDER is a fast, easy-to-use, dynamic benchmark
that can already support a large variety of state-of-the-art foundation, as
well as local user-defined models for direct tile-based comparison. In this
paper, we provide a comprehensive comparison of 23 foundation models on 16
different datasets covering diverse tasks, feature analysis, and robustness.
The code for THUNDER is publicly available at
https://github.com/MICS-Lab/thunder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeD-3D: A Multimodal Deep Learning Framework for Precise Recurrence
  Prediction in Clear Cell Renal Cell Carcinoma (ccRCC) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasaan Maqsood, Saif Ur Rehman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of recurrence in clear cell renal cell carcinoma (ccRCC)
remains a major clinical challenge due to the disease complex molecular,
pathological, and clinical heterogeneity. Traditional prognostic models, which
rely on single data modalities such as radiology, histopathology, or genomics,
often fail to capture the full spectrum of disease complexity, resulting in
suboptimal predictive accuracy. This study aims to overcome these limitations
by proposing a deep learning (DL) framework that integrates multimodal data,
including CT, MRI, histopathology whole slide images (WSI), clinical data, and
genomic profiles, to improve the prediction of ccRCC recurrence and enhance
clinical decision-making. The proposed framework utilizes a comprehensive
dataset curated from multiple publicly available sources, including TCGA, TCIA,
and CPTAC. To process the diverse modalities, domain-specific models are
employed: CLAM, a ResNet50-based model, is used for histopathology WSIs, while
MeD-3D, a pre-trained 3D-ResNet18 model, processes CT and MRI images. For
structured clinical and genomic data, a multi-layer perceptron (MLP) is used.
These models are designed to extract deep feature embeddings from each
modality, which are then fused through an early and late integration
architecture. This fusion strategy enables the model to combine complementary
information from multiple sources. Additionally, the framework is designed to
handle incomplete data, a common challenge in clinical settings, by enabling
inference even when certain modalities are missing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-ADAM: A <span class="highlight-title">Dataset</span> for 3D Anomaly Detection in Advanced Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul McHard, Florent P. Audonnet, Oliver Summerell, Sebastian Andraos, Paul Henderson, Gerardo Aragon-Camarasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface defects are one of the largest contributors to low yield in the
manufacturing sector. Accurate and reliable detection of defects during the
manufacturing process is therefore of great value across the sector.
State-of-the-art approaches to automated defect detection yield impressive
performance on current datasets, yet still fall short in real-world
manufacturing settings and developing improved methods relies on large datasets
representative of real-world scenarios. Unfortunately, high-quality,
high-precision RGB+3D industrial anomaly detection datasets are scarce, and
typically do not reflect real-world industrial deployment scenarios. To address
this, we introduce 3D-ADAM, the first large-scale industry-relevant dataset for
high-precision 3D Anomaly Detection. 3D-ADAM comprises 14,120 high-resolution
scans across 217 unique parts, captured using 4 industrial depth imaging
sensors. It includes 27,346 annotated defect instances from 12 categories,
covering the breadth of industrial surface defects. 3D-ADAM uniquely captures
an additional 8,110 annotations of machine element features, spanning the range
of relevant mechanical design form factors. Unlike existing datasets, 3D-ADAM
is captured in a real industrial environment with variations in part position
and orientation, camera positioning, ambient lighting conditions, as well as
partial occlusions. Our evaluation of SOTA models across various RGB+3D anomaly
detection tasks demonstrates the significant challenge this dataset presents to
current approaches. We further validated the industrial relevance and quality
of the dataset through an expert labelling survey conducted by industry
partners. By providing this challenging benchmark, 3D-ADAM aims to accelerate
the development of robust 3D Anomaly Detection models capable of meeting the
demands of modern manufacturing environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Query-based <span class="highlight-title">Transformer</span> for Continual Image Segmentation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Zhu, Cheng Shi, Dingyou Wang, Jiajin Tang, Zhengxuan Wei, Yu Wu, Guanbin Li, Sibei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental/Continual image segmentation (CIS) aims to train an image
segmenter in stages, where the set of available categories differs at each
stage. To leverage the built-in objectness of query-based transformers, which
mitigates catastrophic forgetting of mask proposals, current methods often
decouple mask generation from the continual learning process. This study,
however, identifies two key issues with decoupled frameworks: loss of
plasticity and heavy reliance on input data order. To address these, we conduct
an in-depth investigation of the built-in objectness and find that highly
aggregated image features provide a shortcut for queries to generate masks
through simple feature alignment. Based on this, we propose SimCIS, a simple
yet powerful baseline for CIS. Its core idea is to directly select image
features for query assignment, ensuring "perfect alignment" to preserve
objectness, while simultaneously allowing queries to select new classes to
promote plasticity. To further combat catastrophic forgetting of categories, we
introduce cross-stage consistency in selection and an innovative "visual
query"-based replay mechanism. Experiments demonstrate that SimCIS consistently
outperforms state-of-the-art methods across various segmentation tasks,
settings, splits, and input data orders. All models and codes will be made
publicly available at https://github.com/SooLab/SimCIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Dirauf, Florian Wolz, Dario Zanca, Björn Eskofier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content-based puzzle solvers have been extensively studied, demonstrating
significant progress in computational techniques. However, their evaluation
often lacks realistic challenges crucial for real-world applications, such as
the reassembly of fragmented artefacts or shredded documents. In this work, we
investigate the robustness of State-Of-The-Art content-based puzzle solvers
introducing three types of jigsaw puzzle corruptions: missing pieces, eroded
edges, and eroded contents. Evaluating both heuristic and deep learning-based
solvers, we analyse their ability to handle these corruptions and identify key
limitations. Our results show that solvers developed for standard puzzles have
a rapid decline in performance if more pieces are corrupted. However, deep
learning models can significantly improve their robustness through fine-tuning
with augmented data. Notably, the advanced Positional Diffusion model adapts
particularly well, outperforming its competitors in most experiments. Based on
our findings, we highlight promising research directions for enhancing the
automated reconstruction of real-world artefacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICIAP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patient-specific vs Multi-Patient Vision <span class="highlight-title">Transformer</span> for Marke<span class="highlight-title">rl</span>ess
  Tumor Motion Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauthier Rotsart de Hertaing, Dani Manjah, Benoit Macq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Accurate forecasting of lung tumor motion is essential for
precise dose delivery in proton therapy. While current markerless methods
mostly rely on deep learning, transformer-based architectures remain unexplored
in this domain, despite their proven performance in trajectory forecasting.
  Purpose: This work introduces a markerless forecasting approach for lung
tumor motion using Vision Transformers (ViT). Two training strategies are
evaluated under clinically realistic constraints: a patient-specific (PS)
approach that learns individualized motion patterns, and a multi-patient (MP)
model designed for generalization. The comparison explicitly accounts for the
limited number of images that can be generated between planning and treatment
sessions.
  Methods: Digitally reconstructed radiographs (DRRs) derived from planning
4DCT scans of 31 patients were used to train the MP model; a 32nd patient was
held out for evaluation. PS models were trained using only the target patient's
planning data. Both models used 16 DRRs per input and predicted tumor motion
over a 1-second horizon. Performance was assessed using Average Displacement
Error (ADE) and Final Displacement Error (FDE), on both planning (T1) and
treatment (T2) data.
  Results: On T1 data, PS models outperformed MP models across all training set
sizes, especially with larger datasets (up to 25,000 DRRs, p < 0.05). However,
MP models demonstrated stronger robustness to inter-fractional anatomical
variability and achieved comparable performance on T2 data without retraining.
  Conclusions: This is the first study to apply ViT architectures to markerless
tumor motion forecasting. While PS models achieve higher precision, MP models
offer robust out-of-the-box performance, well-suited for time-constrained
clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergistic <span class="highlight-title">Prompt</span>ing for Robust Visual Recognition with Missing
  Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihui Zhang, Luanyuan Dai, Qika Lin, Yunfeng Diao, Guangyin Jin, Yufei Guo, Jing Zhang, Xiaoshuai Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale multi-modal models have demonstrated remarkable performance
across various visual recognition tasks by leveraging extensive paired
multi-modal training data. However, in real-world applications, the presence of
missing or incomplete modality inputs often leads to significant performance
degradation. Recent research has focused on prompt-based strategies to tackle
this issue; however, existing methods are hindered by two major limitations:
(1) static prompts lack the flexibility to adapt to varying missing-data
conditions, and (2) basic prompt-tuning methods struggle to ensure reliable
performance when critical modalities are missing.To address these challenges,
we propose a novel Synergistic Prompting (SyP) framework for robust visual
recognition with missing modalities. The proposed SyP introduces two key
innovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to
dynamically generate prompts, replacing static parameters for flexible
multi-modal adaptation, and (II) a Synergistic Prompting Strategy, which
combines static and dynamic prompts to balance information across modalities,
ensuring robust reasoning even when key modalities are missing. The proposed
SyP achieves significant performance improvements over existing approaches
across three widely-used visual recognition datasets, demonstrating robustness
under diverse missing rates and conditions. Extensive experiments and ablation
studies validate its effectiveness in handling missing modalities, highlighting
its superior adaptability and reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Attention Residual U-Net for curvilinear structure segmentation
  in fluorescence microscopy and biomedical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achraf Ait Laydi, Louis Cueff, Mewen Crespo, Yousef El Mourabit, Hélène Bouvrais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting curvilinear structures in fluorescence microscopy remains a
challenging task, particularly under noisy conditions and in dense filament
networks commonly seen in vivo. To address this, we created two original
datasets consisting of hundreds of synthetic images of fluorescently labelled
microtubules within cells. These datasets are precisely annotated and closely
mimic real microscopy images, including realistic noise. The second dataset
presents an additional challenge, by simulating varying fluorescence
intensities along filaments that complicate segmentation. While deep learning
has shown strong potential in biomedical image analysis, its performance often
declines in noisy or low-contrast conditions. To overcome this limitation, we
developed a novel advanced architecture: the Adaptive Squeeze-and-Excitation
Residual U-Net (ASE_Res_UNet). This model enhanced the standard U-Net by
integrating residual blocks in the encoder and adaptive SE attention mechanisms
in the decoder. Through ablation studies and comprehensive visual and
quantitative evaluations, ASE_Res_UNet consistently outperformed its variants,
namely standard U-Net, ASE_UNet and Res_UNet architectures. These improvements,
particularly in noise resilience and detecting fine, low-intensity structures,
were largely attributed to the adaptive SE attention module that we created. We
further benchmarked ASE_Res_UNet against various state-of-the-art models, and
found it achieved superior performance on our most challenging dataset.
Finally, the model also generalized well to real microscopy images of stained
microtubules as well as to other curvilinear structures. Indeed, it
successfully segmented retinal blood vessels and nerves in noisy or
low-contrast biomedical images, demonstrating its strong potential for
applications in disease diagnosis and treatment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Instance-aware <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Xiao, Yunbei Zhang, Xingjian Li, Tianyang Wang, Xiao Wang, Yuxiang Wei, Jihun Hamm, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) has emerged as a parameter-efficient fine-tuning
paradigm for vision transformers, with conventional approaches utilizing
dataset-level prompts that remain the same across all input instances. We
observe that this strategy results in sub-optimal performance due to high
variance in downstream datasets. To address this challenge, we propose Visual
Instance-aware Prompt Tuning (ViaPT), which generates instance-aware prompts
based on each individual input and fuses them with dataset-level prompts,
leveraging Principal Component Analysis (PCA) to retain important prompting
information. Moreover, we reveal that VPT-Deep and VPT-Shallow represent two
corner cases based on a conceptual understanding, in which they fail to
effectively capture instance-specific information, while random dimension
reduction on prompts only yields performance between the two extremes. Instead,
ViaPT overcomes these limitations by balancing dataset-level and instance-level
knowledge, while reducing the amount of learnable parameters compared to
VPT-Deep. Extensive experiments across 34 diverse datasets demonstrate that our
method consistently outperforms state-of-the-art baselines, establishing a new
paradigm for analyzing and optimizing visual prompts for vision transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Generalizable Heart Rate Estimation via Deep Learning for
  Remote Photoplethysmography in Complex Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Cen, Chang-Hong Fu, Hong Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-contact remote photoplethysmography (rPPG) technology enables heart rate
measurement from facial videos. However, existing network models still face
challenges in accu racy, robustness, and generalization capability under
complex scenarios. This paper proposes an end-to-end rPPG extraction network
that employs 3D convolutional neural networks to reconstruct accurate rPPG
signals from raw facial videos. We introduce a differential frame fusion module
that integrates differential frames with original frames, enabling frame-level
representations to capture blood volume pulse (BVP) variations. Additionally,
we incorporate Temporal Shift Module (TSM) with self-attention mechanisms,
which effectively enhance rPPG features with minimal computational overhead.
Furthermore, we propose a novel dynamic hybrid loss function that provides
stronger supervision for the network, effectively mitigating over fitting.
Comprehensive experiments were conducted on not only the PURE and UBFC-rPPG
datasets but also the challenging MMPD dataset under complex scenarios,
involving both intra dataset and cross-dataset evaluations, which demonstrate
the superior robustness and generalization capability of our network.
Specifically, after training on PURE, our model achieved a mean absolute error
(MAE) of 7.58 on the MMPD test set, outperforming the state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computationally Efficient Information-Driven Optical Design with
  Interchanging Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Markley, Henry Pinkard, Leyla Kabuli, Nalini Singh, Laura Waller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated that imaging systems can be evaluated through
the information content of their measurements alone, enabling
application-agnostic optical design that avoids computational decoding
challenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed
to automate this process through gradient-based. In this work, we study IDEAL
across diverse imaging systems and find that it suffers from high memory usage,
long runtimes, and a potentially mismatched objective function due to
end-to-end differentiability requirements. We introduce IDEAL with
Interchanging Optimization (IDEAL-IO), a method that decouples density
estimation from optical parameter optimization by alternating between fitting
models to current measurements and updating optical parameters using fixed
models for information estimation. This approach reduces runtime and memory
usage by up to 6x while enabling more expressive density models that guide
optimization toward superior designs. We validate our method on diffractive
optics, lensless imaging, and snapshot 3D microscopy applications, establishing
information-theoretic optimization as a practical, scalable strategy for
real-world imaging system design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SURPRISE3D: A <span class="highlight-title">Dataset</span> for Spatial Understanding and Reasoning in Complex
  3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Huang, Ziwen Li, Hanlve Zhang, Runnan Chen, Xiao He, Yandong Guo, Wenping Wang, Tongliang Liu, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of language and 3D perception is critical for embodied AI and
robotic systems to perceive, understand, and interact with the physical world.
Spatial reasoning, a key capability for understanding spatial relationships
between objects, remains underexplored in current 3D vision-language research.
Existing datasets often mix semantic cues (e.g., object name) with spatial
context, leading models to rely on superficial shortcuts rather than genuinely
interpreting spatial relationships. To address this gap, we introduce
S\textsc{urprise}3D, a novel dataset designed to evaluate language-guided
spatial reasoning segmentation in complex 3D scenes. S\textsc{urprise}3D
consists of more than 200k vision language pairs across 900+ detailed indoor
scenes from ScanNet++ v2, including more than 2.8k unique object classes. The
dataset contains 89k+ human-annotated spatial queries deliberately crafted
without object name, thereby mitigating shortcut biases in spatial
understanding. These queries comprehensively cover various spatial reasoning
skills, such as relative position, narrative perspective, parametric
perspective, and absolute distance reasoning. Initial benchmarks demonstrate
significant challenges for current state-of-the-art expert 3D visual grounding
methods and 3D-LLMs, underscoring the necessity of our dataset and the
accompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite.
S\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially
aware AI, paving the way for effective embodied interaction and robotic
planning. The code and datasets can be found in
https://github.com/liziwennba/SUPRISE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Where are we with calibration under <span class="highlight-title">dataset</span> shift in image
  classification? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Roschewitz, Raghav Mehta, Fabio de Sousa Ribeiro, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conduct an extensive study on the state of calibration under real-world
dataset shift for image classification. Our work provides important insights on
the choice of post-hoc and in-training calibration techniques, and yields
practical guidelines for all practitioners interested in robust calibration
under shift. We compare various post-hoc calibration methods, and their
interactions with common in-training calibration strategies (e.g., label
smoothing), across a wide range of natural shifts, on eight different
classification tasks across several imaging domains. We find that: (i)
simultaneously applying entropy regularisation and label smoothing yield the
best calibrated raw probabilities under dataset shift, (ii) post-hoc
calibrators exposed to a small amount of semantic out-of-distribution data
(unrelated to the task) are most robust under shift, (iii) recent calibration
methods specifically aimed at increasing calibration under shifts do not
necessarily offer significant improvements over simpler post-hoc calibration
methods, (iv) improving calibration under shifts often comes at the cost of
worsening in-distribution calibration. Importantly, these findings hold for
randomly initialised classifiers, as well as for those finetuned from
foundation models, the latter being consistently better calibrated compared to
models trained from scratch. Finally, we conduct an in-depth analysis of
ensembling effects, finding that (i) applying calibration prior to ensembling
(instead of after) is more effective for calibration under shifts, (ii) for
ensembles, OOD exposure deteriorates the ID-shifted calibration trade-off,
(iii) ensembling remains one of the most effective methods to improve
calibration robustness and, combined with finetuning from foundation models,
yields best calibration results overall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/biomedia-mira/calibration_under_shifts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time
  Training <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseong Jeong, Jegyeong Cho, Youngho Yoon, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizing neural networks to unseen target domains is a significant
challenge in real-world deployments. Test-time training (TTT) addresses this by
using an auxiliary self-supervised task to reduce the domain gap caused by
distribution shifts between the source and target. However, we find that when
models are required to perform multiple tasks under domain shifts, conventional
TTT methods suffer from unsynchronized task behavior, where the adaptation
steps needed for optimal performance in one task may not align with the
requirements of other tasks. To address this, we propose a novel TTT approach
called Synchronizing Tasks for Test-time Training (S4T), which enables the
concurrent handling of multiple tasks. The core idea behind S4T is that
predicting task relations across domain shifts is key to synchronizing tasks
during test time. To validate our approach, we apply S4T to conventional
multi-task benchmarks, integrating it with traditional TTT protocols. Our
empirical results show that S4T outperforms state-of-the-art TTT methods across
various benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial
  Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dren Fazlija, Monty-Maximilian Zühlke, Johanna Schrader, Arkadij Orlov, Clara Stein, Iyiola E. Olatunji, Daniel Kudenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unrestricted adversarial attacks aim to fool computer vision models without
being constrained by $\ell_p$-norm bounds to remain imperceptible to humans,
for example, by changing an object's color. This allows attackers to circumvent
traditional, norm-bounded defense strategies such as adversarial training or
certified defense strategies. However, due to their unrestricted nature, there
are also no guarantees of norm-based imperceptibility, necessitating human
evaluations to verify just how authentic these adversarial examples look. While
some related work assesses this vital quality of adversarial attacks, none
provide statistically significant insights. This issue necessitates a unified
framework that supports and streamlines such an assessment for evaluating and
comparing unrestricted attacks. To close this gap, we introduce SCOOTER - an
open-source, statistically powered framework for evaluating unrestricted
adversarial examples. Our contributions are: $(i)$ best-practice guidelines for
crowd-study power, compensation, and Likert equivalence bounds to measure
imperceptibility; $(ii)$ the first large-scale human vs. model comparison
across 346 human participants showing that three color-space attacks and three
diffusion-based attacks fail to produce imperceptible images. Furthermore, we
found that GPT-4o can serve as a preliminary test for imperceptibility, but it
only consistently detects adversarial examples for four out of six tested
attacks; $(iii)$ open-source software tools, including a browser-based task
template to collect annotations and analysis scripts in Python and R; $(iv)$ an
ImageNet-derived benchmark dataset containing 3K real images, 7K adversarial
examples, and over 34K human ratings. Our findings demonstrate that automated
vision systems do not align with human perception, reinforcing the need for a
ground-truth SCOOTER benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 16 figures, 11 tables, Under Review, Code:
  https://github.com/DrenFazlija/Scooter, Data:
  https://doi.org/10.5281/zenodo.15771501</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image
  Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youqian Zhang, Xinyu Ji, Zhihao Wang, Qinhong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image sensors are integral to a wide range of safety- and security-critical
systems, including surveillance infrastructure, autonomous vehicles, and
industrial automation. These systems rely on the integrity of visual data to
make decisions. In this work, we investigate a novel class of electromagnetic
signal injection attacks that target the analog domain of image sensors,
allowing adversaries to manipulate raw visual inputs without triggering
conventional digital integrity checks. We uncover a previously undocumented
attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced
in images captured by image sensors through carefully tuned electromagnetic
interference. We further evaluate the impact of these attacks on
state-of-the-art object detection models, showing that the injected artifacts
propagate through the image signal processing pipeline and lead to significant
mispredictions. Our findings highlight a critical and underexplored
vulnerability in the visual perception stack, highlighting the need for more
robust defenses against physical-layer attacks in such systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIX- Trading Adversarial Fairness via Mixed Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejaswini Medi, Steffen Jung, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Training (AT) is a widely adopted defense against adversarial
examples. However, existing approaches typically apply a uniform training
objective across all classes, overlooking disparities in class-wise
vulnerability. This results in adversarial unfairness: classes with well
distinguishable features (strong classes) tend to become more robust, while
classes with overlapping or shared features(weak classes) remain
disproportionately susceptible to adversarial attacks. We observe that strong
classes do not require strong adversaries during training, as their non-robust
features are quickly suppressed. In contrast, weak classes benefit from
stronger adversaries to effectively reduce their vulnerabilities. Motivated by
this, we introduce TRIX, a feature-aware adversarial training framework that
adaptively assigns weaker targeted adversaries to strong classes, promoting
feature diversity via uniformly sampled targets, and stronger untargeted
adversaries to weak classes, enhancing their focused robustness. TRIX further
incorporates per-class loss weighting and perturbation strength adjustments,
building on prior work, to emphasize weak classes during the optimization.
Comprehensive experiments on standard image classification benchmarks,
including evaluations under strong attacks such as PGD and AutoAttack,
demonstrate that TRIX significantly improves worst-case class accuracy on both
clean and adversarial data, reducing inter-class robustness disparities, and
preserves overall accuracy. Our results highlight TRIX as a practical step
toward fair and effective adversarial defense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning based 3D Volume Correlation for Additive Manufacturing
  Using High-Resolution Industrial X-ray Computed Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keerthana Chand, Tobias Fritsch, Bardia Hejazi, Konstantin Poka, Giovanni Bruno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality control in additive manufacturing (AM) is vital for industrial
applications in areas such as the automotive, medical and aerospace sectors.
Geometric inaccuracies caused by shrinkage and deformations can compromise the
life and performance of additively manufactured components. Such deviations can
be quantified using Digital Volume Correlation (DVC), which compares the
computer-aided design (CAD) model with the X-ray Computed Tomography (XCT)
geometry of the components produced. However, accurate registration between the
two modalities is challenging due to the absence of a ground truth or reference
deformation field. In addition, the extremely large data size of
high-resolution XCT volumes makes computation difficult. In this work, we
present a deep learning-based approach for estimating voxel-wise deformations
between CAD and XCT volumes. Our method uses a dynamic patch-based processing
strategy to handle high-resolution volumes. In addition to the Dice Score, we
introduce a Binary Difference Map (BDM) that quantifies voxel-wise mismatches
between binarized CAD and XCT volumes to evaluate the accuracy of the
registration. Our approach shows a 9.2\% improvement in the Dice Score and a
9.9\% improvement in the voxel match rate compared to classic DVC methods,
while reducing the interaction time from days to minutes. This work sets the
foundation for deep learning-based DVC methods to generate compensation meshes
that can then be used in closed-loop correlations during the AM production
process. Such a system would be of great interest to industries since the
manufacturing process will become more reliable and efficient, saving time and
material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-RAFT: Cross-Modal Non-Rigid Registration of Blue and White Light
  Neurosurgical Hyperspectral Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Budd, Silvère Ségaud, Matthew Elliot, Graeme Stasiuk, Yijing Xie, Jonathan Shapey, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integration of hyperspectral imaging into fluorescence-guided neurosurgery
has the potential to improve surgical decision making by providing quantitative
fluorescence measurements in real-time. Quantitative fluorescence requires
paired spectral data in fluorescence (blue light) and reflectance (white light)
mode. Blue and white image acquisition needs to be performed sequentially in a
potentially dynamic surgical environment. A key component to the fluorescence
quantification process is therefore the ability to find dense cross-modal image
correspondences between two hyperspectral images taken under these drastically
different lighting conditions. We address this challenge with the introduction
of X-RAFT, a Recurrent All-Pairs Field Transforms (RAFT) optical flow model
modified for cross-modal inputs. We propose using distinct image encoders for
each modality pair, and fine-tune these in a self-supervised manner using
flow-cycle-consistency on our neurosurgical hyperspectral data. We show an
error reduction of 36.6% across our evaluation metrics when comparing to a
naive baseline and 27.83% reduction compared to an existing cross-modal optical
flow method (CrossRAFT). Our code and models will be made publicly available
after the review process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse-Dense Side-Tuner for efficient Video Temporal Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Pujol-Perich, Sergio Escalera, Albert Clapés
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Temporal Grounding (VTG) involves Moment Retrieval (MR) and Highlight
Detection (HD) based on textual queries. For this, most methods rely solely on
final-layer features of frozen large pre-trained backbones, limiting their
adaptability to new domains. While full fine-tuning is often impractical,
parameter-efficient fine-tuning -- and particularly side-tuning (ST) -- has
emerged as an effective alternative. However, prior ST approaches this problem
from a frame-level refinement perspective, overlooking the inherent sparse
nature of MR. To address this, we propose the Sparse-Dense Side-Tuner (SDST),
the first anchor-free ST architecture for VTG. We also introduce the
Reference-based Deformable Self-Attention, a novel mechanism that enhances the
context modeling of the deformable attention -- a key limitation of existing
anchor-free methods. Additionally, we present the first effective integration
of InternVideo2 backbone into an ST framework, showing its profound
implications in performance. Overall, our method significantly improves
existing ST methods, achieving highly competitive or SOTA results on
QVHighlights, TACoS, and Charades-STA, while reducing up to a 73% the parameter
count w.r.t. the existing SOTA methods. The code is publicly accessible at
https://github.com/davidpujol/SDST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EEvAct: Ea<span class="highlight-title">rl</span>y Event-Based Action Recognition with High-Rate Two-Stream
  Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Neumeier, Jules Lecomte, Nils Kazinski, Soubarna Banik, Bing Li, Axel von Arnim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing human activities early is crucial for the safety and
responsiveness of human-robot and human-machine interfaces. Due to their high
temporal resolution and low latency, event-based vision sensors are a perfect
match for this early recognition demand. However, most existing processing
approaches accumulate events to low-rate frames or space-time voxels which
limits the early prediction capabilities. In contrast, spiking neural networks
(SNNs) can process the events at a high-rate for early predictions, but most
works still fall short on final accuracy. In this work, we introduce a
high-rate two-stream SNN which closes this gap by outperforming previous work
by 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark
the SNNs within a novel early event-based recognition framework by reporting
Top-1 and Top-5 recognition scores for growing observation time. Finally, we
exemplify the impact of these methods on a real-world task of early action
triggering for human motion capture in sports.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Neuromorphic Systems (ICONS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance
  Transfer and Reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyang Zhou, Fang-Lue Zhang, Zichen Wang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in
novel view synthesis. However, rendering reflective objects remains a
significant challenge, particularly in inverse rendering and relighting. We
introduce RTR-GS, a novel inverse rendering framework capable of robustly
rendering objects with arbitrary reflectance properties, decomposing BRDF and
lighting, and delivering credible relighting results. Given a collection of
multi-view images, our method effectively recovers geometric structure through
a hybrid rendering model that combines forward rendering for radiance transfer
with deferred rendering for reflections. This approach successfully separates
high-frequency and low-frequency appearances, mitigating floating artifacts
caused by spherical harmonic overfitting when handling high-frequency details.
We further refine BRDF and lighting decomposition using an additional
physically-based deferred rendering branch. Experimental results show that our
method enhances novel view synthesis, normal estimation, decomposition, and
relighting while maintaining efficient training inference process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Guided Decoding for Object Hallucination Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xixi Liu, Ailin Deng, Christopher Zach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating object hallucination in large vision-language models (LVLMs) is
critical to their safe deployment. Existing methods either are restricted to
specific decoding methods, or demand sophisticated modifications to visual
inputs, or rely on knowledge from external models. In this work, we first
reveal the phenomenon that VLMs exhibit significant imbalance in the ``Yes''
ratio ( \ie, the fraction of ``Yes'' answers among the total number of
questions) across three different visual question answering (VQA) datasets.
Furthermore, we propose an energy-based decoding method, which dynamically
selects the hidden states from the layer with minimal energy score. It is
simple yet effective in reducing the bias for the yes ratio while boosting
performance across three benchmarks (POPE, MME, and MMVP). Our method
consistently improves accuracy and F1 score on three VQA datasets across three
commonly used VLMs over several baseline methods. The average accuracy
improvement is 4.82% compared to greedy decoding. Moreover, the average
yes-ratio gap reduction is 8.81%, meaning the proposed method is less biased as
shown in Figure 1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAPS-3D: Efficient interactive segmentation for 3D radiological imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Danielou, Daniel Tordjman, Pierre Manceron, Corentin Dancette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Promptable segmentation, introduced by the Segment Anything Model (SAM), is a
promising approach for medical imaging, as it enables clinicians to guide and
refine model predictions interactively. However, SAM's architecture is designed
for 2D images and does not extend naturally to 3D volumetric data such as CT or
MRI scans. Adapting 2D models to 3D typically involves autoregressive
strategies, where predictions are propagated slice by slice, resulting in
increased inference complexity. Processing large 3D volumes also requires
significant computational resources, often leading existing 3D methods to also
adopt complex strategies like sliding-window inference to manage memory usage,
at the cost of longer inference times and greater implementation complexity. In
this paper, we present a simplified 3D promptable segmentation method, inspired
by SegVol, designed to reduce inference time and eliminate prompt management
complexities associated with sliding windows while achieving state-of-the-art
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract accepted at MIUA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding <span class="highlight-title">Dataset</span> Bias in Medical Imaging: A Case Study on Chest
  X-rays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Dack, Chengliang Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has revisited the infamous task Name that dataset and established
that in non-medical datasets, there is an underlying bias and achieved high
Accuracies on the dataset origin task. In this work, we revisit the same task
applied to popular open-source chest X-ray datasets. Medical images are
naturally more difficult to release for open-source due to their sensitive
nature, which has led to certain open-source datasets being extremely popular
for research purposes. By performing the same task, we wish to explore whether
dataset bias also exists in these datasets. % We deliberately try to increase
the difficulty of the task by dataset transformations. We apply simple
transformations of the datasets to try to identify bias. Given the importance
of AI applications in medical imaging, it's vital to establish whether modern
methods are taking shortcuts or are focused on the relevant pathology. We
implement a range of different network architectures on the datasets: NIH,
CheXpert, MIMIC-CXR and PadChest. We hope this work will encourage more
explainable research being performed in medical imaging and the creation of
more open-source datasets in the medical domain. The corresponding code will be
released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breast Ultrasound Tumor Generation via Mask Generator and Text-Guided
  Network:A Clinically Controllable Framework with Downstream Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Pan, Hongxin Lin, Zetian Feng, Chuxuan Lin, Junyang Mo, Chu Zhang, Zijian Wu, Yi Wang, Qingqing Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of robust deep learning models for breast ultrasound (BUS)
image analysis is significantly constrained by the scarcity of expert-annotated
data. To address this limitation, we propose a clinically controllable
generative framework for synthesizing BUS images. This framework integrates
clinical descriptions with structural masks to generate tumors, enabling
fine-grained control over tumor characteristics such as morphology,
echogencity, and shape. Furthermore, we design a semantic-curvature mask
generator, which synthesizes structurally diverse tumor masks guided by
clinical priors. During inference, synthetic tumor masks serve as input to the
generative framework, producing highly personalized synthetic BUS images with
tumors that reflect real-world morphological diversity. Quantitative
evaluations on six public BUS datasets demonstrate the significant clinical
utility of our synthetic images, showing their effectiveness in enhancing
downstream breast cancer diagnosis tasks. Furthermore, visual Turing tests
conducted by experienced sonographers confirm the realism of the generated
images, indicating the framework's potential to support broader clinical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing the Past and Present: A Coordinated Replay Framework for
  Federated Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Qi, Lei Meng, Han Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Class Incremental Learning (FCIL) aims to collaboratively process
continuously increasing incoming tasks across multiple clients. Among various
approaches, data replay has become a promising solution, which can alleviate
forgetting by reintroducing representative samples from previous tasks.
However, their performance is typically limited by class imbalance, both within
the replay buffer due to limited global awareness and between replayed and
newly arrived classes. To address this issue, we propose a class wise balancing
data replay method for FCIL (FedCBDR), which employs a global coordination
mechanism for class-level memory construction and reweights the learning
objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has
two key components: 1) the global-perspective data replay module reconstructs
global representations of prior task in a privacy-preserving manner, which then
guides a class-aware and importance-sensitive sampling strategy to achieve
balanced replay; 2) Subsequently, to handle class imbalance across tasks, the
task aware temperature scaling module adaptively adjusts the temperature of
logits at both class and instance levels based on task dynamics, which reduces
the model's overconfidence in majority classes while enhancing its sensitivity
to minority classes. Experimental results verified that FedCBDR achieves
balanced class-wise sampling under heterogeneous data distributions and
improves generalization under task imbalance between earlier and recent tasks,
yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack
  on Unified Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Zhao, Xinyang Jiang, Junyao Gao, Yuhao Xue, Cairong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified vision-language models(VLMs) have recently shown remarkable progress,
enabling a single model to flexibly address diverse tasks through different
instructions within a shared computational architecture. This instruction-based
control mechanism creates unique security challenges, as adversarial inputs
must remain effective across multiple task instructions that may be
unpredictably applied to process the same malicious content. In this paper, we
introduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with
GPT-4-assisted annotations for systematically evaluating cross-task adversarial
attacks on unified VLMs. CrossVLAD centers on the object-change
objective-consistently manipulating a target object's classification across
four downstream tasks-and proposes a novel success rate metric that measures
simultaneous misclassification across all tasks, providing a rigorous
evaluation of adversarial transferability. To tackle this challenge, we present
CRAFT (Cross-task Region-based Attack Framework with Token-alignment), an
efficient region-centric attack method. Extensive experiments on Florence-2 and
other popular unified VLMs demonstrate that our method outperforms existing
approaches in both overall cross-task attack performance and targeted
object-change success rates, highlighting its effectiveness in adversarially
influencing unified VLMs across diverse tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion
  Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shang, Dongwei Ren, Wanying Zhang, Pengfei Zhu, Qinghua Hu, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local motion blur in digital images originates from the relative motion
between dynamic objects and static imaging systems during exposure. Existing
deblurring methods face significant challenges in addressing this problem due
to their inefficient allocation of computational resources and inadequate
handling of spatially varying blur patterns. To overcome these limitations, we
first propose a trainable mask predictor that identifies blurred regions in the
image. During training, we employ blur masks to exclude sharp regions. For
inference optimization, we implement structural reparameterization by
converting $3\times 3$ convolutions to computationally efficient $1\times 1$
convolutions, enabling pixel-level pruning of sharp areas to reduce
computation. Second, we develop an intra-frame motion analyzer that translates
relative pixel displacements into motion trajectories, establishing adaptive
guidance for region-specific blur restoration. Our method is trained end-to-end
using a combination of reconstruction loss, reblur loss, and mask loss guided
by annotated blur masks. Extensive experiments demonstrate superior performance
over state-of-the-art methods on both local and global blur datasets while
reducing FLOPs by 49\% compared to SOTA models (e.g., LMD-ViT). The source code
is available at https://github.com/shangwei5/M2AENet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressive Imaging Reconstruction via Tensor Decomposed
  Multi-Resolution Grid Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Jin, Yisi Luo, Xile Zhao, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressive imaging (CI) reconstruction, such as snapshot compressive imaging
(SCI) and compressive sensing magnetic resonance imaging (MRI), aims to recover
high-dimensional images from low-dimensional compressed measurements. This
process critically relies on learning an accurate representation of the
underlying high-dimensional image. However, existing unsupervised
representations may struggle to achieve a desired balance between
representation ability and efficiency. To overcome this limitation, we propose
Tensor Decomposed multi-resolution Grid encoding (GridTD), an unsupervised
continuous representation framework for CI reconstruction. GridTD optimizes a
lightweight neural network and the input tensor decomposition model whose
parameters are learned via multi-resolution hash grid encoding. It inherently
enjoys the hierarchical modeling ability of multi-resolution grid encoding and
the compactness of tensor decomposition, enabling effective and efficient
reconstruction of high-dimensional images. Theoretical analyses for the
algorithm's Lipschitz property, generalization error bound, and fixed-point
convergence reveal the intrinsic superiority of GridTD as compared with
existing continuous representation models. Extensive experiments across diverse
CI tasks, including video SCI, spectral SCI, and compressive dynamic MRI
reconstruction, consistently demonstrate the superiority of GridTD over
existing methods, positioning GridTD as a versatile and state-of-the-art CI
reconstruction method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D-CNN and VQ-VAE Autoencoders for Compression and Denoising of
  Industrial X-ray Computed Tomography Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bardia Hejazi, Keerthana Chand, Tobias Fritsch, Giovanni Bruno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ever-growing volume of data in imaging sciences stemming from the
advancements in imaging technologies, necessitates efficient and reliable
storage solutions for such large datasets. This study investigates the
compression of industrial X-ray computed tomography (XCT) data using deep
learning autoencoders and examines how these compression algorithms affect the
quality of the recovered data. Two network architectures with different
compression rates were used, a deep convolution neural network (D-CNN) and a
vector quantized variational autoencoder (VQ-VAE). The XCT data used was from a
sandstone sample with a complex internal pore network. The quality of the
decoded images obtained from the two different deep learning architectures with
different compression rates were quantified and compared to the original input
data. In addition, to improve image decoding quality metrics, we introduced a
metric sensitive to edge preservation, which is crucial for three-dimensional
data analysis. We showed that different architectures and compression rates are
required depending on the specific characteristics needed to be preserved for
later analysis. The findings presented here can aid scientists to determine the
requirements and strategies for their data storage and analysis needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree-Mamba: A Tree-Aware Mamba for Underwater Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peixian Zhuang, Yijian Wang, Zhenqi Fu, Hongliang Zhang, Sam Kwong, Chongyi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater Monocular Depth Estimation (UMDE) is a critical task that aims to
estimate high-precision depth maps from underwater degraded images caused by
light absorption and scattering effects in marine environments. Recently,
Mamba-based methods have achieved promising performance across various vision
tasks; however, they struggle with the UMDE task because their inflexible state
scanning strategies fail to model the structural features of underwater images
effectively. Meanwhile, existing UMDE datasets usually contain unreliable depth
labels, leading to incorrect object-depth relationships between underwater
images and their corresponding depth maps. To overcome these limitations, we
develop a novel tree-aware Mamba method, dubbed Tree-Mamba, for estimating
accurate monocular depth maps from underwater degraded images. Specifically, we
propose a tree-aware scanning strategy that adaptively constructs a minimum
spanning tree based on feature similarity. The spatial topological features
among the tree nodes are then flexibly aggregated through bottom-up and
top-down traversals, enabling stronger multi-scale feature representation
capabilities. Moreover, we construct an underwater depth estimation benchmark
(called BlueDepth), which consists of 38,162 underwater image pairs with
reliable depth labels. This benchmark serves as a foundational dataset for
training existing deep learning-based UMDE methods to learn accurate
object-depth relationships. Extensive experiments demonstrate the superiority
of the proposed Tree-Mamba over several leading methods in both qualitative
results and quantitative evaluations with competitive computational efficiency.
Code and dataset will be available at https://wyjgr.github.io/Tree-Mamba.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shin'ya Yamaguchi, Kosuke Nishida, Daiki Chijiwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have demonstrated remarkable
capabilities by integrating pre-trained vision encoders with large language
models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting
has been adapted for LVLMs to enhance multi-modal reasoning by generating
intermediate rationales based on visual and textual inputs. While CoT is
assumed to improve grounding and accuracy in LVLMs, our experiments reveal a
key challenge: existing LVLMs often ignore the contents of generated rationales
in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as
a KL-constrained reward maximization focused on rationale-conditional
log-likelihood. As the optimal solution, we propose rationale-enhanced decoding
(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes
visual and rationale information by multiplying distinct image-conditional and
rationale-conditional next token distributions. Extensive experiments show that
RED consistently and significantly improves reasoning over standard CoT and
other decoding methods across multiple benchmarks and LVLMs. Our work offers a
practical and effective approach to improve both the faithfulness and accuracy
of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded
multi-modal systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action Unit Enhance Dynamic Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liu, Lingna Gu, Chen Shi, Xiaolan Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Facial Expression Recognition(DFER) is a rapidly evolving field of
research that focuses on the recognition of time-series facial expressions.
While previous research on DFER has concentrated on feature learning from a
deep learning perspective, we put forward an AU-enhanced Dynamic Facial
Expression Recognition architecture, namely AU-DFER, that incorporates
AU-expression knowledge to enhance the effectiveness of deep learning modeling.
In particular, the contribution of the Action Units(AUs) to different
expressions is quantified, and a weight matrix is designed to incorporate a
priori knowledge. Subsequently, the knowledge is integrated with the learning
outcomes of a conventional deep learning network through the introduction of AU
loss. The design is incorporated into the existing optimal model for dynamic
expression recognition for the purpose of validation. Experiments are conducted
on three recent mainstream open-source approaches to DFER on the principal
datasets in this field. The results demonstrate that the proposed architecture
outperforms the state-of-the-art(SOTA) methods without the need for additional
arithmetic and generally produces improved results. Furthermore, we investigate
the potential of AU loss function redesign to address data label imbalance
issues in established dynamic expression datasets. To the best of our
knowledge, this is the first attempt to integrate quantified AU-expression
knowledge into various DFER models. We also devise strategies to tackle label
imbalance, or minor class problems. Our findings suggest that employing a
diverse strategy of loss function design can enhance the effectiveness of DFER.
This underscores the criticality of addressing data imbalance challenges in
mainstream datasets within this domain. The source code is available at
https://github.com/Cross-Innovation-Lab/AU-DFER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attend-and-Refine: Interactive keypoint estimation and quantitative
  cervical vertebrae analysis for bone age assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhee Kim, Taesung Kim, Taewoo Kim, Dong-Wook Kim, Byungduk Ahn, Yoon-Ji Kim, In-Seok Song, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In pediatric orthodontics, accurate estimation of growth potential is
essential for developing effective treatment strategies. Our research aims to
predict this potential by identifying the growth peak and analyzing cervical
vertebra morphology solely through lateral cephalometric radiographs. We
accomplish this by comprehensively analyzing cervical vertebral maturation
(CVM) features from these radiographs. This methodology provides clinicians
with a reliable and efficient tool to determine the optimal timings for
orthodontic interventions, ultimately enhancing patient outcomes. A crucial
aspect of this approach is the meticulous annotation of keypoints on the
cervical vertebrae, a task often challenged by its labor-intensive nature. To
mitigate this, we introduce Attend-and-Refine Network (ARNet), a
user-interactive, deep learning-based model designed to streamline the
annotation process. ARNet features Interaction-guided recalibration network,
which adaptively recalibrates image features in response to user feedback,
coupled with a morphology-aware loss function that preserves the structural
consistency of keypoints. This novel approach substantially reduces manual
effort in keypoint identification, thereby enhancing the efficiency and
accuracy of the process. Extensively validated across various datasets, ARNet
demonstrates remarkable performance and exhibits wide-ranging applicability in
medical imaging. In conclusion, our research offers an effective AI-assisted
diagnostic tool for assessing growth potential in pediatric orthodontics,
marking a significant advancement in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Medical Image Analysis (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MolCLIP: A Molecular-Auxiliary CLIP Framework for Identifying Drug
  Mechanism of Action Based on Time-Lapsed Mitochondrial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengqian Pang, Chunyue Lei, Hongfei Zhao, Chenghao Liu, Zhiqiang Xing, Huafeng Wang, Chuyang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug Mechanism of Action (MoA) mainly investigates how drug molecules
interact with cells, which is crucial for drug discovery and clinical
application. Recently, deep learning models have been used to recognize MoA by
relying on high-content and fluorescence images of cells exposed to various
drugs. However, these methods focus on spatial characteristics while
overlooking the temporal dynamics of live cells. Time-lapse imaging is more
suitable for observing the cell response to drugs. Additionally, drug molecules
can trigger cellular dynamic variations related to specific MoA. This indicates
that the drug molecule modality may complement the image counterpart. This
paper proposes MolCLIP, the first visual language model to combine microscopic
cell video- and molecule-modalities. MolCLIP designs a molecule-auxiliary CLIP
framework to guide video features in learning the distribution of the molecular
latent space. Furthermore, we integrate a metric learning strategy with MolCLIP
to optimize the aggregation of video features. Experimental results on the
MitoDataset demonstrate that MolCLIP achieves improvements of 51.2% and 20.5%
in mAP for drug identification and MoA recognition, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the gap in FER: addressing age bias in deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. Xavier Gaya-Morey, Julia Sanchez-Perez, Cristina Manresa-Yee, Jose M. Buades-Rubio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Expression Recognition (FER) systems based on deep learning have
achieved impressive performance in recent years. However, these models often
exhibit demographic biases, particularly with respect to age, which can
compromise their fairness and reliability. In this work, we present a
comprehensive study of age-related bias in deep FER models, with a particular
focus on the elderly population. We first investigate whether recognition
performance varies across age groups, which expressions are most affected, and
whether model attention differs depending on age. Using Explainable AI (XAI)
techniques, we identify systematic disparities in expression recognition and
attention patterns, especially for "neutral", "sadness", and "anger" in elderly
individuals. Based on these findings, we propose and evaluate three bias
mitigation strategies: Multi-task Learning, Multi-modal Input, and Age-weighted
Loss. Our models are trained on a large-scale dataset, AffectNet, with
automatically estimated age labels and validated on balanced benchmark datasets
that include underrepresented age groups. Results show consistent improvements
in recognition accuracy for elderly individuals, particularly for the most
error-prone expressions. Saliency heatmap analysis reveals that models trained
with age-aware strategies attend to more relevant facial regions for each age
group, helping to explain the observed improvements. These findings suggest
that age-related bias in FER can be effectively mitigated using simple training
modifications, and that even approximate demographic labels can be valuable for
promoting fairness in large-scale affective computing systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhitao Wang, Hengyu Man, Wenrui Li, Xingtao Wang, Xiaopeng Fan, Debin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in video generation techniques have given rise to an emerging
paradigm of generative video coding, aiming to achieve semantically accurate
reconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong
generative priors. However, most existing methods are limited by domain
specificity (e.g., facial or human videos) or an excessive dependence on
high-level text guidance, which often fails to capture motion details and
results in unrealistic reconstructions. To address these challenges, we propose
a Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC
employs a semantic-aware sparse motion sampling pipeline to effectively bridge
low-level motion tracking with high-level semantic understanding by extracting
pixel-wise motion as sparse trajectory points based on their semantic
importance, not only significantly reducing the bitrate but also preserving
critical temporal semantic information. In addition, by incorporating
trajectory-aligned loss constraints into diffusion processes, we introduce a
training-free latent space guidance mechanism to ensure physically plausible
motion patterns without sacrificing the inherent capabilities of generative
models. Experimental results demonstrate that our framework outperforms both
traditional codecs and state-of-the-art end-to-end video compression methods
under ULB conditions. Furthermore, additional experiments confirm that our
approach achieves more precise motion control than existing text-guided
methods, paving the way for a novel direction of generative video coding guided
by geometric motion modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyan Cong, Yiqing Liang, Yancheng Zhang, Ziyi Yang, Yan Wang, Boris Ivanovic, Marco Pavone, Chen Chen, Zhangyang Wang, Zhiwen Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial intelligence, encompassing 3D reconstruction, perception, and
reasoning, is fundamental to applications such as robotics, aerial imaging, and
extended reality. A key enabler is the real-time, accurate estimation of core
3D attributes (camera parameters, point clouds, depth maps, and 3D point
tracks) from unstructured or streaming imagery. Inspired by the success of
large foundation models in language and 2D vision, a new class of end-to-end 3D
geometric foundation models (GFMs) has emerged, directly predicting dense 3D
representations in a single feed-forward pass, eliminating the need for slow or
unavailable precomputed camera parameters. Since late 2023, the field has
exploded with diverse variants, but systematic evaluation is lacking. In this
work, we present the first comprehensive benchmark for 3D GFMs, covering five
core tasks: sparse-view depth estimation, video depth estimation, 3D
reconstruction, multi-view pose estimation, novel view synthesis, and spanning
both standard and challenging out-of-distribution datasets. Our standardized
toolkit automates dataset handling, evaluation protocols, and metric
computation to ensure fair, reproducible comparisons. We evaluate 16
state-of-the-art GFMs, revealing their strengths and limitations across tasks
and domains, and derive key insights to guide future model scaling and
optimization. All code, evaluation scripts, and processed data will be publicly
released to accelerate research in 3D spatial intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://e3dbench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across diverse tasks, yet they lag significantly behind humans in
spatial reasoning. We investigate this gap through Transformation-Driven Visual
Reasoning (TVR), a challenging task requiring identification of object
transformations across images under varying viewpoints. While traditional
Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in
cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from
inefficient exploration and slow convergence. To address these limitations, we
propose STAR-R1, a novel framework that integrates a single-stage RL paradigm
with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1
rewards partial correctness while penalizing excessive enumeration and passive
inaction, enabling efficient exploration and precise reasoning. Comprehensive
evaluations demonstrate that STAR-R1 achieves state-of-the-art performance
across all 11 metrics, outperforming SFT by 23% in cross-view scenarios.
Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its
unique ability to compare all objects for improving spatial reasoning. Our work
provides critical insights in advancing the research of MLLMs and reasoning
models. The codes, model weights, and data will be publicly available at
https://github.com/zongzhao23/STAR-R1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Generative AI: Multi-modal LLMs, <span class="highlight-title">Diffusion</span>s and the
  Unification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Yuwei Zhou, Bin Huang, Hong Chen, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal generative AI (Artificial Intelligence) has attracted increasing
attention from both academia and industry. Particularly, two dominant families
of techniques have emerged: i) Multi-modal large language models (LLMs)
demonstrate impressive ability for multi-modal understanding; and ii) Diffusion
models exhibit remarkable multi-modal powers in terms of multi-modal
generation. Therefore, this paper provides a comprehensive overview of
multi-modal generative AI, including multi-modal LLMs, diffusions, and the
unification for understanding and generation. To lay a solid foundation for
unified models, we first provide a detailed review of both multi-modal LLMs and
diffusion models respectively, including their probabilistic modeling
procedure, multi-modal architecture design, and advanced applications to
image/video LLMs as well as text-to-image/video generation. Furthermore, we
explore the emerging efforts toward unified models for understanding and
generation. To achieve the unification of understanding and generation, we
investigate key designs including autoregressive-based and diffusion-based
modeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then
introduce several strategies for unified models, analyzing their potential
advantages and disadvantages. In addition, we summarize the common datasets
widely used for multi-modal generative AI pretraining. Last but not least, we
present several challenging future research directions which may contribute to
the ongoing advancement of multi-modal generative AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Inverse Problems using <span class="highlight-title">Diffusion</span> with Iterative Colored
  Renoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imaging inverse problems can be solved in an unsupervised manner using
pre-trained diffusion models, but doing so requires approximating the gradient
of the measurement-conditional score function in the diffusion reverse process.
We show that the approximations produced by existing methods are relatively
poor, especially early in the reverse process, and so we propose a new approach
that iteratively reestimates and "renoises" the estimate several times per
diffusion step. This iterative approach, which we call Fast Iterative REnoising
(FIRE), injects colored noise that is shaped to ensure that the pre-trained
diffusion model always sees white noise, in accordance with how it was trained.
We then embed FIRE into the DDIM reverse process and show that the resulting
"DDfire" offers state-of-the-art accuracy and runtime on several linear inverse
problems, as well as phase retrieval. Our implementation is at
https://github.com/matt-bendel/DDfire
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Vision <span class="highlight-title">Transformer</span> Representations Semantically Meaningful? A Case
  Study in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Image Modeling: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Hondru, Florinel Alin Croitoru, Shervin Minaee, Radu Tudor Ionescu, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we survey recent studies on masked image modeling (MIM), an
approach that emerged as a powerful self-supervised learning technique in
computer vision. The MIM task involves masking some information, e.g. pixels,
patches, or even latent representations, and training a model, usually an
autoencoder, to predicting the missing information by using the context
available in the visible part of the input. We identify and formalize two
categories of approaches on how to implement MIM as a pretext task, one based
on reconstruction and one based on contrastive learning. Then, we construct a
taxonomy and review the most prominent papers in recent years. We complement
the manually constructed taxonomy with a dendrogram obtained by applying a
hierarchical clustering algorithm. We further identify relevant clusters via
manually inspecting the resulting dendrogram. Our review also includes datasets
that are commonly used in MIM research. We aggregate the performance results of
various masked image modeling methods on the most popular datasets, to
facilitate the comparison of competing methods. Finally, we identify research
gaps and propose several interesting directions of future work. We supplement
our survey with the following public repository containing organized
references: https://github.com/vladhondru25/MIM-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event
  Slicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Argha Sen, Nuwan Bandara, Ila Gokarn, Thivya Kandappu, Archan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eye-tracking technology has gained significant attention in recent years due
to its wide range of applications in human-computer interaction, virtual and
augmented reality, and wearable health. Traditional RGB camera-based
eye-tracking systems often struggle with poor temporal resolution and
computational constraints, limiting their effectiveness in capturing rapid eye
movements. To address these limitations, we propose EyeTrAES, a novel approach
using neuromorphic event cameras for high-fidelity tracking of natural
pupillary movement that shows significant kinematic variance. One of EyeTrAES's
highlights is the use of a novel adaptive windowing/slicing algorithm that
ensures just the right amount of descriptive asynchronous event data
accumulation within an event frame, across a wide range of eye movement
patterns. EyeTrAES then applies lightweight image processing functions over
accumulated event frames from just a single eye to perform pupil segmentation
and tracking. We show that these methods boost pupil tracking fidelity by 6+%,
achieving IoU~=92%, while incurring at least 3x lower latency than competing
pure event-based eye tracking alternatives [38]. We additionally demonstrate
that the microscopic pupillary motion captured by EyeTrAES exhibits distinctive
variations across individuals and can thus serve as a biometric fingerprint.
For robust user authentication, we train a lightweight per-user Random Forest
classifier using a novel feature vector of short-term pupillary kinematics,
comprising a sliding window of pupil (location, velocity, acceleration)
triples. Experimental studies with two different datasets demonstrate that the
EyeTrAES-based authentication technique can simultaneously achieve high
authentication accuracy (~=0.82) and low processing latency (~=12ms), and
significantly outperform multiple state-of-the-art competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages,15 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skywork-R1V3 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Jianhao Zhang, Yahui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mamba-CL: Optimizing Selective State Space Model in Null Space for
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De Cheng, Yue Lu, Lingfeng He, Shizhou Zhang, Xi Yang, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) aims to equip AI models with the ability to learn a
sequence of tasks over time, without forgetting previously learned knowledge.
Recently, State Space Models (SSMs), particularly the Mamba model, have
achieved notable success in computer vision. Building on the strengths of SSMs,
this study explores leveraging the Mamba model for CL. Therefore, we introduce
Mamba-CL, a framework that continuously fine-tunes the core SSMs of the
large-scale Mamba foundation model by updating parameters orthogonal to the
feature subspace of previous tasks. This approach theoretically guarantees the
consistency objective aiming to preserves consistent output for each SSM module
across both previous and current tasks, so as to overcome catastrophic
forgetting issue. Specifically, we achieve this goal by deducing the overall
consistency constraints on four key time-invariant parameters in the Mamba
model, streamlining its recurrent state-space structure and non-linear
discretization process in SSM. In practice, we apply the null-space projection
to efficiently implement the orthogonality within Mamba model. Extensive
experiments on four class-incremental benchmarks demonstrate the effectiveness
of Mamba-CL for anti-forgetting, achieving superior performances to
state-of-the-art methods. Code is available in the supplementary materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed
  View Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runjia Li, Philip Torr, Andrea Vedaldi, Tomas Jakab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel memory mechanism to build video generators that can
explore environments interactively. Similar results have previously been
achieved by out-painting 2D views of the scene while incrementally
reconstructing its 3D geometry, which quickly accumulates errors, or by video
generators with a short context window, which struggle to maintain scene
coherence over the long term. To address these limitations, we introduce
Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by
indexing them geometrically based on the 3D surface elements (surfels) they
have observed. VMem enables the efficient retrieval of the most relevant past
views when generating new ones. By focusing only on these relevant views, our
method produces consistent explorations of imagined environments at a fraction
of the computational cost of using all past views as context. We evaluate our
approach on challenging long-term scene synthesis benchmarks and demonstrate
superior performance compared to existing methods in maintaining scene
coherence and camera control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://v-mem.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-source automatic pipeline for efficient conversion of large-scale
  point clouds to IFC format 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11498v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11498v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Slávek Zbirovský, Václav Nežerka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building Information Modeling (BIM) is an essential component in the
sustainable reconstruction and revitalization of ageing structures. However,
model creation usually relies on laborious manual transformation of the
unstructured point cloud data provided by laser scans or photogrammetry. This
paper presents Cloud2BIM, an open-source software tool designed to automate the
conversion of point clouds into BIM models compliant with the Industry
Foundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms for
wall and slab segmentation, opening detection, and room zoning based on real
wall surfaces, resulting in a comprehensive and fully automated workflow.
Unlike existing tools, it avoids computationally- and calibration-intensive
techniques such as RANSAC, supports non-orthogonal geometries, and provides
unprecedented processing speed-achieving results up to seven times faster than
fastest competing solutions. Systematic validation using benchmark datasets
confirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution for
generating accurate BIM models, capable of converting extensive point cloud
datasets for entire buildings into IFC format with minimal user input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published version, 23 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Underwater Monocular Metric Depth Estimation: Real-Wo<span class="highlight-title">rl</span>d Benchmarks and
  Synthetic Fine-Tuning with Vision Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Cai, Christopher Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation has recently progressed beyond ordinal depth to
provide metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, including FLSea and SQUID. We evaluated a diverse set
of state-of-the-art Vision Foundation Models across a range of underwater
conditions and depth ranges. Our results show that large-scale models trained
on terrestrial data (real or synthetic) are effective in in-air settings, but
perform poorly underwater due to significant domain shifts. To address this, we
fine-tune Depth Anything V2 with a ViT-S backbone encoder on a synthetic
underwater variant of the Hypersim dataset, which we simulated using a
physically based underwater image formation model. Our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. This study
presents a detailed evaluation and visualization of monocular metric depth
estimation in underwater scenes, emphasizing the importance of domain
adaptation and scale-aware supervision for achieving robust and generalizable
metric depth predictions using foundation models in challenging environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VOTE: Vision-Language-Action Optimization with Trajectory Ensemble
  Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyi Lin, Amir Taherin, Arash Akbari, Arman Akbari, Lei Lu, Guangyu Chen, Taskin Padir, Xiaomeng Yang, Weiwei Chen, Yiqian Li, Xue Lin, David Kaeli, Pu Zhao, Yanzhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35x faster inference and 145 Hz throughput.
All the details and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-Enhanced Deep Learning Ensemble for Breast Density
  Classification in Mammography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Sharifian, Xiaotong Hong, Alireza Karimian, Mehdi Amini, Hossein Arabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast density assessment is a crucial component of mammographic
interpretation, with high breast density (BI-RADS categories C and D)
representing both a significant risk factor for developing breast cancer and a
technical challenge for tumor detection. This study proposes an automated deep
learning system for robust binary classification of breast density (low: A/B
vs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four
advanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,
and DenseNet121, each enhanced with channel attention mechanisms. To address
the inherent class imbalance, we developed a novel Combined Focal Label
Smoothing Loss function that integrates focal loss, label smoothing, and
class-balanced weighting. Our preprocessing pipeline incorporated advanced
techniques, including contrast-limited adaptive histogram equalization (CLAHE)
and comprehensive data augmentation. The individual models were combined
through an optimized ensemble voting approach, achieving superior performance
(AUC: 0.963, F1-score: 0.952) compared to any single model. This system
demonstrates significant potential to standardize density assessments in
clinical practice, potentially improving screening efficiency and early cancer
detection rates while reducing inter-observer variability among radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE Nuclear Science Symposium, Medical Imaging Conference and
  Room Temperature Semiconductor Detector Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Information-driven design of imaging systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20559v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20559v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Pinkard, Leyla Kabuli, Eric Markley, Tiffany Chien, Jiantao Jiao, Laura Waller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern imaging systems that computationally process raw measurements
before or instead of human viewing, information content matters more than
visual appearance. However, developing information estimators that can handle
the complexity of real-world measurements yet remain practical enough for
widespread use has proven challenging. We introduce a data-driven approach for
estimating mutual information between unknown objects and their noisy
measurements. Our technique fits probabilistic models to measurements and their
noise processes, quantifying information content without requiring ground truth
data or making assumptions about object structure. We validate our approach
across diverse applications-color photography, radio astronomy, lensless
imaging, and microscopy-demonstrating that information estimates reliably
predict system performance. Finally, we introduce Information-Driven Encoder
Analysis Learning (IDEAL), which optimizes imaging systems to maximize
information capture. Our work unlocks information theory as a powerful,
practical tool for analyzing and designing imaging systems across a broad range
of applications.
  A video summarizing this work can be found at:
https://waller-lab.github.io/EncodingInformationWebsite/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PWD: Prior-Guided and Wavelet-Enhanced <span class="highlight-title">Diffusion</span> Model for Limited-Angle
  CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models have received increasing attention in medical
imaging, particularly in limited-angle computed tomography (LACT). Standard
diffusion models achieve high-quality image reconstruction but require a large
number of sampling steps during inference, resulting in substantial
computational overhead. Although skip-sampling strategies have been proposed to
improve efficiency, they often lead to loss of fine structural details. To
address this issue, we propose a prior information embedding and wavelet
feature fusion fast sampling diffusion model for LACT reconstruction. The PWD
enables efficient sampling while preserving reconstruction fidelity in LACT,
and effectively mitigates the degradation typically introduced by
skip-sampling. Specifically, during the training phase, PWD maps the
distribution of LACT images to that of fully sampled target images, enabling
the model to learn structural correspondences between them. During inference,
the LACT image serves as an explicit prior to guide the sampling trajectory,
allowing for high-quality reconstruction with significantly fewer steps. In
addition, PWD performs multi-scale feature fusion in the wavelet domain,
effectively enhancing the reconstruction of fine details by leveraging both
low-frequency and high-frequency information. Quantitative and qualitative
evaluations on clinical dental arch CBCT and periapical datasets demonstrate
that PWD outperforms existing methods under the same sampling condition. Using
only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and
10% gain in SSIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunHOI: Annotation-Free 3D Hand-Object Interaction Generation via
  Functional Text Guidanc 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Tian, Xueyu Sun, Haoyuan He, Linji Hao, Ning Ding, Caigui Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-object interaction(HOI) is the fundamental link between human and
environment, yet its dexterous and complex pose significantly challenges for
gesture control. Despite significant advances in AI and robotics, enabling
machines to understand and simulate hand-object interactions, capturing the
semantics of functional grasping tasks remains a considerable challenge. While
previous work can generate stable and correct 3D grasps, they are still far
from achieving functional grasps due to unconsidered grasp semantics. To
address this challenge, we propose an innovative two-stage framework,
Functional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven by
functional text. This framework consists of a text-guided 3D model generator,
Functional Grasp Generator (FGG), and a pose optimization strategy, Functional
Grasp Refiner (FGR). FGG generates 3D models of hands and objects based on text
input, while FGR fine-tunes the poses using Object Pose Approximator and energy
functions to ensure the relative position between the hand and object aligns
with human intent and remains physically plausible. Extensive experiments
demonstrate that our approach achieves precise and high-quality HOI generation
without requiring additional 3D annotation data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Chen, Zecheng Zhao, Jingcai Guo, Jingjing Li, Zi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot learning (ZSL) aims to recognize unseen classes without labeled
training examples by leveraging class-level semantic descriptors such as
attributes. A fundamental challenge in ZSL is semantic misalignment, where
semantic-unrelated information involved in visual features introduce ambiguity
to visual-semantic interaction. Unlike existing methods that suppress
semantic-unrelated information post hoc either in the feature space or the
model space, we propose addressing this issue at the input stage, preventing
semantic-unrelated patches from propagating through the network. To this end,
we introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a
transformer-based framework designed to enhance visual-semantic alignment.
Specifically, we propose a self-supervised patch selection mechanism that
preemptively learns to identify semantic-unrelated patches in the input space.
This is trained with the supervision from aggregated attention scores across
all transformer layers, which estimate each patch's semantic score. As removing
semantic-unrelated patches from the input sequence may disrupt object
structure, we replace them with learnable patch embeddings. With initialization
from word embeddings, we can ensure they remain semantically meaningful
throughout feature extraction. Extensive experiments on ZSL benchmarks
demonstrate that SVIP achieves state-of-the-art performance results while
providing more interpretable and semantically rich feature representations.
Code is available at https://github.com/uqzhichen/SVIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Representations for Fine-grained Multi-label Critical View
  of Safety Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Britty Baby, Vinkle Srivastav, Pooja P. Jain, Kun Yuan, Pietro Mascagni, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Critical View of Safety (CVS) is crucial for safe laparoscopic
cholecystectomy, yet assessing CVS criteria remains a complex and challenging
task, even for experts. Traditional models for CVS recognition depend on
vision-only models learning with costly, labor-intensive spatial annotations.
This study investigates how text can be harnessed as a powerful tool for both
training and inference in multi-modal surgical foundation models to automate
CVS recognition. Unlike many existing multi-modal models, which are primarily
adapted for multi-class classification, CVS recognition requires a multi-label
framework. Zero-shot evaluation of existing multi-modal surgical models shows a
significant performance gap for this task. To address this, we propose
CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,
binary classification across multiple labels by aligning image embeddings with
textual descriptions of each CVS criterion using positive and negative prompts.
By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the
Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the
ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that
CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,
boosts CVS recognition over image-only methods. We also propose text-specific
inference methods, that helps in analysing the image-text alignment. While
further work is needed to match state-of-the-art spatial annotation-based
methods, this approach highlights the potential of adapting generalist models
to specialized surgical tasks. Code:
https://github.com/CAMMA-public/CVS-AdaptNet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation of Multi-modal Representation Models for Multi-task Surgical
  Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Walimbe, Britty Baby, Vinkle Srivastav, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical AI often involves multiple tasks within a single procedure, like
phase recognition or assessing the Critical View of Safety in laparoscopic
cholecystectomy. Traditional models, built for one task at a time, lack
flexibility, requiring a separate model for each. To address this, we introduce
MML-SurgAdapt, a unified multi-task framework with Vision-Language Models
(VLMs), specifically CLIP, to handle diverse surgical tasks through natural
language supervision. A key challenge in multi-task learning is the presence of
partial annotations when integrating different tasks. To overcome this, we
employ Single Positive Multi-Label (SPML) learning, which traditionally reduces
annotation burden by training models with only one positive label per instance.
Our framework extends this approach to integrate data from multiple surgical
tasks within a single procedure, enabling effective learning despite incomplete
or noisy annotations. We demonstrate the effectiveness of our model on a
combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,
utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt
performs comparably to task-specific benchmarks, with the added advantage of
handling noisy annotations. It also outperforms the existing SPML frameworks
for the task. By reducing the required labels by 23%, our approach proposes a
more scalable and efficient labeling process, significantly easing the
annotation burden on clinicians. To our knowledge, this is the first
application of SPML to integrate data from multiple surgical tasks, presenting
a novel and generalizable solution for multi-task learning in surgical computer
vision. Implementation is available at:
https://github.com/CAMMA-public/MML-SurgAdapt
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Gradient Stabilization for Small Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huixin Sun, Yanjing Li, Linlin Yang, Xianbin Cao, Baochang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in generic object detection, there remains a performance gap
in detecting small objects compared to normal-scale objects. We reveal that
conventional object localization methods suffer from gradient instability in
small objects due to sharper loss curvature, leading to a convergence
challenge. To address the issue, we propose Uncertainty-Aware Gradient
Stabilization (UGS), a framework that reformulates object localization as a
classification task to stabilize gradients. UGS quantizes continuous labels
into interval non-uniform discrete representations. Under a
classification-based objective, the localization branch generates bounded and
confidence-driven gradients, mitigating instability. Furthermore, UGS
integrates an uncertainty minimization (UM) loss that reduces prediction
variance and an uncertainty-guided refinement (UR) module that identifies and
refines high-uncertainty regions via perturbations. Evaluated on four
benchmarks, UGS consistently improves anchor-based, anchor-free, and leading
small object detectors. Especially, UGS enhances DINO-5scale by 2.6 AP on
VisDrone, surpassing previous state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">35</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniTac: Whole-Robot Touch Sensing Without Tactile Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanjia Fu, Hongyu Li, Ivy X. He, Stefanie Tellex, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots can better interact with humans and unstructured environments through
touch sensing. However, most commercial robots are not equipped with tactile
skins, making it challenging to achieve even basic touch-sensing functions,
such as contact localization. We present UniTac, a data-driven whole-body
touch-sensing approach that uses only proprioceptive joint sensors and does not
require the installation of additional sensors. Our approach enables a robot
equipped solely with joint sensors to localize contacts. Our goal is to
democratize touch sensing and provide an off-the-shelf tool for HRI researchers
to provide their robots with touch-sensing capabilities. We validate our
approach on two platforms: the Franka robot arm and the Spot quadruped. On
Franka, we can localize contact to within 8.0 centimeters, and on Spot, we can
localize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU
without adding any additional sensors to the robot. Project website:
https://ivl.cs.brown.edu/research/unitac.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Action Chunking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyang Li, Zhiyuan Zhou, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Q-chunking, a simple yet effective recipe for improving
reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.
Our recipe is designed for the offline-to-online RL setting, where the goal is
to leverage an offline prior dataset to maximize the sample-efficiency of
online learning. Effective exploration and sample-efficient learning remain
central challenges in this setting, as it is not obvious how the offline data
should be utilized to acquire a good exploratory policy. Our key insight is
that action chunking, a technique popularized in imitation learning where
sequences of future actions are predicted rather than a single action at each
timestep, can be applied to temporal difference (TD)-based RL methods to
mitigate the exploration challenge. Q-chunking adopts action chunking by
directly running RL in a 'chunked' action space, enabling the agent to (1)
leverage temporally consistent behaviors from offline data for more effective
online exploration and (2) use unbiased $n$-step backups for more stable and
efficient TD learning. Our experimental results demonstrate that Q-chunking
exhibits strong offline performance and online sample efficiency, outperforming
prior best offline-to-online methods on a range of long-horizon, sparse-reward
manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving AEBS Validation Through Objective Intervention Classification
  Leveraging the Prediction Divergence Principle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Betschinske, Steven Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication at the 2025 IEEE
  International Automated Vehicle Validation Conference (IAVVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROS Help Desk: GenAI Powered, User-Centric Framework for ROS Error
  Diagnosis and Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavindie Katuwandeniya, Samith Rajapaksha Jayasekara Widhanapathirana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the robotics systems increasingly integrate into daily life, from smart
home assistants to the new-wave of industrial automation systems (Industry
4.0), there's an increasing need to bridge the gap between complex robotic
systems and everyday users. The Robot Operating System (ROS) is a flexible
framework often utilised in writing robot software, providing tools and
libraries for building complex robotic systems. However, ROS's distributed
architecture and technical messaging system create barriers for understanding
robot status and diagnosing errors. This gap can lead to extended maintenance
downtimes, as users with limited ROS knowledge may struggle to quickly diagnose
and resolve system issues. Moreover, this deficit in expertise often delays
proactive maintenance and troubleshooting, further increasing the frequency and
duration of system interruptions. ROS Help Desk provides intuitive error
explanations and debugging support, dynamically customized to users of varying
expertise levels. It features user-centric debugging tools that simplify error
diagnosis, implements proactive error detection capabilities to reduce
downtime, and integrates multimodal data processing for comprehensive system
state understanding across multi-sensor data (e.g., lidar, RGB). Testing
qualitatively and quantitatively with artificially induced errors demonstrates
the system's ability to proactively and accurately diagnose problems,
ultimately reducing maintenance time and fostering more effective human-robot
collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual Distortions and Autonomous Representation Learning in a
  Minimal Robotic System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Warutumo, Ciira wa Maina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents, particularly in the field of robotics, rely on sensory
information to perceive and navigate their environment. However, these sensory
inputs are often imperfect, leading to distortions in the agent's internal
representation of the world. This paper investigates the nature of these
perceptual distortions and how they influence autonomous representation
learning using a minimal robotic system. We utilize a simulated two-wheeled
robot equipped with distance sensors and a compass, operating within a simple
square environment. Through analysis of the robot's sensor data during random
exploration, we demonstrate how a distorted perceptual space emerges. Despite
these distortions, we identify emergent structures within the perceptual space
that correlate with the physical environment, revealing how the robot
autonomously learns a structured representation for navigation without explicit
spatial information. This work contributes to the understanding of embodied
cognition, minimal agency, and the role of perception in self-generated
navigation strategies in artificial life.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 authors, 23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Robustness: Learning Unknown Dynamic Load Adaptation for
  Quadruped Locomotion on Rough Terrain <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leixin Chang, Yuxuan Nai, Hua Chen, Liangjing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unknown dynamic load carrying is one important practical application for
quadruped robots. Such a problem is non-trivial, posing three major challenges
in quadruped locomotion control. First, how to model or represent the dynamics
of the load in a generic manner. Second, how to make the robot capture the
dynamics without any external sensing. Third, how to enable the robot to
interact with load handling the mutual effect and stabilizing the load. In this
work, we propose a general load modeling approach called load characteristics
modeling to capture the dynamics of the load. We integrate this proposed
modeling technique and leverage recent advances in Reinforcement Learning (RL)
based locomotion control to enable the robot to infer the dynamics of load
movement and interact with the load indirectly to stabilize it and realize the
sim-to-real deployment to verify its effectiveness in real scenarios. We
conduct extensive comparative simulation experiments to validate the
effectiveness and superiority of our proposed method. Results show that our
method outperforms other methods in sudden load resistance, load stabilizing
and locomotion with heavy load on rough terrain.
\href{https://leixinjonaschang.github.io/leggedloadadapt.github.io/}{Project
Page}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2025 IEEE International Conference on Robotics &
  Automation (ICRA). 8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Human-Robot Surgery for Mandibular Angle Split Osteotomy:
  Optical Tracking based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Han, Huanyu Tian, Tom Vercauteren, Da Liu, Changsheng Li, Xingguang Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mandibular Angle Split Osteotomy (MASO) is a significant procedure in oral
and maxillofacial surgery. Despite advances in technique and instrumentation,
its success still relies heavily on the surgeon's experience. In this work, a
human-robot collaborative system is proposed to perform MASO according to a
preoperative plan and under guidance of a surgeon. A task decomposition
methodology is used to divide the collaborative surgical procedure into three
subtasks: (1) positional control and (2) orientation control, both led by the
robot for precise alignment; and (3) force-control, managed by surgeon to
ensure safety. Additionally, to achieve patient tracking without the need for a
skull clamp, an optical tracking system (OTS) is utilized. Movement of the
patient mandibular is measured with an optical-based tracker mounted on a
dental occlusal splint. A registration method and Robot-OTS calibration method
are introduced to achieve reliable navigation within our framework. The
experiments of drilling were conducted on the realistic phantom model, which
demonstrated that the average error between the planned and actual drilling
points is 1.85mm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SURPRISE3D: A <span class="highlight-title">Dataset</span> for Spatial Understanding and Reasoning in Complex
  3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Huang, Ziwen Li, Hanlve Zhang, Runnan Chen, Xiao He, Yandong Guo, Wenping Wang, Tongliang Liu, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of language and 3D perception is critical for embodied AI and
robotic systems to perceive, understand, and interact with the physical world.
Spatial reasoning, a key capability for understanding spatial relationships
between objects, remains underexplored in current 3D vision-language research.
Existing datasets often mix semantic cues (e.g., object name) with spatial
context, leading models to rely on superficial shortcuts rather than genuinely
interpreting spatial relationships. To address this gap, we introduce
S\textsc{urprise}3D, a novel dataset designed to evaluate language-guided
spatial reasoning segmentation in complex 3D scenes. S\textsc{urprise}3D
consists of more than 200k vision language pairs across 900+ detailed indoor
scenes from ScanNet++ v2, including more than 2.8k unique object classes. The
dataset contains 89k+ human-annotated spatial queries deliberately crafted
without object name, thereby mitigating shortcut biases in spatial
understanding. These queries comprehensively cover various spatial reasoning
skills, such as relative position, narrative perspective, parametric
perspective, and absolute distance reasoning. Initial benchmarks demonstrate
significant challenges for current state-of-the-art expert 3D visual grounding
methods and 3D-LLMs, underscoring the necessity of our dataset and the
accompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite.
S\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially
aware AI, paving the way for effective embodied interaction and robotic
planning. The code and datasets can be found in
https://github.com/liziwennba/SUPRISE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End
  for Visual SLAM in Challenging Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh Nguyen Canh, Bao Nguyen Quoc, Haolan Zhang, Bupesh Rethinam Veeraiah, Xiem HoangVan, Nak Young Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Visual SLAM (vSLAM) is essential for autonomous systems operating in
real-world environments, where challenges such as dynamic objects, low texture,
and critically, varying illumination conditions often degrade performance.
Existing feature-based SLAM systems rely on fixed front-end parameters, making
them vulnerable to sudden lighting changes and unstable feature tracking. To
address these challenges, we propose ``IRAF-SLAM'', an Illumination-Robust and
Adaptive Feature-Culling front-end designed to enhance vSLAM resilience in
complex and challenging environments. Our approach introduces: (1) an image
enhancement scheme to preprocess and adjust image quality under varying
lighting conditions; (2) an adaptive feature extraction mechanism that
dynamically adjusts detection sensitivity based on image entropy, pixel
intensity, and gradient analysis; and (3) a feature culling strategy that
filters out unreliable feature points using density distribution analysis and a
lighting impact factor. Comprehensive evaluations on the TUM-VI and European
Robotics Challenge (EuRoC) datasets demonstrate that IRAF-SLAM significantly
reduces tracking failures and achieves superior trajectory accuracy compared to
state-of-the-art vSLAM methods under adverse illumination conditions. These
results highlight the effectiveness of adaptive front-end strategies in
improving vSLAM robustness without incurring significant computational
overhead. The implementation of IRAF-SLAM is publicly available at
https://thanhnguyencanh. github.io/IRAF-SLAM/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the European Conference on Mobile Robots 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the capabilities of LLMs for classifying and segmenting time series
  of fruit picking motions into primitive actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleni Konstantinidou, Nikolaos Kounalakis, Nikolaos Efstathopoulos, Dimitrios Papageorgiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their recent introduction to human society, Large Language Models
(LLMs) have significantly affected the way we tackle mental challenges in our
everyday lives. From optimizing our linguistic communication to assisting us in
making important decisions, LLMs, such as ChatGPT, are notably reducing our
cognitive load by gradually taking on an increasing share of our mental
activities. In the context of Learning by Demonstration (LbD), classifying and
segmenting complex motions into primitive actions, such as pushing, pulling,
twisting etc, is considered to be a key-step towards encoding a task. In this
work, we investigate the capabilities of LLMs to undertake this task,
considering a finite set of predefined primitive actions found in fruit picking
operations. By utilizing LLMs instead of simple supervised learning or analytic
methods, we aim at making the method easily applicable and deployable in a
real-life scenario. Three different fine-tuning approaches are investigated,
compared on datasets captured kinesthetically, using a UR10e robot, during a
fruit-picking scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a Late Breaking Results report and it will be presented
  through a poster at the 34th IEEE International Conference on Robot and Human
  Interactive Communication (ROMAN), 2025 at Eindhoven, the Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Surface Inspection via Operational Modal Analysis by a Swarm
  of Miniaturized Vibration-Sensing Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thiemen Siemensma, Niels de Boer, Bahar Haghighat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot swarms offer the potential to serve a variety of distributed sensing
applications. An interesting real-world application that stands to benefit
significantly from deployment of swarms is structural monitoring, where
traditional sensor networks face challenges in structural coverage due to their
static nature. This paper investigates the deployment of a swarm of
miniaturized vibration sensing robots to inspect and localize structural
damages on a surface section within a high-fidelity simulation environment. In
particular, we consider a 1 m x 1 m x 3 mm steel surface section and utilize
finite element analysis using Abaqus to obtain realistic structural vibration
data. The resulting vibration data is imported into the physics-based robotic
simulator Webots, where we simulate the dynamics of our surface inspecting
robot swarm. We employ (i) Gaussian process estimators to guide the robots'
exploration as they collect vibration samples across the surface and (ii)
operational modal analysis to detect structural damages by estimating and
comparing existing and intact structural vibration patterns. We analyze the
influence of exploration radii on estimation uncertainty and assess the
effectiveness of our method across 10 randomized scenarios, where the number,
locations, surface area, and depth of structural damages vary. Our simulation
studies validate the efficacy of our miniaturized robot swarm for
vibration-based structural inspection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation and Assessment of an Augmented Training Curriculum for
  Surgical Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Rota, Ke Fan, Elena De Momi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of high-level assistance algorithms in surgical robotics
training curricula may be beneficial in establishing a more comprehensive and
robust skillset for aspiring surgeons, improving their clinical performance as
a consequence. This work presents the development and validation of a
haptic-enhanced Virtual Reality simulator for surgical robotics training,
featuring 8 surgical tasks that the trainee can interact with thanks to the
embedded physics engine. This virtual simulated environment is augmented by the
introduction of high-level haptic interfaces for robotic assistance that aim at
re-directing the motion of the trainee's hands and wrists toward targets or
away from obstacles, and providing a quantitative performance score after the
execution of each training exercise.An experimental study shows that the
introduction of enhanced robotic assistance into a surgical robotics training
curriculum improves performance during the training process and, crucially,
promotes the transfer of the acquired skills to an unassisted surgical
scenario, like the clinical one.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Gaussian Mixture Models-based Anomaly Detection for
  under-constrained Cable-Driven Parallel Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julio Garrido, Javier Vales, Diego Silva-Muñiz, Enrique Riveiro, Pablo López-Matencio, Josué Rivera-Andrade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cable-Driven Parallel Robots (CDPRs) are increasingly used for load
manipulation tasks involving predefined toolpaths with intermediate stops. At
each stop, where the platform maintains a fixed pose and the motors keep the
cables under tension, the system must evaluate whether it is safe to proceed by
detecting anomalies that could compromise performance (e.g., wind gusts or
cable impacts). This paper investigates whether anomalies can be detected using
only motor torque data, without additional sensors. It introduces an adaptive,
unsupervised outlier detection algorithm based on Gaussian Mixture Models
(GMMs) to identify anomalies from torque signals. The method starts with a
brief calibration period, just a few seconds, during which a GMM is fit on
known anomaly-free data. Real-time torque measurements are then evaluated using
Mahalanobis distance from the GMM, with statistically derived thresholds
triggering anomaly flags. Model parameters are periodically updated using the
latest segments identified as anomaly-free to adapt to changing conditions.
Validation includes 14 long-duration test sessions simulating varied wind
intensities. The proposed method achieves a 100% true positive rate and 95.4%
average true negative rate, with 1-second detection latency. Comparative
evaluation against power threshold and non-adaptive GMM methods indicates
higher robustness to drift and environmental variation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiDTouch: A 3D Wearable Haptic Display for the Finger Pad 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Trinitatova, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The applications of fingertip haptic devices have spread to various fields
from revolutionizing virtual reality and medical training simulations to
facilitating remote robotic operations, proposing great potential for enhancing
user experiences, improving training outcomes, and new forms of interaction. In
this work, we present FiDTouch, a 3D wearable haptic device that delivers
cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin
stretch, and vibrotactile feedback. The application of a tiny inverted Delta
robot in the mechanism design allows providing accurate contact and fast
changing dynamic stimuli to the finger pad surface. The performance of the
developed display was evaluated in a two-stage user study of the perception of
static spatial contact stimuli and skin stretch stimuli generated on the finger
pad. The proposed display, by providing users with precise touch and force
stimuli, can enhance user immersion and efficiency in the fields of
human-computer and human-robot interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the IEEE World Haptics Conference 2025 (IEEE WHC 2025), 7
  pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conjugated Capabilities: Interrelations of Elementary Human Capabilities
  and Their Implication on Human-Machine Task Allocation and Capability Testing
  Procedures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Mandischer, Larissa Füller, Torsten Alles, Frank Flemisch, Lars Mikelsons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and automation capabilities are the foundation of every human-autonomy
interaction and interaction pattern. Therefore, machines need to understand the
capacity and performance of human doing, and adapt their own behavior,
accordingly. In this work, we address the concept of conjugated capabilities,
i.e. capabilities that are dependent or interrelated and between which effort
can be distributed. These may be used to overcome human limitations, by
shifting effort from a deficient to a conjugated capability with performative
resources. For example: A limited arm's reach may be compensated by tilting the
torso forward. We analyze the interrelation between elementary capabilities
within the IMBA standard to uncover potential conjugation, and show evidence in
data of post-rehabilitation patients. From the conjugated capabilities, within
the example application of stationary manufacturing, we create a network of
interrelations. With this graph, a manifold of potential uses is enabled. We
showcase the graph's usage in optimizing IMBA test design to accelerate data
recordings, and discuss implications of conjugated capabilities on task
allocation between the human and an autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was accepted by the IEEE International Conference on
  Systems, Man, and Cybernetics (SMC), Vienna, Austria, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pluri-perspectivism in Human-robot Co-creativity with Older Adults 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marianne Bossema, Rob Saunders, Aske Plaat, Somaya Ben Allouch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper explores pluriperspectivism as a core element of human
creative experience and its relevance to humanrobot cocreativity We propose a
layered fivedimensional model to guide the design of cocreative behaviors and
the analysis of interaction dynamics This model is based on literature and
results from an interview study we conducted with 10 visual artists and 8 arts
educators examining how pluriperspectivism supports creative practice The
findings of this study provide insight in how robots could enhance human
creativity through adaptive contextsensitive behavior demonstrating the
potential of pluriperspectivism This paper outlines future directions for
integrating pluriperspectivism with visionlanguage models VLMs to support
context sensitivity in cocreative robots
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCREP: Scene Coordinate Regression and Evidential Learning-based
  Perception-Aware Trajectory Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous flight in GPS denied indoor spaces requires trajectories that keep
visual localization error tightly bounded across varied missions. Whereas
visual inertial odometry (VIO) accumulates drift over time, scene coordinate
regression (SCR) yields drift-free, high accuracy absolute pose estimation. We
present a perception-aware framework that couples an evidential learning-based
SCR pose estimator with a receding horizon trajectory optimizer. The optimizer
steers the onboard camera toward pixels whose uncertainty predicts reliable
scene coordinates, while a fixed-lag smoother fuses the low rate SCR stream
with high rate IMU data to close the perception control loop in real time. In
simulation, our planner reduces translation (rotation) mean error by 54% / 15%
(40% / 31%) relative to yaw fixed and forward-looking baselines, respectively.
Moreover, hardware in the loop experiment validates the feasibility of our
proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Safe Autonomous Driving: A Real-Time Safeguarding Concept for
  Motion Planning Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Korbinian Moller, Rafael Neher, Marvin Seegert, Johannes Betz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the functional safety of motion planning modules in autonomous
vehicles remains a critical challenge, especially when dealing with complex or
learning-based software. Online verification has emerged as a promising
approach to monitor such systems at runtime, yet its integration into embedded
real-time environments remains limited. This work presents a safeguarding
concept for motion planning that extends prior approaches by introducing a time
safeguard. While existing methods focus on geometric and dynamic feasibility,
our approach additionally monitors the temporal consistency of planning outputs
to ensure timely system response. A prototypical implementation on a real-time
operating system evaluates trajectory candidates using constraint-based
feasibility checks and cost-based plausibility metrics. Preliminary results
show that the safeguarding module operates within real-time bounds and
effectively detects unsafe trajectories. However, the full integration of the
time safeguard logic and fallback strategies is ongoing. This study contributes
a modular and extensible framework for runtime trajectory verification and
highlights key aspects for deployment on automotive-grade hardware. Future work
includes completing the safeguarding logic and validating its effectiveness
through hardware-in-the-loop simulations and vehicle-based testing. The code is
available at: https://github.com/TUM-AVS/motion-planning-supervisor
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, submitted to the IEEE ICVES 2025, Coventry, UK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication
  Framework for Dynamic Target Search of Multi-Agent in Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengrui Liu, Yi Feng, Qilong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster
response, exploration, and reconnaissance. However, dynamic and unknown
environments pose significant challenges due to target unpredictability and
environmental uncertainty. To tackle these issues, we propose PILOC, a
framework that operates without global prior knowledge, leveraging local
perception and communication. It introduces a pheromone inverse guidance
mechanism to enable efficient coordination and dynamic target localization.
PILOC promotes decentralized cooperation through local communication,
significantly reducing reliance on global channels. Unlike conventional
heuristics, the pheromone mechanism is embedded into the observation space of
Deep Reinforcement Learning (DRL), supporting indirect agent coordination based
on environmental cues. We further integrate this strategy into a DRL-based
multi-agent architecture and conduct extensive experiments. Results show that
combining local communication with pheromone-based guidance significantly
boosts search efficiency, adaptability, and system robustness. Compared to
existing methods, PILOC performs better under dynamic and
communication-constrained scenarios, offering promising directions for future
MASAR applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven Kinematic Modeling in Soft Robots: System Identification and
  Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanhong Jiang, Dylan Shah, Hsin-Jung Yang, Soumik Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise kinematic modeling is critical in calibration and controller design
for soft robots, yet remains a challenging issue due to their highly nonlinear
and complex behaviors. To tackle the issue, numerous data-driven machine
learning approaches have been proposed for modeling nonlinear dynamics.
However, these models suffer from prediction uncertainty that can negatively
affect modeling accuracy, and uncertainty quantification for kinematic modeling
in soft robots is underexplored. In this work, using limited simulation and
real-world data, we first investigate multiple linear and nonlinear machine
learning models commonly used for kinematic modeling of soft robots. The
results reveal that nonlinear ensemble methods exhibit the most robust
generalization performance. We then develop a conformal kinematic modeling
framework for soft robots by utilizing split conformal prediction to quantify
predictive position uncertainty, ensuring distribution-free prediction
intervals with a theoretical guarantee.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; 6 figures; accepted at the 5th Modeling, Estimation and
  Control Conference (MECC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Yin, Weishuai Zeng, Ke Fan, Zirui Wang, Qiang Zhang, Zheng Tian, Jingbo Wang, Jiangmiao Pang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots must achieve diverse, robust, and generalizable whole-body
control to operate effectively in complex, human-centric environments. However,
existing methods, particularly those based on teacher-student frameworks often
suffer from a loss of motion diversity during policy distillation and exhibit
limited generalization to unseen behaviors. In this work, we present
UniTracker, a simplified yet powerful framework that integrates a Conditional
Variational Autoencoder (CVAE) into the student policy to explicitly model the
latent diversity of human motion. By leveraging a learned CVAE prior, our
method enables the student to retain expressive motion characteristics while
improving robustness and adaptability under partial observations. The result is
a single policy capable of tracking a wide spectrum of whole-body motions with
high fidelity and stability. Comprehensive experiments in both simulation and
real-world deployments demonstrate that UniTracker significantly outperforms
MLP-based DAgger baselines in motion quality, generalization to unseen
references, and deployment robustness, offering a practical and scalable
solution for expressive humanoid control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relative Pose Estimation for Nonholonomic Robot Formation with UWB-IO
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunrui Ze, Wei Wang, Shuoyu Yue, Guibin Sun, Kexin Liu, Jinhu Lü
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article studies the problem of distributed formation control for
multiple robots by using onboard ultra wide band (UWB) distance and inertial
odometer (IO) measurements.
  Although this problem has been widely studied, a fundamental limitation of
most works is that they require each robot's pose and sensor measurements are
expressed in a common reference frame.
  However, it is inapplicable for nonholonomic robot formations due to the
practical difficulty of aligning IO measurements of individual robot in a
common frame.
  To address this problem, firstly, a concurrent-learning based estimator is
firstly proposed to achieve relative localization between neighboring robots in
a local frame.
  Different from most relative localization methods in a global frame, both
relative position and orientation in a local frame are estimated with only UWB
ranging and IO
  measurements.
  Secondly, to deal with information loss caused by directed communication
topology, a cooperative localization algorithm is introduced to estimate the
relative pose to the leader robot.
  Thirdly, based on the theoretical results on relative pose estimation, a
distributed formation tracking controller is proposed for nonholonomic robots.
  Both 3D and 2D real-world experiments conducted on aerial robots and grounded
robots are provided to demonstrate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reference Free Platform Adaptive Locomotion for Quadrupedal Robots using
  a Dynamics Conditioned Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Rytz, Suyoung Choi, Wanming Yu, Wolfgang Merkt, Jemin Hwangbo, Ioannis Havoutis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents Platform Adaptive Locomotion (PAL), a unified control
method for quadrupedal robots with different morphologies and dynamics. We
leverage deep reinforcement learning to train a single locomotion policy on
procedurally generated robots. The policy maps proprioceptive robot state
information and base velocity commands into desired joint actuation targets,
which are conditioned using a latent embedding of the temporally local system
dynamics. We explore two conditioning strategies - one using a GRU-based
dynamics encoder and another using a morphology-based property estimator - and
show that morphology-aware conditioning outperforms temporal dynamics encoding
regarding velocity task tracking for our hardware test on ANYmal C. Our results
demonstrate that both approaches achieve robust zero-shot transfer across
multiple unseen simulated quadrupeds. Furthermore, we demonstrate the need for
careful robot reference modelling during training: exposing the policy to a
diverse set of robot morphologies and dynamics leads to improved
generalization, reducing the velocity tracking error by up to 30% compared to
the baseline method. Despite PAL not surpassing the best-performing
reference-free controller in all cases, our analysis uncovers critical design
choices and informs improvements to the state of the art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Machine Learning for Estimating Workload: Considering
  Unknown Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Bhagat Smith, Julie A. Adams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful human-robot teaming will require robots to adapt autonomously to a
human teammate's internal state, where a critical element of such adaptation is
the ability to estimate the human's workload in unknown situations. Existing
workload models use machine learning to model the relationship between
physiological signals and workload. These methods often struggle to generalize
to unknown tasks, as the relative importance of various physiological signals
change significantly between tasks. Many of these changes constitute a
meaningful shift in the data's distribution, which violates a core assumption
made by the underlying machine learning approach. A survey of machine learning
techniques designed to overcome these challenges is presented, where common
techniques are evaluated using three criteria: portability, model complexity,
and adaptability. These criteria are used to analyze each technique's
applicability to estimating workload during unknown tasks in dynamic
environments and guide future empirical experimentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariant IMU Preintegration with Biases: a Galilean Group Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05548v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05548v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Delama, Alessandro Fornasier, Robert Mahony, Stephan Weiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter proposes a new approach for Inertial Measurement Unit (IMU)
preintegration, a fundamental building block that can be leveraged in different
optimization-based Inertial Navigation System (INS) localization solutions.
Inspired by recent advances in equivariant theory applied to biased INSs, we
derive a discrete-time formulation of the IMU preintegration on
${\mathbf{Gal}(3) \ltimes \mathfrak{gal}(3)}$, the left-trivialization of the
tangent group of the Galilean group $\mathbf{Gal}(3)$. We define a novel
preintegration error that geometrically couples the navigation states and the
bias leading to lower linearization error. Our method improves in consistency
compared to existing preintegration approaches which treat IMU biases as a
separate state-space. Extensive validation against state-of-the-art methods,
both in simulation and with real-world IMU data, implementation in the Lie++
library, and open-source code are provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VOTE: Vision-Language-Action Optimization with Trajectory Ensemble
  Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyi Lin, Amir Taherin, Arash Akbari, Arman Akbari, Lei Lu, Guangyu Chen, Taskin Padir, Xiaomeng Yang, Weiwei Chen, Yiqian Li, Xue Lin, David Kaeli, Pu Zhao, Yanzhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35x faster inference and 145 Hz throughput.
All the details and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunHOI: Annotation-Free 3D Hand-Object Interaction Generation via
  Functional Text Guidanc 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Tian, Xueyu Sun, Haoyuan He, Linji Hao, Ning Ding, Caigui Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-object interaction(HOI) is the fundamental link between human and
environment, yet its dexterous and complex pose significantly challenges for
gesture control. Despite significant advances in AI and robotics, enabling
machines to understand and simulate hand-object interactions, capturing the
semantics of functional grasping tasks remains a considerable challenge. While
previous work can generate stable and correct 3D grasps, they are still far
from achieving functional grasps due to unconsidered grasp semantics. To
address this challenge, we propose an innovative two-stage framework,
Functional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven by
functional text. This framework consists of a text-guided 3D model generator,
Functional Grasp Generator (FGG), and a pose optimization strategy, Functional
Grasp Refiner (FGR). FGG generates 3D models of hands and objects based on text
input, while FGR fine-tunes the poses using Object Pose Approximator and energy
functions to ensure the relative position between the hand and object aligns
with human intent and remains physically plausible. Extensive experiments
demonstrate that our approach achieves precise and high-quality HOI generation
without requiring additional 3D annotation data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collective Bayesian Decision-Making in a Swarm of Miniaturized Robots
  for Surface Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thiemen Siemensma, Darren Chiu, Sneha Ramshanker, Radhika Nagpal, Bahar Haghighat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot swarms can effectively serve a variety of sensing and inspection
applications. Certain inspection tasks require a binary classification
decision. This work presents an experimental setup for a surface inspection
task based on vibration sensing and studies a Bayesian two-outcome
decision-making algorithm in a swarm of miniaturized wheeled robots. The robots
are tasked with individually inspecting and collectively classifying a 1mx1m
tiled surface consisting of vibrating and non-vibrating tiles based on the
majority type of tiles. The robots sense vibrations using onboard IMUs and
perform collision avoidance using a set of IR sensors. We develop a simulation
and optimization framework leveraging the Webots robotic simulator and a
Particle Swarm Optimization (PSO) method. We consider two existing information
sharing strategies and propose a new one that allows the swarm to rapidly reach
accurate classification decisions. We first find optimal parameters that allow
efficient sampling in simulation and then evaluate our proposed strategy
against the two existing ones using 100 randomized simulation and 10 real
experiments. We find that our proposed method compels the swarm to make
decisions at an accelerated rate, with an improvement of up to 20.52% in mean
decision time at only 0.78% loss in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and
  Wall Climbing <span class="chip">IROS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keita Yoneda, Kento Kawaharazuka, Temma Suzuki, Takahiro Hattori, Kei Okada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, advancements in hardware have enabled quadruped robots to
operate with high power and speed, while robust locomotion control using
reinforcement learning (RL) has also been realized. As a result, expectations
are rising for the automation of tasks such as material transport and
exploration in unknown environments. However, autonomous locomotion in rough
terrains with significant height variations requires vertical movement, and
robots capable of performing such movements stably, along with their control
methods, have not yet been fully established. In this study, we developed the
quadruped robot KLEIYN, which features a waist joint, and aimed to expand
quadruped locomotion by enabling chimney climbing through RL. To facilitate the
learning of vertical motion, we introduced Contact-Guided Curriculum Learning
(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to
1000 mm in width at an average speed of 150 mm/s, 50 times faster than
conventional robots. Furthermore, we demonstrated that the introduction of a
waist joint improves climbing performance, particularly enhancing tracking
ability on narrow walls.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IROS2025, website -
  https://keitayoneda.github.io/kleiyn-chimney-climbing/, YouTube -
  https://www.youtube.com/watch?v=cLfUhyNFOeY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a cognitive architecture to enable natural language interaction
  in co-constructive task learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Scheibl, Birte Richter, Alissa Müller, Michael Beetz, Britta Wrede
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the question, which characteristics a cognitive
architecture must have to leverage the benefits of natural language in
Co-Constructive Task Learning (CCTL). To provide context, we first discuss
Interactive Task Learning (ITL), the mechanisms of the human memory system, and
the significance of natural language and multi-modality. Next, we examine the
current state of cognitive architectures, analyzing their capabilities to
inform a concept of CCTL grounded in multiple sources. We then integrate
insights from various research domains to develop a unified framework. Finally,
we conclude by identifying the remaining challenges and requirements necessary
to achieve CCTL in Human-Robot Interaction (HRI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, accepted at: IEEE RO-MAN 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Vision-Language Planning for Multi-Step Humanoid
  Manipulation <span class="chip">RSS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22827v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22827v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Schakkal, Ben Zandonati, Zhutian Yang, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling humanoid robots to reliably execute complex multi-step manipulation
tasks is crucial for their effective deployment in industrial and household
environments. This paper presents a hierarchical planning and control framework
designed to achieve reliable multi-step humanoid manipulation. The proposed
system comprises three layers: (1) a low-level RL-based controller responsible
for tracking whole-body motion targets; (2) a mid-level set of skill policies
trained via imitation learning that produce motion targets for different steps
of a task; and (3) a high-level vision-language planning module that determines
which skills should be executed and also monitors their completion in real-time
using pretrained vision-language models (VLMs). Experimental validation is
performed on a Unitree G1 humanoid robot executing a non-prehensile
pick-and-place task. Over 40 real-world trials, the hierarchical system
achieved a 73% success rate in completing the full manipulation sequence. These
experiments confirm the feasibility of the proposed hierarchical system,
highlighting the benefits of VLM-based skill planning and monitoring for
multi-step manipulation scenarios. See https://vlp-humanoid.github.io/ for
video demonstrations of the policy rollout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the RSS 2025 Workshop on Robot Planning in the Era of
  Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Assignment and Exploration Optimization for Low Altitude UAV Rescue
  via Generative AI Enhanced Multi-agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Tang, Qian Chen, Wenjie Weng, Chao Jin, Zhang Liu, Jiacheng Wang, Geng Sun, Xiaohuan Li, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of emerging uncrewed aerial vehicles (UAVs) with artificial
intelligence (AI) and ground-embedded robots (GERs) has transformed emergency
rescue operations in unknown environments. However, the high computational
demands often exceed a single UAV's capacity, making it difficult to
continuously provide stable high-level services. To address this, this paper
proposes a cooperation framework involving UAVs, GERs, and airships. The
framework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship
(U2A) links, offering computing services for offloaded tasks. Specifically, we
formulate the multi-objective problem of task assignment and exploration as a
dynamic long-term optimization problem aiming to minimize task completion time
and energy use while ensuring stability. Using Lyapunov optimization, we
transform it into a per-slot deterministic problem and propose HG-MADDPG, which
combines the Hungarian algorithm with a GDM-based multi-agent deep
deterministic policy gradient. Simulations demonstrate significant improvements
in offloading efficiency, latency, and system stability over baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MapNav: A Novel Memory Representation via Annotated Semantic Maps for
  Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13451v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13451v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng Zhang, Xiaoshuai Hao, Qinwen Xu, Qiang Zhang, Xinyao Zhang, Pengwei Wang, Jing Zhang, Zhongyuan Wang, Shanghang Zhang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring
agents to navigate diverse and unseen environments while following natural
language instructions. Traditional approaches rely heavily on historical
observations as spatio-temporal contexts for decision making, leading to
significant storage and computational overhead. In this paper, we introduce
MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map
(ASM) to replace historical frames. Specifically, our approach constructs a
top-down semantic map at the start of each episode and update it at each
timestep, allowing for precise object mapping and structured navigation
information. Then, we enhance this map with explicit textual labels for key
regions, transforming abstract semantics into clear navigation cues and
generate our ASM. MapNav agent using the constructed ASM as input, and use the
powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments
demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both
simulated and real-world environments, validating the effectiveness of our
method. Moreover, we will release our ASM generation source code and dataset to
ensure reproducibility, contributing valuable resources to the field. We
believe that our proposed MapNav can be used as a new memory representation
method in VLN, paving the way for future research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hallucinating 360°: Panoramic Street-View Generation via Local
  Scenes <span class="highlight-title">Diffusion</span> and Probabilistic <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Teng, Kai Luo, Sheng Wu, Siyu Li, Pujun Guo, Jiale Wei, Kunyu Peng, Jiaming Zhang, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be publicly available at
  https://github.com/Bryant-Teng/Percep360</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Pathfinding Under Team-Connected Communication Constraint
  via Adaptive Path Expansion and Dynamic Leading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02770v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02770v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang-Dung Bui, Erion Plaku, Gregoy J. Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel planning framework to handle a multi-agent
pathfinding problem under team-connected communication constraint, where all
agents must have a connected communication channel to the rest of the team
during their entire movements. Standard multi-agent path finding approaches
(e.g., priority-based search) have potential in this domain but fail when
neighboring configurations at start and goal differ. Their single-expansion
approach -- computing each agent's path from the start to the goal in just a
single expansion -- cannot reliably handle planning under communication
constraints for agents as their neighbors change during navigating. Similarly,
leader-follower approaches (e.g., platooning) are effective at maintaining team
communication, but fixing the leader at the outset of planning can cause
planning to become stuck in dense-clutter environments, limiting their
practical utility. To overcome this limitation, we propose a novel two-level
multi-agent pathfinding framework that integrates two techniques: adaptive path
expansion to expand agent paths to their goals in multiple stages; and dynamic
leading technique that enables the reselection of the leading agent during each
agent path expansion whenever progress cannot be made. Simulation experiments
show the efficiency of our planners, which can handle up to 25 agents across
five environment types under a limited communication range constraint and up to
11-12 agents on three environment types under line-of-sight communication
constraint, exceeding 90% success-rate where baselines routinely fail.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-09T00:00:00Z">2025-07-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Statistics - Machine Learning <span class="chip" style="font-size: 60%">38</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Double Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Polson, Vadim Sokolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Double descent is a phenomenon of over-parameterized statistical models. Our
goal is to view double descent from a Bayesian perspective. Over-parameterized
models such as deep neural networks have an interesting re-descending property
in their risk characteristics. This is a recent phenomenon in machine learning
and has been the subject of many studies. As the complexity of the model
increases, there is a U-shaped region corresponding to the traditional
bias-variance trade-off, but then as the number of parameters equals the number
of observations and the model becomes one of interpolation, the risk can become
infinite and then, in the over-parameterized region, it re-descends -- the
double descent effect. We show that this has a natural Bayesian interpretation.
Moreover, we show that it is not in conflict with the traditional Occam's razor
that Bayesian models possess, in that they tend to prefer simpler models when
possible. We illustrate the approach with an example of Bayesian model
selection in neural networks. Finally, we conclude with directions for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Foote, Danny Krizanc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Along with accurate prediction, understanding the contribution of each
feature to the making of the prediction, i.e., the importance of the feature,
is a desirable and arguably necessary component of a machine learning model.
For a complex model such as a random forest, such importances are not innate --
as they are, e.g., with linear regression. Efficient methods have been created
to provide such capabilities, with one of the most popular among them being
permutation feature importance due to its efficiency, model-agnostic nature,
and perceived intuitiveness. However, permutation feature importance has been
shown to be misleading in the presence of dependent features as a result of the
creation of unrealistic observations when permuting the dependent features. In
this work, we develop TRIP (Test for Reliable Interpretation via Permutation),
a test requiring minimal assumptions that is able to detect unreliable
permutation feature importance scores that are the result of model
extrapolation. To build on this, we demonstrate how the test can be
complemented in order to allow its use in high dimensional settings. Through
testing on simulated data and applications, our results show that the test can
be used to reliably detect when permutation feature importance scores are
unreliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Workshop on Explainable Artificial Intelligence (XAI)
  at IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Asymptotic Analysis of Online Local Private Learning with SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Shi, Jinhan Xie, Bei Jiang, Linglong Kong, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially Private Stochastic Gradient Descent (DP-SGD) has been widely
used for solving optimization problems with privacy guarantees in machine
learning and statistics. Despite this, a systematic non-asymptotic convergence
analysis for DP-SGD, particularly in the context of online problems and local
differential privacy (LDP) models, remains largely elusive. Existing
non-asymptotic analyses have focused on non-private optimization methods, and
hence are not applicable to privacy-preserving optimization problems. This work
initiates the analysis to bridge this gap and opens the door to non-asymptotic
convergence analysis of private optimization problems. A general framework is
investigated for the online LDP model in stochastic optimization problems. We
assume that sensitive information from individuals is collected sequentially
and aim to estimate, in real-time, a static parameter that pertains to the
population of interest. Most importantly, we conduct a comprehensive
non-asymptotic convergence analysis of the proposed estimators in finite-sample
situations, which gives their users practical guidelines regarding the effect
of various hyperparameters, such as step size, parameter dimensions, and
privacy budgets, on convergence rates. Our proposed estimators are validated in
the theoretical and practical realms by rigorous mathematical derivations and
carefully constructed numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topological Machine Learning with Unreduced Persistence Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicole Abreu, Parker B. Edwards, Francis Motta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised machine learning pipelines trained on features derived from
persistent homology have been experimentally observed to ignore much of the
information contained in a persistence diagram. Computing persistence diagrams
is often the most computationally demanding step in such a pipeline, however.
To explore this, we introduce several methods to generate topological feature
vectors from unreduced boundary matrices. We compared the performance of
pipelines trained on vectorizations of unreduced PDs to vectorizations of
fully-reduced PDs across several data and task types. Our results indicate that
models trained on PDs built from unreduced diagrams can perform on par and even
outperform those trained on fully-reduced diagrams on some tasks. This
observation suggests that machine learning pipelines which incorporate
topology-based features may benefit in terms of computational cost and
performance by utilizing information contained in unreduced boundary matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 figures, 2 tables, 8 pages(without appendix and references)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Re-Identification, Attribute Inference, and Data Reconstruction
  Risks in Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Jamie Hayes, Borja Balle, Flavio du Pin Calmon, Jean Louis Raisaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Off-Policy Evaluation Under Nonignorable Missing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Yang Xu, Wenbin Lu, Rui Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-Policy Evaluation (OPE) aims to estimate the value of a target policy
using offline data collected from potentially different policies. In real-world
applications, however, logged data often suffers from missingness. While OPE
has been extensively studied in the literature, a theoretical understanding of
how missing data affects OPE results remains unclear. In this paper, we
investigate OPE in the presence of monotone missingness and theoretically
demonstrate that the value estimates remain unbiased under ignorable
missingness but can be biased under nonignorable (informative) missingness. To
retain the consistency of value estimation, we propose an inverse probability
weighted value estimator and conduct statistical inference to quantify the
uncertainty of the estimates. Through a series of numerical experiments, we
empirically demonstrate that our proposed estimator yields a more reliable
value inference under missing data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DICE: Data Influence Cascade in Decentralized Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtian Zhu, Wenhao Li, Can Wang, Fengxiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a poster at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution-free inference for LightGBM and GLM with Tweedie loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alokesh Manna, Aditya Vikram Sett, Dipak K. Dey, Yuwen Gu, Elizabeth D. Schifano, Jichao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction uncertainty quantification is a key research topic in recent years
scientific and business problems. In insurance industries
(\cite{parodi2023pricing}), assessing the range of possible claim costs for
individual drivers improves premium pricing accuracy. It also enables insurers
to manage risk more effectively by accounting for uncertainty in accident
likelihood and severity. In the presence of covariates, a variety of
regression-type models are often used for modeling insurance claims, ranging
from relatively simple generalized linear models (GLMs) to regularized GLMs to
gradient boosting models (GBMs). Conformal predictive inference has arisen as a
popular distribution-free approach for quantifying predictive uncertainty under
relatively weak assumptions of exchangeability, and has been well studied under
the classic linear regression setting. In this work, we propose new
non-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized
Tweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal
prediction performance with these non-conformity measures in insurance claims
data. Our simulation results favor the use of locally weighted Pearson
residuals for LightGBM over other methods considered, as the resulting
intervals maintained the nominal coverage with the smallest average width.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Prediction for Long-Tailed Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiffany Ding, Jean-Baptiste Fermanian, Joseph Salmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world classification problems, such as plant identification, have
extremely long-tailed class distributions. In order for prediction sets to be
useful in such settings, they should (i) provide good class-conditional
coverage, ensuring that rare classes are not systematically omitted from the
prediction sets, and (ii) be a reasonable size, allowing users to easily verify
candidate labels. Unfortunately, existing conformal prediction methods, when
applied to the long-tailed setting, force practitioners to make a binary choice
between small sets with poor class-conditional coverage or sets with very good
class-conditional coverage but that are extremely large. We propose methods
with guaranteed marginal coverage that smoothly trade off between set size and
class-conditional coverage. First, we propose a conformal score function,
prevalence-adjusted softmax, that targets a relaxed notion of class-conditional
coverage called macro-coverage. Second, we propose a label-weighted conformal
prediction method that allows us to interpolate between marginal and
class-conditional conformal prediction. We demonstrate our methods on Pl@ntNet
and iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive collaboration for online personalized distributed learning with
  heterogeneous clients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Philippenko, Batiste Le Bars, Kevin Scaman, Laurent Massoulié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of online personalized decentralized learning with $N$
statistically heterogeneous clients collaborating to accelerate local training.
An important challenge in this setting is to select relevant collaborators to
reduce gradient variance while mitigating the introduced bias. To tackle this,
we introduce a gradient-based collaboration criterion, allowing each client to
dynamically select peers with similar gradients during the optimization
process. Our criterion is motivated by a refined and more general theoretical
analysis of the All-for-one algorithm, proved to be optimal in Even et al.
(2022) for an oracle collaboration scheme. We derive excess loss upper-bounds
for smooth objective functions, being either strongly convex, non-convex, or
satisfying the Polyak-Lojasiewicz condition; our analysis reveals that the
algorithm acts as a variance reduction method where the speed-up depends on a
sufficient variance. We put forward two collaboration methods instantiating the
proposed general schema; and we show that one variant preserves the optimality
of All-for-one. We validate our results with experiments on synthetic and real
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise
  Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Andreas Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis, University of Cambridge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mutual Information Free Topological Generalization Bounds via Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Tuci, Lennart Bastian, Benjamin Dupuis, Nassir Navab, Tolga Birdal, Umut Şimşekli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mathematical artificial data for operator learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Wu, Benzhuo Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ stCEG: An R Package for Modelling Events over Spatial Areas Using Chain
  Event Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hollie Calley, Daniel Williamson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  stCEG is an R package which allows a user to fully specify a Chain Event
Graph (CEG) model from data and to produce interactive plots. It includes
functions for the user to visualise spatial variables they wish to include in
the model. There is also a web-based graphical user interface (GUI) provided,
increasing ease of use for those without knowledge of R. We demonstrate stCEG
using a dataset of homicides in London, which is included in the package. stCEG
is the first software package for CEGs that allows for full model
customisation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 20 figures. Submitted to the Journal of Statistical
  Software</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class conditional conformal prediction for multiple inputs by p-value
  aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Fermanian, Mohamed Hebiri, Joseph Salmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction methods are statistical tools designed to quantify
uncertainty and generate predictive sets with guaranteed coverage
probabilities. This work introduces an innovative refinement to these methods
for classification tasks, specifically tailored for scenarios where multiple
observations (multi-inputs) of a single instance are available at prediction
time. Our approach is particularly motivated by applications in citizen
science, where multiple images of the same plant or animal are captured by
individuals. Our method integrates the information from each observation into
conformal prediction, enabling a reduction in the size of the predicted label
set while preserving the required class-conditional coverage guarantee. The
approach is based on the aggregation of conformal p-values computed from each
observation of a multi-input. By exploiting the exact distribution of these
p-values, we propose a general aggregation framework using an abstract scoring
function, encompassing many classical statistical tools. Knowledge of this
distribution also enables refined versions of standard strategies, such as
majority voting. We evaluate our method on simulated and real data, with a
particular focus on Pl@ntNet, a prominent citizen science platform that
facilitates the collection and identification of plant species through
user-submitted images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Gaussian Processes under Monotonicity Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhang, Jasper M. Everink, Jakob Sauer Jørgensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes (GPs) are widely used as surrogate models for complicated
functions in scientific and engineering applications. In many cases, prior
knowledge about the function to be approximated, such as monotonicity, is
available and can be leveraged to improve model fidelity. Incorporating such
constraints into GP models enhances predictive accuracy and reduces
uncertainty, but remains a computationally challenging task for
high-dimensional problems. In this work, we present a novel virtual point-based
framework for building constrained GP models under monotonicity constraints,
based on regularized linear randomize-then-optimize (RLRTO), which enables
efficient sampling from a constrained posterior distribution by means of
solving randomized optimization problems. We also enhance two existing virtual
point-based approaches by replacing Gibbs sampling with the No U-Turn Sampler
(NUTS) for improved efficiency. A Python implementation of these methods is
provided and can be easily applied to a wide range of problems. This
implementation is then used to validate the approaches on approximating a range
of synthetic functions, demonstrating comparable predictive performance between
all considered methods and significant improvements in computational efficiency
with the two NUTS methods and especially with the RLRTO method. The framework
is further applied to construct surrogate models for systems of differential
equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-asymptotic confidence regions on RKHS. The Paley-Wiener and standard
  Sobolev space cases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabrice Gamboa, Olivier Roustant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of constructing a global, probabilistic, and
non-asymptotic confidence region for an unknown function observed on a random
design. The unknown function is assumed to lie in a reproducing kernel Hilbert
space (RKHS). We show that this construction can be reduced to accurately
estimating the RKHS norm of the unknown function. Our analysis primarily
focuses both on the Paley-Wiener and on the standard Sobolev space settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-parametric Functional Classification via Path Signatures Logistic
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Zeng, Siyuan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Path Signatures Logistic Regression (PSLR), a semi-parametric
framework for classifying vector-valued functional data with scalar covariates.
Classical functional logistic regression models rely on linear assumptions and
fixed basis expansions, which limit flexibility and degrade performance under
irregular sampling. PSLR overcomes these issues by leveraging truncated path
signatures to construct a finite-dimensional, basis-free representation that
captures nonlinear and cross-channel dependencies. By embedding trajectories as
time-augmented paths, PSLR extracts stable, geometry-aware features that are
robust to sampling irregularity without requiring a common time grid, while
still preserving subject-specific timing patterns. We establish theoretical
guarantees for the existence and consistent estimation of the optimal
truncation order, along with non-asymptotic risk bounds. Experiments on
synthetic and real-world datasets show that PSLR outperforms traditional
functional classifiers in accuracy, robustness, and interpretability,
particularly under non-uniform sampling schemes. Our results highlight the
practical and theoretical benefits of integrating rough path theory into modern
functional data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced <span class="highlight-title">Dataset</span>s
  with Differential Privacy with HAM10000 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobo Huang, Fang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and
  Information-Theoretic Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyi Dong, Zixuan Liu, Yongyi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the hardness of unsupervised domain adaptation (UDA) under
covariate shift. We model the uncertainty that the learner faces by a
distribution $\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a
UDA class -- where $(p, q)$ is the source -- target distribution pair and $f$
is the classifier. We define the performance of a learner as the overall target
domain risk, averaged over the randomness of the ground-truth triple. This
formulation couples the source distribution, the target distribution and the
classifier in the ground truth, and deviates from the classical worst-case
analyses, which pessimistically emphasize the impact of hard but rare UDA
instances. In this formulation, we precisely characterize the optimal learner.
The performance of the optimal learner then allows us to define the learning
difficulty for the UDA class and for the observed sample. To quantify this
difficulty, we introduce an information-theoretic quantity -- Posterior Target
Label Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the
sample , which capture the uncertainty in the prediction for the target domain.
Briefly, PTLU is the entropy of the predicted label in the target domain under
the posterior distribution of ground-truth classifier given the observed source
and target samples. By proving that such a quantity serves to lower-bound the
risk of any learner, we suggest that these quantities can be used as proxies
for evaluating the hardness of UDA learning. We provide several examples to
demonstrate the advantage of PTLU, relative to the existing measures, in
evaluating the difficulty of UDA learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 4th Conference on Lifelong Learning Agents (CoLLAs
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Bayesian Learning for Spatiotemporal Mechanistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.06528v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.06528v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudipto Banerjee, Xiang Chen, Ian Frankenburg, Daniel Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an approach for Bayesian learning of spatiotemporal dynamical
mechanistic models. Such learning consists of statistical emulation of the
mechanistic system that can efficiently interpolate the output of the system
from arbitrary inputs. The emulated learner can then be used to train the
system from noisy data achieved by melding information from observed data with
the emulated mechanistic system. This joint melding of mechanistic systems
employ hierarchical state-space models with Gaussian process regression.
Assuming the dynamical system is controlled by a finite collection of inputs,
Gaussian process regression learns the effect of these parameters through a
number of training runs, driving the stochastic innovations of the
spatiotemporal state-space component. This enables efficient modeling of the
dynamics over space and time. This article details exact inference with
analytically accessible posterior distributions in hierarchical matrix-variate
Normal and Wishart models in designing the emulator. This step obviates
expensive iterative algorithms such as Markov chain Monte Carlo or variational
approximations. We also show how emulation is applicable to large-scale
emulation by designing a dynamic Bayesian transfer learning framework.
Inference on mechanistic model parameters proceeds using Markov chain Monte
Carlo as a post-emulation step using the emulator as a regression component. We
demonstrate this framework through solving inverse problems arising in the
analysis of ordinary and partial nonlinear differential equations and, in
addition, to a black-box computer model generating spatiotemporal dynamics
across a graphical model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Estimators for Structured Generalized Linear Models via
  Approximate Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14507v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14507v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Zhang, Hong Chang Ji, Ramji Venkataramanan, Marco Mondelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of parameter estimation in a high-dimensional
generalized linear model. Spectral methods obtained via the principal
eigenvector of a suitable data-dependent matrix provide a simple yet
surprisingly effective solution. However, despite their wide use, a rigorous
performance characterization, as well as a principled way to preprocess the
data, are available only for unstructured (i.i.d.\ Gaussian and Haar
orthogonal) designs. In contrast, real-world data matrices are highly
structured and exhibit non-trivial correlations. To address the problem, we
consider correlated Gaussian designs capturing the anisotropic nature of the
features via a covariance matrix $\Sigma$. Our main result is a precise
asymptotic characterization of the performance of spectral estimators. This
allows us to identify the optimal preprocessing that minimizes the number of
samples needed for parameter estimation. Surprisingly, such preprocessing is
universal across a broad set of designs, which partly addresses a conjecture on
optimal spectral estimators for rotationally invariant models. Our principled
approach vastly improves upon previous heuristic methods, including for designs
common in computational imaging and genetics. The proposed methodology, based
on approximate message passing, is broadly applicable and opens the way to the
precise characterization of spiked matrices and of the corresponding spectral
methods in a variety of settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Gradient Normalization and Independent Sampling for
  (Stochastic) Generalized-Smooth Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Yang, Erin Tripp, Yifan Sun, Shaofeng Zou, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that many nonconvex machine learning problems
satisfy a generalized-smooth condition that extends beyond traditional smooth
nonconvex optimization. However, the existing algorithms are not fully adapted
to such generalized-smooth nonconvex geometry and encounter significant
technical limitations on their convergence analysis. In this work, we first
analyze the convergence of adaptively normalized gradient descent under
function geometries characterized by generalized-smoothness and generalized
P{\L} condition, revealing the advantage of adaptive gradient normalization.
Our results provide theoretical insights into adaptive normalization across
various scenarios.For stochastic generalized-smooth nonconvex optimization, we
propose \textbf{I}ndependent-\textbf{A}daptively \textbf{N}ormalized
\textbf{S}tochastic \textbf{G}radient \textbf{D}escent algorithm, which
leverages adaptive gradient normalization, independent sampling, and gradient
clipping to achieve an $\mathcal{O}(\epsilon^{-4})$ sample complexity under
relaxed noise assumptions. Experiments on large-scale nonconvex
generalized-smooth problems demonstrate the fast convergence of our algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 1 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finite Sample Analysis of Distribution-Free Confidence Ellipsoids for
  Linear Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szabolcs Szentpéteri, Balázs Csanád Csáji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The least squares (LS) estimate is the archetypical solution of linear
regression problems. The asymptotic Gaussianity of the scaled LS error is often
used to construct approximate confidence ellipsoids around the LS estimate,
however, for finite samples these ellipsoids do not come with strict
guarantees, unless some strong assumptions are made on the noise distributions.
The paper studies the distribution-free Sign-Perturbed Sums (SPS) ellipsoidal
outer approximation (EOA) algorithm which can construct non-asymptotically
guaranteed confidence ellipsoids under mild assumptions, such as independent
and symmetric noise terms. These ellipsoids have the same center and
orientation as the classical asymptotic ellipsoids, only their radii are
different, which radii can be computed by convex optimization. Here, we
establish high probability non-asymptotic upper bounds for the sizes of SPS
outer ellipsoids for linear regression problems and show that the volumes of
these ellipsoids decrease at the optimal rate. Finally, the difference between
our theoretical bounds and the empirical sizes of the regions are investigated
experimentally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenges learning from imbalanced data using tree-based models:
  Prevalence estimates systematically depend on hyperparameters and can be
  upwardly biased 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Phelps, Daniel J. Lizotte, Douglas G. Woolford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imbalanced binary classification problems arise in many fields of study. When
using machine learning models for these problems, it is common to subsample the
majority class (i.e., undersampling) to create a (more) balanced dataset for
model training. This biases the model's predictions because the model learns
from a dataset that does not follow the same data generating process as new
data. One way of accounting for this bias is to analytically map the resulting
predictions to new values based on the sampling rate for the majority class,
which was used to create the training dataset. While this approach may work
well for some machine learning models, we show that calibrating a random forest
this way has unintended negative consequences, including prevalence estimates
that can be upwardly biased. These prevalence estimates depend on both i) the
number of predictors considered at each split in the random forest; and ii) the
sampling rate used. We explain the former using known properties of random
forests and analytical calibration. However, in investigating the latter issue,
we made a surprising discovery - contrary to the widespread belief that
decision trees are biased towards the majority class, they actually can be
biased towards the minority class.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skewed Score: A statistical framework to assess autograders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magda Dubois, Harry Coppock, Mario Giulianelli, Timo Flesch, Lennart Luettgau, Cozmin Ududec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of large language model (LLM) outputs is increasingly
performed by other LLMs, a setup commonly known as "LLM-as-a-judge", or
autograders. While autograders offer a scalable alternative to human
evaluation, they have shown mixed reliability and may exhibit systematic
biases, depending on response type, scoring methodology, domain specificity, or
other factors. Here we propose a statistical framework based on Bayesian
generalised linear models (GLMs) that enables researchers to simultaneously
assess their autograders while addressing their primary research questions
(e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores
or pairwise preferences) as a function of properties of the grader (e.g., human
vs. autograder) and the evaluated item (e.g., response length or the LLM that
generated it), allowing for explicit quantification of scoring differences and
potential biases within a unified framework. In addition, our method can be
used to augment traditional metrics such as inter-rater agreement, by providing
uncertainty estimates and clarifying sources of disagreement. Overall, this
approach contributes to more robust and interpretable use of autograders in LLM
evaluation, enabling both performance analysis and bias detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyPOTS: A Python Toolkit for Machine Learning on Partially-Observed Time
  Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Du, Yiyuan Yang, Linglong Qian, Jun Wang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PyPOTS is an open-source Python library dedicated to data mining and analysis
on multivariate partially-observed time series with missing values.
Particularly, it provides easy access to diverse algorithms categorized into
five tasks: imputation, forecasting, anomaly detection, classification, and
clustering. The included models represent a diverse set of methodological
paradigms, offering a unified and well-documented interface suitable for both
academic research and practical applications. With robustness and scalability
in its design philosophy, best practices of software construction, for example,
unit testing, continuous integration and continuous delivery, code coverage,
maintainability evaluation, interactive tutorials, and parallelization, are
carried out as principles during the development of PyPOTS. The toolbox is
available on PyPI, Anaconda, and Docker. PyPOTS is open source and publicly
available on GitHub https://github.com/WenjieDu/PyPOTS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PyPOTS website is at https://pypots.com, and PyPOTS is open source at
  https://github.com/WenjieDu/PyPOTS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Invariance Modeling of Multi-Environment Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luhuan Wu, Mingzhang Yin, Yixin Wang, John P. Cunningham, David M. Blei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from
multiple environments to identify invariant features - those with a stable
predictive relationship to the outcome. Such features support generalization to
new environments and help reveal causal mechanisms. Previous methods have
primarily tackled this problem through hypothesis testing or regularized
optimization. Here we develop Bayesian Invariant Prediction (BIP), a
probabilistic model for invariant prediction. BIP encodes the indices of
invariant features as a latent variable and recover them by posterior
inference. Under the assumptions of Peters et al. [2016], the BIP posterior
targets the true invariant features. We prove that the posterior is consistent
and that greater environment heterogeneity leads to faster posterior
contraction. To handle many features, we design an efficient variational
approximation called VI-BIP. In simulations and real data, we find that BIP and
VI-BIP are more accurate and scalable than existing methods for invariant
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wasserstein Gradient <span class="highlight-title">Flow</span>s of MMD Functionals with Distance Kernel and
  Cauchy Problems on Quantile Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07498v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07498v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Duong, Viktor Stein, Robert Beinert, Johannes Hertrich, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give a comprehensive description of Wasserstein gradient flows of maximum
mean discrepancy (MMD) functionals $\mathcal F_\nu := \text{MMD}_K^2(\cdot,
\nu)$ towards given target measures $\nu$ on the real line, where we focus on
the negative distance kernel $K(x,y) := -|x-y|$. In one dimension, the
Wasserstein-2 space can be isometrically embedded into the cone $\mathcal
C(0,1) \subset L_2(0,1)$ of quantile functions leading to a characterization of
Wasserstein gradient flows via the solution of an associated Cauchy problem on
$L_2(0,1)$. Based on the construction of an appropriate counterpart of
$\mathcal F_\nu$ on $L_2(0,1)$ and its subdifferential, we provide a solution
of the Cauchy problem. For discrete target measures $\nu$, this results in a
piecewise linear solution formula. We prove invariance and smoothing properties
of the flow on subsets of $\mathcal C(0,1)$. For certain $\mathcal F_\nu$-flows
this implies that initial point measures instantly become absolutely
continuous, and stay so over time. Finally, we illustrate the behavior of the
flow by various numerical examples using an implicit Euler scheme, which is
easily computable by a bisection algorithm. For continuous targets $\nu$, also
the explicit Euler scheme can be employed, although with limited convergence
guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We corrected the implicit scheme in our code and updated the plots.
  Also, a minor mistake in the def. (14) has been corrected. 45 pages, 23
  figures, comments welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Canonical Polyadic Factorization for Traffic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Luo, Yikai Hou, Peng Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern intelligent transportation systems rely on accurate spatiotemporal
traffic analysis to optimize urban mobility and infrastructure resilience.
However, pervasive missing data caused by sensor failures and heterogeneous
sensing gaps fundamentally hinders reliable traffic modeling. This paper
proposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes
low-rank tensor algebra with deep representation learning for robust traffic
data imputation. The model innovatively embeds CP decomposition into neural
architecture through learnable embedding projections, where sparse traffic
tensors are encoded into dense latent factors across road segments, time
intervals, and mobility metrics. A hierarchical feature fusion mechanism
employs Hadamard products to explicitly model multilinear interactions, while
stacked multilayer perceptron layers nonlinearly refine these representations
to capture complex spatiotemporal couplings. Extensive evaluations on six urban
traffic datasets demonstrate NCPF's superiority over six state-of-the-art
baselines. By unifying CP decomposition's interpretable factor analysis with
neural network's nonlinear expressive power, NCPF provides a principled yet
flexible approaches for high-dimensional traffic data imputation, offering
critical support for next-generation transportation digital twins and adaptive
traffic control systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Multi-Scale Neural Network for Crowd Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.14245v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.14245v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Sagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting is a challenging yet critical task in computer vision with
applications ranging from public safety to urban planning. Recent advances
using Convolutional Neural Networks (CNNs) that estimate density maps have
shown significant success. However, accurately counting individuals in highly
congested scenes remains an open problem due to severe occlusions, scale
variations, and perspective distortions, where people appear at drastically
different sizes across the image. In this work, we propose a novel deep
learning architecture that effectively addresses these challenges. Our network
integrates a ResNet-based feature extractor for capturing rich hierarchical
representations, followed by a downsampling block employing dilated
convolutions to preserve spatial resolution while expanding the receptive
field. An upsampling block using transposed convolutions reconstructs the
high-resolution density map. Central to our architecture is a novel
Perspective-aware Aggregation Module (PAM) designed to enhance robustness to
scale and perspective variations by adaptively aggregating multi-scale
contextual information. We detail the training procedure, including the loss
functions and optimization strategies used. Our method is evaluated on three
widely used benchmark datasets using Mean Absolute Error (MAE) and Mean Squared
Error (MSE) as evaluation metrics. Experimental results demonstrate that our
model achieves superior performance compared to existing state-of-the-art
methods. Additionally, we incorporate principled Bayesian inference techniques
to provide uncertainty estimates along with the crowd count predictions,
offering a measure of confidence in the model's outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Very fast Bayesian Additive Regression Trees on GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Petrillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian
regression technique based on an ensemble of decision trees. It is part of the
toolbox of many statisticians. The overall statistical quality of the
regression is typically higher than other generic alternatives, and it requires
less manual tuning, making it a good default choice. However, it is a niche
method compared to its natural competitor XGBoost, due to the longer running
time, making sample sizes above 10,000-100,000 a nuisance. I present a
GPU-enabled implementation of BART, faster by up to 200x relative to a single
CPU core, making BART competitive in running time with XGBoost. This
implementation is available in the Python package bartz.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Check out the software at https://github.com/Gattocrucco/bartz</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Networks for Tamed Milstein Approximation of SDEs with Additive
  Symmetric Jump Noise Driven by a Poisson Random Measure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose-Hermenegildo Ramirez-Gonzalez, Ying Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work aims to estimate the drift and diffusion functions in stochastic
differential equations (SDEs) driven by a particular class of L\'evy processes
with finite jump intensity, using neural networks. We propose a framework that
integrates the Tamed-Milstein scheme with neural networks employed as
non-parametric function approximators. Estimation is carried out in a
non-parametric fashion for the drift function $f: \mathbb{Z} \to \mathbb{R}$,
the diffusion coefficient $g: \mathbb{Z} \to \mathbb{R}$. The model of interest
is given by \[ dX(t) = \xi + f(X(t))\, dt + g(X(t))\, dW_t + \gamma
\int_{\mathbb{Z}} z\, N(dt,dz), \] where $W_t$ is a standard Brownian motion,
and $N(dt,dz)$ is a Poisson random measure on $(\mathbb{R}_{+} \times
\mathbb{Z}$, $\mathcal{B} (\mathbb{R}_{+}) \otimes \mathcal{Z}$, $\lambda(
\Lambda \otimes v))$, with $\lambda, \gamma > 0$, $\Lambda$ being the Lebesgue
measure on $\mathbb{R}_{+}$, and $v$ a finite measure on the measurable space
$(\mathbb{Z}, \mathcal{Z})$. Neural networks are used as non-parametric
function approximators, enabling the modeling of complex nonlinear dynamics
without assuming restrictive functional forms. The proposed methodology
constitutes a flexible alternative for inference in systems with
state-dependent noise and discontinuities driven by L\'evy processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Gradient Clipping to Normalization for Heavy Tailed SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13849v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13849v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Hübler, Ilyas Fatkhullin, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent empirical evidence indicates that many machine learning applications
involve heavy-tailed gradient noise, which challenges the standard assumptions
of bounded variance in stochastic optimization. Gradient clipping has emerged
as a popular tool to handle this heavy-tailed noise, as it achieves good
performance in this setting both theoretically and practically. However, our
current theoretical understanding of non-convex gradient clipping has three
main shortcomings. First, the theory hinges on large, increasing clipping
thresholds, which are in stark contrast to the small constant clipping
thresholds employed in practice. Second, clipping thresholds require knowledge
of problem-dependent parameters to guarantee convergence. Lastly, even with
this knowledge, current sampling complexity upper bounds for the method are
sub-optimal in nearly all parameters. To address these issues, we study
convergence of Normalized SGD (NSGD). First, we establish a parameter-free
sample complexity for NSGD of
$\mathcal{O}\left(\varepsilon^{-\frac{2p}{p-1}}\right)$ to find an
$\varepsilon$-stationary point. Furthermore, we prove tightness of this result,
by providing a matching algorithm-specific lower bound. In the setting where
all problem parameters are known, we show this complexity is improved to
$\mathcal{O}\left(\varepsilon^{-\frac{3p-2}{p-1}}\right)$, matching the
previously known lower bound for all first-order methods in all problem
dependent parameters. Finally, we establish high-probability convergence of
NSGD with a mild logarithmic dependence on the failure probability. Our work
complements the studies of gradient clipping under heavy tailed noise improving
the sample complexities of existing algorithms and offering an alternative
mechanism to achieve high probability convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed a typo, and removed the abuse of notation in the proof of
  Theorem 4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Inference Isn't Special: Why It's Just Another Prediction Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Fernández-Loría
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference is often portrayed as fundamentally distinct from predictive
modeling, with its own terminology, goals, and intellectual challenges. But at
its core, causal inference is simply a structured instance of prediction under
distribution shift. In both cases, we begin with labeled data from a source
domain and seek to generalize to a target domain where outcomes are not
observed. The key difference is that in causal inference, the labels --
potential outcomes -- are selectively observed based on treatment assignment,
introducing bias that must be addressed through assumptions. This perspective
reframes causal estimation as a familiar generalization problem and highlights
how techniques from predictive modeling, such as reweighting and domain
adaptation, apply directly to causal tasks. It also clarifies that causal
assumptions are not uniquely strong -- they are simply more explicit. By
viewing causal inference through the lens of prediction, we demystify its
logic, connect it to familiar tools, and make it more accessible to
practitioners and educators alike.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A statistical approach to latent dynamic modeling with differential
  equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Hackenberg, Astrid Pechmann, Clemens Kreutz, Janbernd Kirschner, Harald Binder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ordinary differential equations (ODEs) can provide mechanistic models of
temporally local changes of processes, where parameters are often informed by
external knowledge. While ODEs are popular in systems modeling, they are less
established for statistical modeling of longitudinal cohort data, e.g., in a
clinical setting. Yet, modeling of local changes could also be attractive for
assessing the trajectory of an individual in a cohort in the immediate future
given its current status, where ODE parameters could be informed by further
characteristics of the individual. However, several hurdles so far limit such
use of ODEs, as compared to regression-based function fitting approaches. The
potentially higher level of noise in cohort data might be detrimental to ODEs,
as the shape of the ODE solution heavily depends on the initial value. In
addition, larger numbers of variables multiply such problems and might be
difficult to handle for ODEs. To address this, we propose to use each
observation in the course of time as the initial value to obtain multiple local
ODE solutions and build a combined estimator of the underlying dynamics. Neural
networks are used for obtaining a low-dimensional latent space for dynamic
modeling from a potentially large number of variables, and for obtaining
patient-specific ODE parameters from baseline variables. Simultaneous
identification of dynamic models and of a latent space is enabled by recently
developed differentiable programming techniques. We illustrate the proposed
approach in an application with spinal muscular atrophy patients and a
corresponding simulation study. In particular, modeling of local changes in
health status at any point in time is contrasted to the interpretation of
functions obtained from a global regression. This more generally highlights how
different application settings might demand different modeling strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in Retraining: Roaming the Parameter Space of Exponential Families
  Under Closed-Loop Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fariba Jangjoo, Matteo Marsili, Yasser Roudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented if the data contains at least one data
point generated from a ground truth model, by relying on maximum a posteriori
estimation or by introducing regularisation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PASS: Private Attributes Protection with Stochastic Data Substitution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhuo Chen,  Chun-Fu,  Chen, Hsiang Hsu, Shaohan Hu, Tarek Abdelzaher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed
  during a Teleoperated Robotic Surgery Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Vuong, Josie Davidson, Sangheui Cheon, Kyujin Cho, Allison M. Okamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work has shown that the addition of haptic feedback to the hands can
improve awareness of tool-tissue interactions and enhance performance of
teleoperated tasks in robot-assisted minimally invasive surgery. However,
hand-based haptic feedback occludes direct interaction with the manipulanda of
surgeon console in teleoperated surgical robots. We propose relocating haptic
feedback to the wrist using a wearable haptic device so that haptic feedback
mechanisms do not need to be integrated into the manipulanda. However, it is
unknown if such feedback will be effective, given that it is not co-located
with the finger movements used for manipulation. To test if relocated haptic
feedback improves force application during teleoperated tasks using da Vinci
Research Kit (dVRK) surgical robot, participants learned to palpate a phantom
tissue to desired forces. A soft pneumatic wrist-worn haptic device with an
anchoring system renders tool-tissue interaction forces to the wrist of the
user. Participants performed the palpation task with and without wrist-worn
haptic feedback and were evaluated for the accuracy of applied forces.
Participants demonstrated statistically significant lower force error when
wrist-worn haptic feedback was provided. Participants also performed the
palpation task with longer movement times when provided wrist-worn haptic
feedback, indicating that the haptic feedback may have caused participants to
operate at a different point in the speed-accuracy tradeoff curve.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying Emergence in Robot Swarms: An Observer-Dependent Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Vega, Cameron Nowzari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emergence and swarms are widely discussed topics, yet no consensus exists on
their formal definitions. This lack of agreement makes it difficult not only
for new researchers to grasp these concepts, but also for experts who may use
the same terms to mean different things. Many attempts have been made to
objectively define 'swarm' or 'emergence,' with recent work highlighting the
role of the external observer. Still, several researchers argue that once an
observer's vantage point (e.g., scope, resolution, context) is established, the
terms can be made objective or measured quantitatively. In this note, we
propose a framework to discuss these ideas rigorously by separating externally
observable states from latent, unobservable ones. This allows us to compare and
contrast existing definitions of swarms and emergence on common ground. We
argue that these concepts are ultimately subjective-shaped less by the system
itself than by the perception and tacit knowledge of the observer.
Specifically, we suggest that a 'swarm' is not defined by its group behavior
alone, but by the process generating that behavior. Our broader goal is to
support the design and deployment of robotic swarm systems, highlighting the
critical distinction between multi-robot systems and true swarms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 tables, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of LLMs to Multi-Robot Path Planning and Task Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LangNavBench: Evaluation of Natural Language Understanding in Semantic
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia Raychaudhuri, Enrico Cancelli, Tommaso Campari, Lamberto Ballan, Manolis Savva, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large vision-language models has driven improvements in
language-based semantic navigation, where an embodied agent must reach a target
object described in natural language. Despite these advances, we still lack a
clear, language-focused benchmark for testing how well such agents ground the
words in their instructions. We address this gap with LangNav, an open-set
dataset specifically created to test an agent's ability to locate objects
described at different levels of detail, from broad category names to fine
attributes and object-object relations. Every description in LangNav was
manually checked, yielding a lower error rate than existing lifelong- and
semantic-navigation datasets. On top of LangNav we build LangNavBench, a
benchmark that measures how well current semantic-navigation methods understand
and act on these descriptions while moving toward their targets. LangNavBench
allows us to systematically compare models on their handling of attributes,
spatial and relational cues, and category hierarchies, offering the first
thorough, language-centric evaluation of embodied navigation systems. We also
present Multi-Layered Feature Map (MLFM), a method that builds a queryable
multi-layered semantic map, particularly effective when dealing with small
objects or instructions involving spatial relations. MLFM outperforms
state-of-the-art mapping-based navigation baselines on the LangNav dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MarineFormer: A Spatio-Temporal Attention Model for USV Navigation in
  Dynamic Marine Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13973v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13973v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Kazemi, Dechen Gao, Iman Soltani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation in marine environments can be extremely challenging,
especially in the presence of spatially varying flow disturbances and dynamic
and static obstacles. In this work, we demonstrate that incorporating local
flow field measurements fundamentally alters the nature of the problem,
transforming otherwise unsolvable navigation scenarios into tractable ones.
However, the mere availability of flow data is not sufficient; it must be
effectively fused with conventional sensory inputs such as ego-state and
obstacle states. To this end, we propose \textbf{MarineFormer}, a
Transformer-based policy architecture that integrates two complementary
attention mechanisms: spatial attention for sensor fusion, and temporal
attention for capturing environmental dynamics. MarineFormer is trained
end-to-end via reinforcement learning in a 2D simulated environment with
realistic flow features and obstacles. Extensive evaluations against classical
and state-of-the-art baselines show that our approach improves episode
completion success rate by nearly 23\% while reducing path length. Ablation
studies further highlight the critical role of flow measurements and the
effectiveness of our proposed architecture in leveraging them.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-07-12T05:33:51.041842757Z">
            2025-07-12 05:33:51 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
<script src="search-enhance.js"></script>
</html>
